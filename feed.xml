<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Better Thinking</title>
  
  <subtitle>this is naeun blog</subtitle>
  <link href="https://ne-choi.github.io/feed.xml" rel="self"/>
  
  <link href="https://ne-choi.github.io/"/>
  <updated>2021-02-01T01:02:57.419Z</updated>
  <id>https://ne-choi.github.io/</id>
  
  <author>
    <name>Naeun Choi</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 2: Ⅷ. 회귀분석을 이용한 예측모형</title>
    <link href="https://ne-choi.github.io/2021/01/14/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A7_%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/"/>
    <id>https://ne-choi.github.io/2021/01/14/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A7_%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/</id>
    <published>2021-01-14T00:00:00.000Z</published>
    <updated>2021-02-01T01:02:57.419Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.</li></ul><h1 id="Ⅷ-회귀분석을-이용한-예측모형"><a href="#Ⅷ-회귀분석을-이용한-예측모형" class="headerlink" title="Ⅷ. 회귀분석을 이용한 예측모형"></a>Ⅷ. 회귀분석을 이용한 예측모형</h1><h2 id="1-선형모형의-특성과-상관분석"><a href="#1-선형모형의-특성과-상관분석" class="headerlink" title="1. 선형모형의 특성과 상관분석"></a>1. 선형모형의 특성과 상관분석</h2><h3 id="데이터의-상관관계"><a href="#데이터의-상관관계" class="headerlink" title="데이터의 상관관계"></a>데이터의 상관관계</h3><ul><li>데이터 탐색: 변수 간 관계<ul><li>변수 간 상관관계 여부</li><li>상관관계 형태</li></ul></li></ul><h3 id="상관계수-Pearson’s"><a href="#상관계수-Pearson’s" class="headerlink" title="상관계수(Pearson’s)"></a>상관계수(Pearson’s)</h3><ul><li>상관계수(correlation coefficient)<ul><li>두 변수 간 선형관계의 강도를 나타내주는 척도</li></ul></li><li>상관계수(r) 절댓값은 0-1 사이<ul><li>0에 가까울수록 상관관계가 없음</li><li>1에 가까울수록 강한 상관성이 있음</li></ul></li></ul><h3 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h3><ul><li>상관계수 및 변수 간 산점도: 데이터 탐색</li><li>X, Y 간 선형관계를 산점도에 도식화</li><li>X, Y 간 선형함수식 추정: 회귀모형</li><li>Y 분산에 관한 독립변수를 추가함으로써 선형모형 설명력 향상</li></ul><h2 id="2-실제데이터-예측모형구현"><a href="#2-실제데이터-예측모형구현" class="headerlink" title="2. 실제데이터 예측모형구현"></a>2. 실제데이터 예측모형구현</h2><h3 id="회귀모형의-적합-조선"><a href="#회귀모형의-적합-조선" class="headerlink" title="회귀모형의 적합 조선"></a>회귀모형의 적합 조선</h3><ul><li><p>회귀분석- 예측모형</p></li><li><p>회귀분석의 목적: 예측(prediction)과 추정(estimation)</p><ul><li>선형모형: 독립변수와 종속변수 관계가 선형식으로 적합<ul><li>최소자승법(least squares method): 예측값과 관측치 오차를 최소화시키는 회귀계수 추정</li></ul></li></ul></li><li><p>예측 모형</p><ul><li>회귀 계수(regression coefficient)와 독립변수의 선형 결합으로 표현되는 예측모형</li><li>Y = β<del>0</del> + β<del>1</del>x<del>1</del> + β<del>2</del>x<del>2</del> + … + β<del>k</del>x<del>k</del><ul><li>Y: 종속변수(dependent variable), 반응변수, 타겟변수</li><li>x: 독립변수(independent variable), 설명변수, 원인 역할을 하는 변수</li></ul></li></ul></li></ul><h3 id="회귀모형의-적합도-및-회귀성-검정"><a href="#회귀모형의-적합도-및-회귀성-검정" class="headerlink" title="회귀모형의 적합도 및 회귀성 검정"></a>회귀모형의 적합도 및 회귀성 검정</h3><ul><li>모형의 적합도와 결정계수 (R^2^)<ul><li>회귀식이 데이터를 얼마나 잘 설명하는지에 관한 척도</li><li>회귀모형으로 설명할 수 있는 부분의 비율</li><li>0 ≤ R^2^ ≤ 1</li><li>R^2^ = $\frac{SSR}{SST}$ = 1 - $\frac{SSE}{SST}$</li></ul></li></ul><h3 id="회귀모형의-가정과-진단"><a href="#회귀모형의-가정과-진단" class="headerlink" title="회귀모형의 가정과 진단"></a>회귀모형의 가정과 진단</h3><ul><li><p>가정</p><ul><li>X와 Y는 선형적 관계</li><li>오차항(ε<del>i</del>)은 정규분포를 따름</li><li>오차항의 평균은 0</li><li>오차항의 등분산성</li><li>오차항은 서로 독립</li></ul></li><li><p>진단</p><ul><li>잔차 e<del>i</del>와 Y 산점도</li><li>스튜던트화 잔차</li><li>정규확률분포도</li></ul></li></ul><h3 id="회귀계수에-대한-검정"><a href="#회귀계수에-대한-검정" class="headerlink" title="회귀계수에 대한 검정"></a>회귀계수에 대한 검정</h3><ul><li><p>β<del>1</del> 관한 t-검정</p><ul><li>해당 독립변수가 유의한 영향을 갖고 있는지 판단</li></ul></li><li><p>회귀계수의 p-value</p><ul><li>특정변수의 회귀계수에 관한 p-value가 유의수준(α)보다 작으면 그 변수는 유의(significant)하다고 판단</li></ul></li></ul><h3 id="정리-1"><a href="#정리-1" class="headerlink" title="정리"></a>정리</h3><ul><li>회귀모형의 적합도 검정<ul><li>이론적인 모형이 실제 관측 데이터에 의해 어느 정도 지지를 받는지 검정하는 것<ul><li>R^2^ 결정계수: 회귀모형의 설명력</li><li>개별 β<del>k</del>값 검정: 어떤 변수가 중요한지</li><li>잔차에 관한 가정 확인: 잔차산점도</li></ul></li></ul></li></ul><h2 id="3-회귀모형의-적합조건"><a href="#3-회귀모형의-적합조건" class="headerlink" title="3. 회귀모형의 적합조건"></a>3. 회귀모형의 적합조건</h2><h3 id="실제-데이터로-예측모형-구현"><a href="#실제-데이터로-예측모형-구현" class="headerlink" title="실제 데이터로 예측모형 구현"></a>실제 데이터로 예측모형 구현</h3><ul><li>다중회귀모형(multiple regression)<ul><li>종속변수 Y를 설명하는 데 k개의 독립변수가 있을 때의 회귀모형</li><li>Y<del>i</del> = β<del>0</del> + β<del>1</del>x<del>1</del> + β<del>2</del>x<del>2</del> + … + β<del>k</del>x<del>k</del> + ε<del>i</del>(잔차)</li><li>회귀계수 β<del>k</del>의 해석: 다른 독립변수가 일정할 때, X<del>k</del> 한 단위 변화에 따른 평균변화량</li></ul></li></ul><h3 id="다중회귀모형-예제"><a href="#다중회귀모형-예제" class="headerlink" title="다중회귀모형 예제"></a>다중회귀모형 예제</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">car &lt;- read.csv(&#39;.&#x2F;data&#x2F;autompg.csv&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r1 &lt;- lm(mpg ~ disp+hp+wt+accler, data&#x3D;car)</span><br><span class="line">summary(r1)</span><br></pre></td></tr></table></figure><ul><li><p>결과 해석</p><ul><li>선형회귀식: mpg = 40.88 - 0.001 disp + 0.0048 hp - 0.0061 wt + 0.17 accler</li><li>선형회귀식의 결정계수: R^2^ = 0.7006<ul><li>분산을 70% 정도 설명한다고 해석할 수 있음</li></ul></li></ul></li><li><p>문제</p><ul><li>마력(hp)이 높을수록 연비가 좋은가? 일반적으로는 아니기 때문에 데이터 탐색 필요</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">var1 &lt;- c(&quot;mpg&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;accler&quot;)</span><br><span class="line">pairs(car[var1], main&#x3D;&quot;Autompg&quot;, cex&#x3D;1, col&#x3D;as.integer(car$cyl))</span><br></pre></td></tr></table></figure><ul><li>다중공선성(Multicollinearity)<ul><li>독립변수들 사이 상관관계가 높은 경우</li><li>다중공선성이 존재하면, 회귀계수 분산이 크고 모형이 불안정</li><li>해당 예제에서는 disp(배기량) 제외하는 것이 더 좋음</li></ul></li></ul><ul><li>hp 기준으로 두 개의 모델로 분류해 예측하기<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">car2 &lt;- subset(car, hp&lt;50)</span><br><span class="line">attach(car2)</span><br><span class="line">r2 &lt;- lm(mpg~hp+wt+accler, data&#x3D;car2)</span><br><span class="line">summary(r2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">car3 &lt;- subset(car, hp&gt;&#x3D;50)</span><br><span class="line">attach(car3)</span><br><span class="line">r3 &lt;- lm(mpg~hp+wt+accler, data&#x3D;car2)</span><br><span class="line">summary(r3)</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-예측모형의-핵심과-고려사항"><a href="#4-예측모형의-핵심과-고려사항" class="headerlink" title="4. 예측모형의 핵심과 고려사항"></a>4. 예측모형의 핵심과 고려사항</h2><h3 id="분류분석-로지스틱회귀모형"><a href="#분류분석-로지스틱회귀모형" class="headerlink" title="분류분석- 로지스틱회귀모형"></a>분류분석- 로지스틱회귀모형</h3><ul><li><p>타겟변수(Y)가 범주형일 때</p><ul><li>binary: 2개의 범주 (보험사기, 질병)</li><li>ordinal(서열): 서열이 있는 범주 (순위)</li><li>nominal(명목): 서열이 없는 범주 (직업군)</li></ul></li><li><p>로지스틱회귀모형</p><ul><li>Y가 범주이기 때문에 선형 조합이 나옴</li><li>로짓 트랜스포메이션을 해서 확률값으로 변형</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re &lt;- read.csv(&#39;.&#x2F;data&#x2F;remiss.csv&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t3 &lt;- glm(remiss~cell+li+temp, data&#x3D;re,family&#x3D;binomial(logit))</span><br><span class="line">summary(t3)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>logistic equation<ul><li>logit(p) = 67.63 + 9.65 Cell + 3.87 Li - 82.07 Temp</li></ul></li><li>Li 1단위 증가 시 remission 확률: exp(3.867) = 47.79</li></ul></li></ul><ul><li>예측확률값 출력: 원래 데이터 + 예측확률값<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dat1_pred &lt;- cbind(re,t3$fitted.values)</span><br><span class="line">write.table(dat1_pred,file&#x3D;&#39;dat1_pred.csv&#39;, row.names&#x3D;FALSE, sep&#x3D;&quot;,&quot;, na&#x3D;&quot; &quot;)</span><br></pre></td></tr></table></figure></li></ul><h3 id="예측모형-고려사항"><a href="#예측모형-고려사항" class="headerlink" title="예측모형 고려사항"></a>예측모형 고려사항</h3><ul><li><p>예측모형 선택</p><ul><li>예측모형(어느 모형, 어느 기법)<ul><li>결정계수(기여율)가 높은 회귀식: 설명변수가 많을수록 결정계수가 높아짐</li><li>추정값의 표준오차(잔차의 표준편차)가 낮은 회귀식</li></ul></li><li>변수 선택방법<ul><li>모든 가능한 회귀: 모든 설명변수를 포함한 모형을 시도</li><li>단계적 선택법(stepwise regression): 설명변수를 단계적으로 선택하는 방법</li></ul></li></ul></li><li><p>이상치(outlier) 탐지</p><ul><li>이상치(outlier): 보편적인 데이터 값 번위를 벗어나는 데이터</li><li>왜곡된 예측모형 위험이 있으므로 이상치 제외 후 분석</li><li>이상치 탐색<ul><li>거리 계산: 각 데이터와 독립변수 평균과의 거리가 크거나, 회귀모형에서의 잔차 이상 수치 확인</li><li>산점도: 변수들 간 산점도에서 탐색</li></ul></li></ul></li></ul><h3 id="정리-2"><a href="#정리-2" class="headerlink" title="정리"></a>정리</h3><ol><li>데이터 수집(DB, 설문조사)</li><li>종속변수(타겟변수)와 관계 있는 독립변수(설명변수) 확인</li><li>산포도 작성 → 데이터 특성 파악 및 이상치 탐색</li><li>회귀모형 결정(변수 선택/ 특징 변환)</li><li>회귀모형 적합도 평가</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅷ-회귀분석을-이용한-예측모형&quot;&gt;&lt;a</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="회귀모형" scheme="https://ne-choi.github.io/tags/%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 2: Ⅵ. 현업 데이터 특성과 예측모형</title>
    <link href="https://ne-choi.github.io/2021/01/13/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A6_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C%EC%99%80_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/"/>
    <id>https://ne-choi.github.io/2021/01/13/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A6_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C%EC%99%80_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/</id>
    <published>2021-01-13T00:00:00.000Z</published>
    <updated>2021-02-01T01:00:35.505Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.</li></ul><h1 id="Ⅶ-빅데이터-차원축소와-예측모형"><a href="#Ⅶ-빅데이터-차원축소와-예측모형" class="headerlink" title="Ⅶ. 빅데이터 차원축소와 예측모형"></a>Ⅶ. 빅데이터 차원축소와 예측모형</h1><h2 id="1-데이터-탐색과-정제"><a href="#1-데이터-탐색과-정제" class="headerlink" title="1. 데이터 탐색과 정제"></a>1. 데이터 탐색과 정제</h2><h3 id="데이터-탐색"><a href="#데이터-탐색" class="headerlink" title="데이터 탐색"></a>데이터 탐색</h3><ul><li>Basic Information: 분포, 평균, 최솟값, 최댓값, 분산 등</li><li>Variation: 변동성, 시계열</li><li>Outlier: 모든 데이터가 상식적인 범위 안에 존재하는가?</li><li>Correlation: 변수 간 상관성, 변수 간 상호작용이 있는가?</li></ul><h2 id="2-빅데이터의-차원축소"><a href="#2-빅데이터의-차원축소" class="headerlink" title="2. 빅데이터의 차원축소"></a>2. 빅데이터의 차원축소</h2><h3 id="차원축소기법은-왜-필요한가"><a href="#차원축소기법은-왜-필요한가" class="headerlink" title="차원축소기법은 왜 필요한가"></a>차원축소기법은 왜 필요한가</h3><ul><li><p>현업 데이터의 특성</p><ul><li>타겟변수 특성에 영향을 미치는 요인이 매우 많음</li></ul></li><li><p>변수 간 다중공선성(높은 상관관계, 상호작용)</p></li><li><p>과적합 위험 증가</p></li><li><p>타겟값 정보가 들어있는 구조 파악이 어려움</p></li></ul><h3 id="차원축소기법-주성분분석"><a href="#차원축소기법-주성분분석" class="headerlink" title="차원축소기법- 주성분분석"></a>차원축소기법- 주성분분석</h3><ul><li><p>주성분분석(Principal Component Analysis; PCA)</p><ul><li>가장 대표적인 차원 숙소 방법</li><li>첫 번째 변수가 전체 분산을 가장 많이 설명하고, 다음 변수가 나머지 분산을 가장 많이 설명하는 방식으로 변수 생성</li><li>새로 구성된 변수는 서로 독립</li></ul></li><li><p>주성분분석 시행 방법</p><ol><li>데이터 공간에서 분산이 최대인 축을 찾는다. (첫 번째 주성분: PC1)</li><li>첫 번째 축과 직교하며 분산이 최대인 두 번째 축을 찾는다. (두 번째 주성분: PC2)</li></ol></li><li><p>최적 주성분 찾기</p><ul><li>전체 변동에 대한 기여도: 전체 변동의 약 90%를 설명하는 차원까지</li><li>Scree plot 활용: 기울기가 갑자기 줄어드는 차원까지</li></ul></li></ul><h2 id="3-데이터-변환에-의한-저차원-시각화"><a href="#3-데이터-변환에-의한-저차원-시각화" class="headerlink" title="3. 데이터 변환에 의한 저차원 시각화"></a>3. 데이터 변환에 의한 저차원 시각화</h2><h3 id="고차원데이터의-차원축소"><a href="#고차원데이터의-차원축소" class="headerlink" title="고차원데이터의 차원축소"></a>고차원데이터의 차원축소</h3><ul><li><p>데이터 유형 파악</p><ul><li>Classification(Y: 범주형)</li><li>Prediction(Y: 연속형)</li></ul></li><li><p>변수 선택(feature selection) 기법</p><ul><li>특정 변수를 선택하여 모델링</li><li>다중공선성 존재하는 변수는 그 중 하나의 변수만 선택</li></ul></li><li><p>특징 추출(feature extraction) 기법</p><ul><li>새로운 축을 생성했을 때 생기는 변화</li><li>고차원 데이터 정보를 보존하며 노이즈를 제거하는 방식으로 특징 추출</li></ul></li></ul><h3 id="고차원데이터의-저차원-시각화"><a href="#고차원데이터의-저차원-시각화" class="headerlink" title="고차원데이터의 저차원 시각화"></a>고차원데이터의 저차원 시각화</h3><ul><li>주성분분석: 선형 추출 기법</li><li>인접보존기법: 비선형 추출 기법</li></ul><h3 id="차원축소와-예측모형"><a href="#차원축소와-예측모형" class="headerlink" title="차원축소와 예측모형"></a>차원축소와 예측모형</h3><ul><li>Feature Ectraction<ul><li>인접보존기법</li><li>제한적 볼츠만머신</li><li>오토인코더</li></ul></li><li>Classifier</li></ul><h2 id="4-데이터와-예측모형"><a href="#4-데이터와-예측모형" class="headerlink" title="4. 데이터와 예측모형"></a>4. 데이터와 예측모형</h2><h3 id="데이터와-예측모형"><a href="#데이터와-예측모형" class="headerlink" title="데이터와 예측모형"></a>데이터와 예측모형</h3><ul><li><p><code>데이터 정제 → 데이터 탐색</code> → 통계적 모델링(통계모형, 기계학습, 인공지능)</p></li><li><p>데이터 분석목적</p><ul><li>예측(prediction)</li><li>분류(classification)</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅶ-빅데이터-차원축소와-예측모형&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="주성분분석" scheme="https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"/>
    
    <category term="차원축소" scheme="https://ne-choi.github.io/tags/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 2: Ⅵ. 현업 데이터 특성과 예측모형</title>
    <link href="https://ne-choi.github.io/2021/01/12/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A5_%ED%98%84%EC%97%85_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%8A%B9%EC%84%B1%EA%B3%BC_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/"/>
    <id>https://ne-choi.github.io/2021/01/12/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A5_%ED%98%84%EC%97%85_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%8A%B9%EC%84%B1%EA%B3%BC_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/</id>
    <published>2021-01-12T00:00:00.000Z</published>
    <updated>2021-02-01T01:00:03.851Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.</li></ul><h1 id="Ⅵ-현업-데이터-특성과-예측모형"><a href="#Ⅵ-현업-데이터-특성과-예측모형" class="headerlink" title="Ⅵ. 현업 데이터 특성과 예측모형"></a>Ⅵ. 현업 데이터 특성과 예측모형</h1><h2 id="1-데이터-수집-random의-의미"><a href="#1-데이터-수집-random의-의미" class="headerlink" title="1. 데이터 수집- random의 의미"></a>1. 데이터 수집- random의 의미</h2><h3 id="데이터-수집"><a href="#데이터-수집" class="headerlink" title="데이터 수집"></a>데이터 수집</h3><ul><li>데이터 수집에서는 양질의 데이터를 확보하는 것이 중요<ul><li>나쁜 데이터로는 나쁜 모델밖에 만들 수 없음</li><li>‘대표성’, ‘랜덤’ 데이터</li></ul></li></ul><h3 id="데이터-수집-표본-추출"><a href="#데이터-수집-표본-추출" class="headerlink" title="데이터 수집_표본 추출"></a>데이터 수집_표본 추출</h3><ul><li>좋은 표본: 모집단의 특성을 가능한 정확하게 반영한 표본</li><li>표본추출에서 가장 중요한 문제는 대표성 있는 표본을 확보하는 것</li></ul><h3 id="전수조사-vs-표본조사"><a href="#전수조사-vs-표본조사" class="headerlink" title="전수조사 vs 표본조사"></a>전수조사 vs 표본조사</h3><ul><li>전수조사: 연구대상집단의 모든 데이터 수집</li><li>표본조사: 연구대상집단 일부 데이터 추출</li></ul><h3 id="랜덤의-의미"><a href="#랜덤의-의미" class="headerlink" title="랜덤의 의미"></a>랜덤의 의미</h3><ul><li>무작위 추출(Random sampling)<ul><li>모집단에서 표본을 뽑을 때 각 개체가 선택될 확률은 동일</li><li>무작위로 추출된 표본은 편의가 최대한 배제됨</li></ul></li></ul><h3 id="군집표본추출-vs-층화표본추출"><a href="#군집표본추출-vs-층화표본추출" class="headerlink" title="군집표본추출 vs 층화표본추출"></a>군집표본추출 vs 층화표본추출</h3><ul><li><p>군집표본추출(cluster sampling)</p><ul><li>각 군집(cluster)이 동일한 특성을 갖고 있다면, 그 중 무작위로 cluster 선택<ul><li>e.g. 학급, ##구, ##동</li></ul></li></ul></li><li><p>층화표본추출(stratified sampling)</p><ul><li>모집단 내 하위집단 특성이 다를 때, 하위집단을 기반으로 표본 선택<ul><li>e.g. 정치적 의견(연령별, 지역별)</li></ul></li></ul></li></ul><h3 id="복원추출-vs-비복원추출"><a href="#복원추출-vs-비복원추출" class="headerlink" title="복원추출 vs 비복원추출"></a>복원추출 vs 비복원추출</h3><ul><li>복원추출: 표본추출 시 뽑은 표본의 데이터를 다시 넣고 추출</li><li>비복원추출: 표본추출 시 추출된 표본을 제외하고 다음 추출</li></ul><h2 id="2-예측모형에서-training과-test-set"><a href="#2-예측모형에서-training과-test-set" class="headerlink" title="2. 예측모형에서 training과 test set"></a>2. 예측모형에서 training과 test set</h2><h3 id="예측모형을-구현할-때"><a href="#예측모형을-구현할-때" class="headerlink" title="예측모형을 구현할 때"></a>예측모형을 구현할 때</h3><ul><li><p>e.g. 영화 추천, 음악 추천</p></li><li><p>어떻게 원하는 콘텐츠를 예측하여 추천하는가?</p><ul><li>과거 구매패턴을 분석하여 미래 구매를 예측(추천)<ul><li>과거 데이터 → 분석 → 예측 모형 → 예측</li></ul></li></ul></li><li><p>주어진 데이터 → 예측모형</p><ul><li>모형이 적절한지 어떻게 판단하는가?</li></ul></li></ul><h3 id="예측모형-좋은-예측모형"><a href="#예측모형-좋은-예측모형" class="headerlink" title="예측모형: 좋은 예측모형"></a>예측모형: 좋은 예측모형</h3><ul><li><p>좋은 예측모형이란</p><ul><li>새로운 데이터가 들어왔을 때 정확하게 예측하는 모형<ul><li>훈련데이터(Training set): 모형을 만들기 위해 사용</li><li>검증데이터(Test set): 만들어진 모형 성능 평가에 사용</li></ul></li></ul></li><li><p>k-fold cross-validation (k=3, 5, 10)</p><ul><li>주어진 데이터를 몇 개로 나눌 것인지</li><li>k-1/k는 훈련데이터로, 1/k는 검증데이터로 사용</li><li>데이터 나누는 것은 random split으로</li></ul></li></ul><h2 id="3-예측모형의-과적합-overfitting"><a href="#3-예측모형의-과적합-overfitting" class="headerlink" title="3. 예측모형의 과적합(overfitting)"></a>3. 예측모형의 과적합(overfitting)</h2><h3 id="예측모형의-과적합"><a href="#예측모형의-과적합" class="headerlink" title="예측모형의 과적합"></a>예측모형의 과적합</h3><ul><li><p>과적합(overfitting): 주어진 데이터에 과하게 적합하여, 새로운 데이터가 들어왔을 때 정확도를 보장할 수 없는 경우</p></li><li><p>과소적합(Under-fitted)</p></li><li><p>적정적합(Generalized-fitted)</p></li><li><p>과잉적합(Over-fitted)</p></li></ul><h2 id="4-over-amp-under-sampling-문제"><a href="#4-over-amp-under-sampling-문제" class="headerlink" title="4. over &amp; under sampling 문제"></a>4. over &amp; under sampling 문제</h2><h3 id="데이터-기반-예측-모형"><a href="#데이터-기반-예측-모형" class="headerlink" title="데이터 기반 예측 모형"></a>데이터 기반 예측 모형</h3><ul><li>데이터를 기반으로 모델을 생성하여 새로운 데이터 예측 가능</li></ul><h3 id="데이터-불균형-문제"><a href="#데이터-불균형-문제" class="headerlink" title="데이터 불균형 문제"></a>데이터 불균형 문제</h3><ul><li>집단 간 데이터 비율 차이가 크면 다수 집단의 정확도를 기준으로 예측모형이 결정될 수 있음<ul><li>e.g. 보험 사기 건 수</li><li>전체 데이터 25개 중, 범주 1인 22개만 정확히 분류하고 범주 2인 3개는 모두 오분류된다고 해도 전체 정확도는 88%로 높음</li></ul></li></ul><p>→ 전체 정확도 기준 이외, 소수집단 오분류율에 위험 비용을 주고 최적화된 모형을 찾는 것이 바람직함</p><h3 id="데이터-균형-맞추기"><a href="#데이터-균형-맞추기" class="headerlink" title="데이터 균형 맞추기"></a>데이터 균형 맞추기</h3><ul><li><strong>Over-sampling</strong>: 소수범주에서 다수범주 수만큼 복원 추출<ul><li>장) 정보 손실 없음</li><li>단) 소수 데이터가 단순 복사되어 과적합 가능성 있음</li></ul></li><li><strong>Under-sampling</strong>: 다수범주에서 소수범주 수만큼 랜덤하게 추출<ul><li>장) 데이터 저장 용량 감소, 데이터가 적어 실행 속도 향상</li><li>단) 중요 정보 누락될 가능성 있음</li></ul></li></ul><p>→ 혼합형 방식의 sampling 사용</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅵ-현업-데이터-특성과-예측모형&quot;&gt;&lt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="표본추출" scheme="https://ne-choi.github.io/tags/%ED%91%9C%EB%B3%B8%EC%B6%94%EC%B6%9C/"/>
    
    <category term="과적합" scheme="https://ne-choi.github.io/tags/%EA%B3%BC%EC%A0%81%ED%95%A9/"/>
    
    <category term="과소적합" scheme="https://ne-choi.github.io/tags/%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 2: Ⅴ. 통계검정방법</title>
    <link href="https://ne-choi.github.io/2021/01/11/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A4_%ED%86%B5%EA%B3%84%EA%B2%80%EC%A0%95%EB%B0%A9%EB%B2%95/"/>
    <id>https://ne-choi.github.io/2021/01/11/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A4_%ED%86%B5%EA%B3%84%EA%B2%80%EC%A0%95%EB%B0%A9%EB%B2%95/</id>
    <published>2021-01-11T00:00:00.000Z</published>
    <updated>2021-02-01T00:58:27.877Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.</li></ul><h1 id="Ⅴ-통계검정방법"><a href="#Ⅴ-통계검정방법" class="headerlink" title="Ⅴ. 통계검정방법"></a>Ⅴ. 통계검정방법</h1><h2 id="1-신뢰구간의-의미"><a href="#1-신뢰구간의-의미" class="headerlink" title="1. 신뢰구간의 의미"></a>1. 신뢰구간의 의미</h2><h3 id="신뢰구간"><a href="#신뢰구간" class="headerlink" title="신뢰구간"></a>신뢰구간</h3><ul><li>신뢰구간: 구간추정<ul><li>실제 모수가 존재할 가능성이 높은 구간으로 추정</li><li>모평균, 모비율 등 모수를 포함할 확률</li><li>신뢰수준(Confidence level): 구간에 모수 μ가 포함될 확률</li><li>일반적으로 100(1-α)%로 나타냄  </li></ul></li></ul><h3 id="95-신뢰구간의-의미"><a href="#95-신뢰구간의-의미" class="headerlink" title="95% 신뢰구간의 의미"></a>95% 신뢰구간의 의미</h3><ul><li>100번의 반복샘플링으로 얻은 평균과 편차로 계산한 100개의 신뢰구간 중, 5개는 실제 모평균(μ)을 포함하고 있지 않는다.</li><li>표본을 통해 얻은 95% 신뢰구간에 실제 모평균이 포함되지 않을 확률은 5%이다</li><li>전체 집단(즉, 모집단 전체)을 조사할 수 없기 때문에 이용한다</li><li>신뢰구간은 고정된 단일 값이 아닌 움직이는 여러 값이다.</li></ul><h3 id="적당한-신뢰구간은"><a href="#적당한-신뢰구간은" class="headerlink" title="적당한 신뢰구간은?"></a>적당한 신뢰구간은?</h3><ul><li>90% 신뢰구간: z<del>α/2</del> = z<del>0.5</del> = 1.65</li><li>95% 신뢰구간: z<del>α/2</del> = z<del>0.25</del> = 1.96</li><li>99% 신뢰구간: z<del>α/2</del> = z<del>0.05</del> = 2.57  </li></ul><h3 id="표본사이즈와-허용오차"><a href="#표본사이즈와-허용오차" class="headerlink" title="표본사이즈와 허용오차"></a>표본사이즈와 허용오차</h3><ul><li>허용오차를 일정수준으로 정하면 그에 따른 표본크기가 정해짐</li><li>신뢰구간에서 허용오차에 영향을 미치는 요소<ul><li>표본의 크기, 유의수준, 표준편차 (표본평균은 X)</li></ul></li></ul><h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul><li>실험 및 조사설계 시에는 허용오차 및 신뢰수준을 고려해야 정확한 분석이 가능</li></ul><h2 id="2-통계적-검정은-왜-필요한가"><a href="#2-통계적-검정은-왜-필요한가" class="headerlink" title="2. 통계적 검정은 왜 필요한가"></a>2. 통계적 검정은 왜 필요한가</h2><h3 id="통계적-검정이란"><a href="#통계적-검정이란" class="headerlink" title="통계적 검정이란?"></a>통계적 검정이란?</h3><ul><li><p>가설의 진위 여부를 판단, 증명, 검정하는 통계적 추론 방식</p><ul><li><p><strong>귀무가설(Null Hypothesis, H0)</strong></p><ul><li>검정 대상이 되는 가설</li><li>기각(reject)을 목표로 함</li></ul></li><li><p><strong>대립가설(Alternative Hypothesis, H1)</strong></p><ul><li>귀무가설이 기각될 때 받아들여지는 가설</li><li>채택(accept)을 목표로 함</li></ul></li></ul></li><li><p>가설 검정 절차</p><ul><li>가설 설정 → 유의수준 설정 → 검정통계량 산출 → 가설 기각/채택</li></ul></li></ul><h3 id="검정-예시-T-test"><a href="#검정-예시-T-test" class="headerlink" title="검정 예시: T-test"></a>검정 예시: T-test</h3><ul><li>단측검정(one-sided test)</li><li>양측검정(two-sided test): 기각역이 양측</li></ul><h3 id="단측검정"><a href="#단측검정" class="headerlink" title="단측검정"></a>단측검정</h3><ul><li>소비자 주장) 카페에서 파는 커피 용량이 200ml보다 작다.<ul><li>귀무가설(H<del>0</del>): 커피 용량은 200ml다</li><li>대립가설(H<del>1</del>): 커피 용량은 200ml보다 적다</li></ul></li></ul><h3 id="t-분포를-이용한-검정"><a href="#t-분포를-이용한-검정" class="headerlink" title="t-분포를 이용한 검정"></a>t-분포를 이용한 검정</h3><ul><li>한 집단 혹은 두 집단 간 평균 차이에 대한 통계적 검정 방법<ul><li><strong>T-distribution</strong><ul><li>사회 현상은 평균 데이터가 많으므로 정규 분포(normal distribution) 형태</li><li>정규 분포는 표본의 데이터 수가 많아야 신뢰도가 향상됨(일반적으로 30개 이상)</li><li>데이터가 적은 경우, 예측 범위가 더 넓은 T-distribution 사용</li></ul></li><li><strong>T-test의 가정</strong><ul><li>독립성: 두 집단 변수는 서로 독립</li><li>정규성: 두 집단 데이터는 정규분포를 만족</li><li>등분산성: 두 집단 분산은 동일</li></ul></li></ul></li></ul><h3 id="통계적-검정의-계산-예시"><a href="#통계적-검정의-계산-예시" class="headerlink" title="통계적 검정의 계산 예시"></a>통계적 검정의 계산 예시</h3><ul><li>t = 표본평균 - μ / (표본표준편차(s)/표본의 수 루트)</li></ul><h2 id="3-두-집단-t-검정"><a href="#3-두-집단-t-검정" class="headerlink" title="3. 두 집단 t-검정"></a>3. 두 집단 t-검정</h2><h3 id="두-집단-간-평균-비교"><a href="#두-집단-간-평균-비교" class="headerlink" title="두 집단 간 평균 비교"></a>두 집단 간 평균 비교</h3><ul><li>학습 목표<ul><li>두 모집단 평균 비교를 위한 t-검정 계산 과정 익히기</li></ul></li></ul><h3 id="t-검정-검정통계량과-기각역"><a href="#t-검정-검정통계량과-기각역" class="headerlink" title="t-검정 검정통계량과 기각역"></a>t-검정 검정통계량과 기각역</h3><ul><li>가설 → 조사 → 데이터 → 검정통계량 → 가설에 대한 결정 (유의수준 α)<ul><li>검정통계량이 기각역 안에 있으면 귀무가설 기각 → 대립가설 인정</li></ul></li></ul><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><h2 id="4-p-value의-실제-의미"><a href="#4-p-value의-실제-의미" class="headerlink" title="4. p-value의 실제 의미"></a>4. p-value의 실제 의미</h2><h3 id="p-value"><a href="#p-value" class="headerlink" title="p-value"></a>p-value</h3><ul><li>데이터 결과 자료에서 보게 되는 값</li><li>변수의 통계적 유의도를 나타내는 값 (유의확률)</li></ul><h3 id="p-value-찾기"><a href="#p-value-찾기" class="headerlink" title="p-value 찾기"></a>p-value 찾기</h3><ul><li><p>예제</p><ul><li><p>25개 표본의 과자 한 팩 평균 무게가 87g이다. (모표준편차 = 15g) 유의수준 α = 0.05에서 과자 한 팩이 80g 이상이라고 할 수 있는가?</p></li><li><p>귀무가설: μ = 80, 대립가설: μ &gt; 80</p></li><li><p>검정통계량 Z = 2.33</p></li><li><p>검정통계량이 기각역 하에 있으면 귀무가설을 기각</p></li></ul></li><li><p>결과: 과자 한 팩 무게는 80g이라는 주장을 인정</p></li><li><p>통계검정에서 p-value는?</p><ul><li>p-value는 유의확률: 대립가설에 대한 증거의 정도</li><li>p-value가 작다는 것은 그 검정이 매우 유의하다는 증거<br>→ p-value는 변수의 유의성 정도 혹은 검정의 유의도를 나타냄</li></ul></li></ul><h3 id="검정의-오류"><a href="#검정의-오류" class="headerlink" title="검정의 오류"></a>검정의 오류</h3><ul><li>통계적 추정을 해석할 때 발생하는 오차<ul><li>Type 1 error: H<del>0</del>이 참인데 기각하는 오류</li><li>Type 2 error: H<del>0</del>이 거짓인데 채택하는 오류</li></ul></li></ul><p>|||H<del>0</del> true|H<del>0</del> false|<br>|—|—|—|<br>|H<del>0</del> accept|1-α|Type 2 error(β)|<br>|H<del>0</del> reject|Type 1 error(α)|Power(1-β)|</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅴ-통계검정방법&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="신뢰구간" scheme="https://ne-choi.github.io/tags/%EC%8B%A0%EB%A2%B0%EA%B5%AC%EA%B0%84/"/>
    
    <category term="t-test" scheme="https://ne-choi.github.io/tags/t-test/"/>
    
    <category term="p-value" scheme="https://ne-choi.github.io/tags/p-value/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 1: Ⅳ. 빅데이터 분석에서 확률과 분포</title>
    <link href="https://ne-choi.github.io/2021/01/07/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A3_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EC%97%90%EC%84%9C_%ED%99%95%EB%A5%A0%EA%B3%BC_%EB%B6%84%ED%8F%AC/"/>
    <id>https://ne-choi.github.io/2021/01/07/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A3_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EC%97%90%EC%84%9C_%ED%99%95%EB%A5%A0%EA%B3%BC_%EB%B6%84%ED%8F%AC/</id>
    <published>2021-01-07T00:00:00.000Z</published>
    <updated>2021-02-01T00:58:36.934Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.</li></ul><h1 id="Ⅳ-빅데이터-분석에서-확률과-분포"><a href="#Ⅳ-빅데이터-분석에서-확률과-분포" class="headerlink" title="Ⅳ. 빅데이터 분석에서 확률과 분포"></a>Ⅳ. 빅데이터 분석에서 확률과 분포</h1><h2 id="1-확률의-기초개념"><a href="#1-확률의-기초개념" class="headerlink" title="1. 확률의 기초개념"></a>1. 확률의 기초개념</h2><h3 id="통계에-확률-개념이-필요한-이유"><a href="#통계에-확률-개념이-필요한-이유" class="headerlink" title="통계에 확률 개념이 필요한 이유"></a>통계에 확률 개념이 필요한 이유</h3><ul><li><p><strong>통계(Statistics)</strong></p><ul><li>데이터를 수집, 처리, 분석, 활용하는 지식</li><li>실제 얻어진 데이터를 바탕으로 정보 도출</li></ul></li><li><p><strong>확률(Probabilities)</strong></p><ul><li>특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것</li><li>관측 전에 가능성을 논하는 것</li></ul></li><li><p>현실 세계는 매우 랜덤하여 미리 결과를 알 수 없음</p></li><li><p>단기적으로 어떤 사건이 일어날 비율은 매우 랜덤함</p></li><li><p>단, 장기적으로 어떤 사건이 일어날 가능성은 <strong>확률적으로 예측 가능</strong></p></li></ul><h3 id="확률-사건-표본공간"><a href="#확률-사건-표본공간" class="headerlink" title="확률, 사건, 표본공간"></a>확률, 사건, 표본공간</h3><ul><li><p>확률: 특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것</p></li><li><p>사건: 표본공간에서 관심 대상인 부분집합</p></li><li><p>표본공간: 확률실험의 모든 가능한 결과의 집합</p></li><li><p>P(A) = $\frac{사건 A가 일어나는 경우의 수}{모든 가능한 결과의 수}$</p><br></li><li><p>합집합사건: 사건 A 또는 사건 B가 일어날 때</p></li><li><p>교집합사건: 사건 A와 사건 B가 동시에 일어날 때</p></li><li><p>여집합사건: 표본공간 S에서 사건 A가 일어나지 않을 때</p></li><li><p>배반사건: 교집합사건이 공사건일 때, 사건 A와 B가 서로 배반(mutually exclusive)</p></li></ul><h3 id="확률변수와-기대값"><a href="#확률변수와-기대값" class="headerlink" title="확률변수와 기대값"></a>확률변수와 기대값</h3><ul><li><p>확률변수: 확률 실험에서 나타난 결과에 실수를 할당한 점수</p><table><thead><tr><th>표본공간</th><th>확률변수</th></tr></thead><tbody><tr><td>HH</td><td>0</td></tr><tr><td>HT</td><td>1</td></tr><tr><td>TH</td><td>1</td></tr><tr><td>TT</td><td>2</td></tr></tbody></table></li><li><p>기대값: 확률변수의 중심척도</p><ul><li>랜덤한 상황에서 수치로 나타난 결과가 A<del>1</del>, A<del>2</del>, …, A<del>k</del>이고 각 결과 확률이 P<del>1</del>, P<del>2</del>, …, P<del>k</del>면 기대값은 각 결과에 확률을 곱하여 전부 합한 것</li><li>기대값 = A<del>1</del>P<del>1</del> + A<del>2</del>P<del>2</del> + … + A<del>k</del>P<del>k</del></li></ul></li></ul><h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul><li>확률: 특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것</li><li>사건 하나하나를 미리 아는 것은 불가능하지만, 확률적 모형으로 많은 시행 결과 예측이 가능</li><li>표본공간과 사건을 밴다이어그램으로 나타내 특정 사건에 관한 확률을 구할 수 있음</li><li>확률변수: 확률 실험으로부터 나타난 결과에 실수를 할당한 함수</li></ul><h2 id="2-조건부-확률과-베이즈확률"><a href="#2-조건부-확률과-베이즈확률" class="headerlink" title="2. 조건부 확률과 베이즈확률"></a>2. 조건부 확률과 베이즈확률</h2><h3 id="조건부-확률과-통계적-독립"><a href="#조건부-확률과-통계적-독립" class="headerlink" title="조건부 확률과 통계적 독립"></a>조건부 확률과 통계적 독립</h3><ul><li><p>예제</p><ul><li>두 개의 주사위를 던져 두 눈 합이 10일 확률: 1/12</li><li>첫 번째 주사위 눈이 4라는 것을 아는 상황</li><li>두 눈의 합이 10일 확률은?</li><li>→ 확률이 1/6으로 바뀜<br></li></ul></li><li><p>조건부 확률(conditional probability)</p><ul><li>어떤 사건(B)이 발생한다는 조건 하에서 다른 사건(A)이 발생할 확률</li><li>P(A|B) = $\frac{P(A∩B)}{P(B)}$</li></ul></li><li><p>P(A|B) = P(A)일 때, 즉 사건 B가 일어난다는 정보가 사건 A 발생에 전혀 영향을 주지 않을 때, ‘두 사건이 통계적 독립(independent)’이라고 한다.</p></li></ul><h3 id="베이즈-정리"><a href="#베이즈-정리" class="headerlink" title="베이즈 정리"></a>베이즈 정리</h3><ul><li><p>베이즈 정리(Bayes’ Theorem)</p><ul><li>P(A<del>1</del>|B) = $\frac{P(B∩A<del>1</del>)}{P(B)}$<br>= $\frac{P(B|A<del>1</del>)P(A<del>1</del>)}{P(B)}$<br>= $\frac{[P(B|A<del>1</del>)P(A<del>1</del>)]}{P(B|A<del>1</del>)P(A<del>1</del>) + P(B|A<del>2</del>)P(A<del>2</del>)}$</li></ul></li><li><p>베이즈 정리도 아래와 같은 조건부 확률 계산식으로 볼 수 있음</p><ul><li>P(A<del>1</del>|B) = $\frac{P(B∩A<del>1</del>)}{P(B)}$<ul><li>사건 B가 발생했을 때, 사건 A<del>1</del>이 발생할 확률을 조건부 확률 공식으로 표현</li></ul></li><li>$\frac{P(B|A<del>1</del>)P(A<del>1</del>)}{P(B)}$<ul><li>P(B|A<del>1</del>)에 대한 조건부 확률 공식 이용</li></ul></li><li>$\frac{[P(B|A<del>1</del>)P(A<del>1</del>)]}{P(B|A<del>1</del>)P(A<del>1</del>) + P(B|A<del>2</del>)P(A<del>2</del>)}$<ul><li>P(B)를 P(B∩A<del>1</del>) + P(B∩A<del>2</del>)로 계산할 수 있음</li></ul></li></ul></li><li><p>주어진(사전정보) 가설에 새로운 정보(B)가 주어졌을 때, 사후확률을 계산함</p><ul><li>$\frac{[P(B|A<del>1</del>)P(A<del>1</del>)]}{P(B|A<del>1</del>)P(A<del>1</del>) + P(B|A<del>2</del>)P(A<del>2</del>)}$</li><li>P(B|A<del>1</del>): 가농도, P(A<del>1</del>): 사전확률</li></ul></li></ul><h3 id="요약-1"><a href="#요약-1" class="headerlink" title="요약"></a>요약</h3><ul><li>조건부 확률: 어떤 사건이 발생한다는 조건 하 다른 사건이 발생할 확률</li><li>베이즈 정리: 사후확률을 사전확률과 가능도를 이용해 계산할 수 있게 하는 확률 변환식</li><li>머신러닝기법 중, ‘나이브베이즈 분류’ 기법 계산에서 베이즈정리가 활용됨</li></ul><h2 id="3-정규분포-연속형-와-포아송분포-이산형"><a href="#3-정규분포-연속형-와-포아송분포-이산형" class="headerlink" title="3. 정규분포(연속형)와 포아송분포(이산형)"></a>3. 정규분포(연속형)와 포아송분포(이산형)</h2><h3 id="확률분포란"><a href="#확률분포란" class="headerlink" title="확률분포란?"></a>확률분포란?</h3><ul><li>확률분포에는 이산형(discrete) 분포와 연속형(continuous) 분포가 있음<ul><li>이산형 분포: 점이 띄엄띄엄 분포되어 있음</li><li>연속형 분포: 점이 연속적으로 분포되어 있음</li></ul></li></ul><h3 id="이산형-분포"><a href="#이산형-분포" class="headerlink" title="이산형 분포"></a>이산형 분포</h3><ul><li>확률변수가 이산형(discrete)일 때의 확률분포<ul><li>기대값 E(X) = Σx*p(x) ← 가중치 평균의 개념</li><li>분산 Var(X) = E(X^2^) - E(X)^2^</li></ul></li><li>이항분포, 다항분포, 초기하분포, 포아송분포 등</li></ul><ol><li><p><strong>이항분포</strong></p><ul><li>어떤 시행의 결과가 단순히 ‘성공’ 또는 ‘실패’로 나타날 때(베르누이 시행), 성공이 나오는 횟수에 대한 확률분포</li><li>성공화귤이 p인 베르누이시행을 n회 반복할 때 성공 횟수 X</li><li>E(X) = np</li><li>Var(X) = np(1-p)</li></ul></li><li><p><strong>포아송분포</strong></p><ul><li>단위 시간 안에 어떤 사건이 몇 번 발생하는가에 대한 확률분포</li><li>확률변수 X가 포아송확률변수이고, 모수(평균발생횟수)가 λ</li><li>E(X) = λ</li><li>Var(X) = λ</li></ul></li></ol><h3 id="연속형-분포"><a href="#연속형-분포" class="headerlink" title="연속형 분포"></a>연속형 분포</h3><ul><li>확률변수가 연속형(continuous)일 때의 확률분포</li><li>연속형 분포에서는 정규분포(Normal distribution)가 가장 중요함<ul><li>모집단의 분포가 정규분포를 가진다고 가정하면 통계 분석이 쉬워짐</li><li>사회적 자연적 현상 통계치의 분포가 정규분포와 비슷한 형태를 띔</li></ul></li></ul><ol><li><p><strong>표준정규분포</strong></p><ul><li>평균이 0이고 분산이 1인 정규분포</li><li>정규분포를 표준정규분포로 만드는 법<ul><li>X<del>N(μ,σ^2^) → Z = $\frac{X-μ}{σ}$</del>N(0,1)</li></ul></li><li>표준화를 하는 이유?<ul><li>표준정규분포에서의 구간 면적을 미리 구해두면 이를 이용해서 모든 정규분포 면적을 구할 수 있음</li></ul></li></ul></li><li><p><strong>카이제곱(x^2^)분포</strong></p><ul><li>확률변수 Z가 표준정규분포 N(0,1)을 따를 때, z^2^은 자유도가 1인 카이제곱분포를 따름</li></ul></li><li><p><strong>F-분포</strong></p><ul><li>두 확률변수 X<del>1</del>^2^과 x<del>2</del>^2^이 서로 독립이며, 각각의 자유도가 v<del>1</del>, v<del>2</del>인 카이제곱분포를 따를 때, 확률변수 F는 자유도가 (v<del>1</del>, v<del>2</del>)인 F-분포를 따름</li></ul></li></ol><h3 id="요약-2"><a href="#요약-2" class="headerlink" title="요약"></a>요약</h3><ul><li>이산형 분포: 확률변수가 이산형일 때의 확률분포</li><li>이항분포: 베르누이시행에서 ‘성공’이 나오는 횟수에 대한 확률분포</li><li>포아송 분포: 단위시간 안에 어떤 사건이 몇 번 발생하는가에 대한 확률분포</li><li>연속형 분포: 확률변수가 연속형일 때의 확률분포</li><li>정규분포: 정규분포는 평균을 중심으로 대칭을 이루는 종모양의 연속확률분포</li></ul><h2 id="4-데이터에서-출발하는-확률과-분포-중심극한"><a href="#4-데이터에서-출발하는-확률과-분포-중심극한" class="headerlink" title="4. 데이터에서 출발하는 확률과 분포(중심극한)"></a>4. 데이터에서 출발하는 확률과 분포(중심극한)</h2><h3 id="현실의-분포"><a href="#현실의-분포" class="headerlink" title="현실의 분포"></a>현실의 분포</h3><ul><li>현실의 다양한 분포 → 설명할 수 없는 분포 존재</li><li>중심극한정리(central limit theorem)</li></ul><h3 id="중심극한정리"><a href="#중심극한정리" class="headerlink" title="중심극한정리"></a>중심극한정리</h3><ul><li><p>이항분포에서 표본 수가 증가함에 따라 표본들의 전체 합이 점점 정규분포에 근접해짐</p></li><li><p>지수분포에서도 표본 수의 증가에 따른 표본평균의 분포가 점점 정규분포와 비슷해짐</p></li><li><p>원래의 분포가 정규분포가 아니더라도, 표본 수가 증가함에 따라 표본평균이 점점 정규분포모형과 비슷해짐</p></li></ul><h3 id="중심극한정리-정리"><a href="#중심극한정리-정리" class="headerlink" title="중심극한정리 정리"></a>중심극한정리 정리</h3><ul><li>모집단이 정규분포가 아닌 경우에도 표본 수가 증가하면 표본평균의 분포가 정규분포에 근접</li><li>평균이 μ이고 분산이 σ^2^인 모집단으로부터 크기 n인 확률표본을 추출할 때, n이 크면 표뵨평균 X는 N(μ, σ^2^/n)에 근접</li><li>보통 n이 30 이상이면 모집단의 분포에 관계 없이 X는 정규분포에 근사</li></ul><h3 id="중심극한정리가-유용한-이유"><a href="#중심극한정리가-유용한-이유" class="headerlink" title="중심극한정리가 유용한 이유"></a>중심극한정리가 유용한 이유</h3><ul><li>대부분의 통계적 검정과 추정은 모집단이 정규분포를 따른다는 가정 하에 이루어짐<ul><li>→ 모집단의 분포를 몰라도 중심극한정리를 이용하면 표본평균의 통계적 검정과 추정이 가능해짐</li></ul></li></ul><h3 id="요약-3"><a href="#요약-3" class="headerlink" title="요약"></a>요약</h3><ul><li>중심극한정리란 모집단의 분포에 관계 없이 표본의 수가 증가하면 표본평균의 분포가 정규분포에 근접한다는 이론</li><li>평균이 μ이고 분산이 σ^2^인 모집단으로부터 크기 n(≥ 30)인 확률표본을 추출할 때 표본평균 X는 N(μ, σ^2^/n)에 근접</li><li>모집단의 분포를 몰라도 중심극한정리를 이용하면 표본평균의 통계적 검정과 추정이 가능해짐</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅳ-빅데이터-분석에서-확률과-분포&quot;&gt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="통계" scheme="https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"/>
    
    <category term="데이터시각화" scheme="https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle: The future of ML(Americans vs Indians)</title>
    <link href="https://ne-choi.github.io/2021/01/07/Projects/Kaggle%20Competition-The_future_of_ML_americans_vs_indians/"/>
    <id>https://ne-choi.github.io/2021/01/07/Projects/Kaggle%20Competition-The_future_of_ML_americans_vs_indians/</id>
    <published>2021-01-07T00:00:00.000Z</published>
    <updated>2021-01-20T04:42:31.814Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.kaggle.com/nechoi/the-future-of-ml-americans-vs-indians">View to Kaggle</a></p><h1 id="The-future-of-ML-Americans-vs-Indians"><a href="#The-future-of-ML-Americans-vs-Indians" class="headerlink" title="The future of ML (Americans vs Indians)"></a><strong>The future of ML (Americans vs Indians)</strong></h1><p>Since I started Kaggle competitions, I have a question. I called it beginner’s curiosity.   </p><p><strong>“Have you ever thought about which countries have the biggest impact on Kaggle competition?”</strong>  </p><p>As everybody knows, USA is an obvious IT powerhouse along with Silicon Valley. It has the largest number of Kaggle Grandmaster Champions. There is no doubt that the USA has the most influence on Kaggle competitions.  </p><p>You can also see a lot of Indians on the list of participants. India, known as the rising IT powerhouse, is interested in the Kaggle competitions. It can be confirmed easily by checking a percentage of respondents to the 《2020 Kaggle Machine Learning &amp; data Science Survey》. The response rate for Indians is the highest about 29.2%(5,850 people). It is higher than the second-ranked country, which I would explain. </p><p>In addition, a few years ago, every country emphasized the importance of machine learning because It can be used in lots of fields in the world. I also wondered about the future of machine learning, so I decided to look into the future of Machine Learning through responses from two leader countries. Actually, if we look at the current trend of machine learning, we can easily discover it.  </p><p><strong>“Do you know the future of Machine Learning?”</strong>  </p><p>To sum up, I will compare the responses of American and Indian especially about Machine Learning in the order below.  </p><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><p><a href="#chapter1">1. Responses Rate by Country</a><br><a href="#chapter2">2. Percentage by age &amp; gender</a><br><a href="#chapter3">3. Education &amp; Job</a><br><a href="#chapter4">4. Development Environment</a><br><a href="#chapter5">5. Basic of Machine Learning</a><br><a href="#chapter6">6. CV &amp; NLP</a><br><a href="#chapter7">7. Machine Learning in the business</a><br><a href="#chapter8">8. Want to learn machine learning product</a>  </p><h2 id="Library-amp-Data-Import"><a href="#Library-amp-Data-Import" class="headerlink" title="Library &amp; Data Import"></a>Library &amp; Data Import</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualization</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rcParams</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.dpi&#x27;</span>] = <span class="number">200</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;lines.linewidth&#x27;</span>] = <span class="number">2</span></span><br><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> style</span><br><span class="line">style.use(<span class="string">&#x27;fivethirtyeight&#x27;</span>)</span><br><span class="line"><span class="keyword">import</span> plotly.express <span class="keyword">as</span> px</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_style(<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>, category=FutureWarning)</span><br><span class="line">warnings.filterwarnings(<span class="string">&quot;ignore&quot;</span>, category=DeprecationWarning)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">pd.set_option(<span class="string">&#x27;mode.chained_assignment&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">survey = pd.read_csv(<span class="string">&#x27;../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv&#x27;</span>)</span><br><span class="line">question = survey.iloc[<span class="number">0</span>,:].T</span><br><span class="line">full_df = survey.iloc[<span class="number">1</span>:,:]</span><br><span class="line"></span><br><span class="line">full_df[full_df.columns[<span class="number">3</span>]].replace(&#123;<span class="string">&#x27;United States of America&#x27;</span>:<span class="string">&#x27;USA&#x27;</span>,</span><br><span class="line">                                         <span class="string">&#x27;United Kingdom of Great Britain and Northern Ireland&#x27;</span>:<span class="string">&#x27;UK&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df = full_df[(full_df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;India&#x27;</span>)|(full_df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;USA&#x27;</span>)]</span><br><span class="line">df = df.iloc[<span class="number">1</span>:,:]</span><br><span class="line">df.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df = df[(df[<span class="string">&#x27;Q2&#x27;</span>]==<span class="string">&#x27;Man&#x27;</span>)|(df[<span class="string">&#x27;Q2&#x27;</span>]==<span class="string">&#x27;Woman&#x27;</span>)]</span><br><span class="line">df = df.iloc[<span class="number">1</span>:,:]</span><br><span class="line">df.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><pre><code>/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.  interactivity=interactivity, compiler=compiler, result=result)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Extract data by country</span></span><br><span class="line">India = df[df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;India&#x27;</span>]</span><br><span class="line">India = India.iloc[<span class="number">1</span>:,:]</span><br><span class="line">India.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">USA = df[df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;USA&#x27;</span>]</span><br><span class="line">USA = USA.iloc[<span class="number">1</span>:,:]</span><br><span class="line">USA.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">question.head()</span><br></pre></td></tr></table></figure><pre><code>Time from Start to Finish (seconds)                                Duration (in seconds)Q1                                                           What is your age (# years)?Q2                                                What is your gender? - Selected ChoiceQ3                                             In which country do you currently reside?Q4                                     What is the highest level of formal education ...Name: 0, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">full_df.head()</span><br></pre></td></tr></table></figure><h2 id="1-Responses-Rate-by-Country"><a href="#1-Responses-Rate-by-Country" class="headerlink" title="1. Responses Rate by Country "></a>1. Responses Rate by Country <a class="anchor" id="chapter1"></a></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">full_pie = px.pie(full_df, full_df.columns[<span class="number">3</span>],</span><br><span class="line">                 title=<span class="string">&#x27;Survey Responses Rate by Country&#x27;</span>,</span><br><span class="line">                 template=<span class="string">&#x27;seaborn&#x27;</span>, hole=<span class="number">0.5</span>)</span><br><span class="line">full_pie.update_traces(textposition=<span class="string">&#x27;inside&#x27;</span>, textinfo=<span class="string">&#x27;percent+label&#x27;</span>)</span><br><span class="line">full_pie.update_layout(uniformtext_minsize=<span class="number">10</span>, uniformtext_mode=<span class="string">&#x27;hide&#x27;</span>)</span><br><span class="line">full_pie.show()</span><br></pre></td></tr></table></figure><p>The response rate for Indians is the highest which is about 29.2% or 5,850 people. It is about 2.6 times higher than USA, a second-ranked country. Based on the response of the Americans and Indians for this report, I extracted the data by 2 using the two countries.</p><h2 id="Preprocessing"><a href="#Preprocessing" class="headerlink" title="Preprocessing"></a>Preprocessing</h2><p>To organize the data neatly, I pre-processed a gender column in which only females and males would remain. (146 responses will be deleted)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df = df[(df[df.columns[<span class="number">2</span>]]==<span class="string">&#x27;Man&#x27;</span>)|(df[df.columns[<span class="number">2</span>]]==<span class="string">&#x27;Woman&#x27;</span>)]</span><br><span class="line">df = df.iloc[<span class="number">1</span>:,:]</span><br><span class="line">df.reset_index(drop=<span class="literal">True</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df.shape</span><br></pre></td></tr></table></figure><pre><code>(7940, 355)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">&quot;- Valid American Response: &quot;</span>,<span class="built_in">len</span>(df[df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;USA&#x27;</span>]))</span><br><span class="line">print(<span class="string">&quot;- Valid Indian Response: &quot;</span>,<span class="built_in">len</span>(df[df[<span class="string">&#x27;Q3&#x27;</span>]==<span class="string">&#x27;India&#x27;</span>]))</span><br></pre></td></tr></table></figure><pre><code>- Valid American Response:  2161- Valid Indian Response:  5779</code></pre><p>I also made each data set(‘USA’, ‘India’) to make pie graphs.</p><h2 id="2-Percentage-by-age-amp-gender"><a href="#2-Percentage-by-age-amp-gender" class="headerlink" title="2. Percentage by age &amp; gender "></a>2. Percentage by age &amp; gender <a class="anchor" id="chapter2"></a></h2><p>To see their interest in coding, let’s look at the age and gender data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">legend_list = [<span class="string">&#x27;USA&#x27;</span>, <span class="string">&#x27;India&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;18-21&#x27;</span>,<span class="string">&#x27;22-24&#x27;</span>,<span class="string">&#x27;25-29&#x27;</span>,<span class="string">&#x27;30-34&#x27;</span>,<span class="string">&#x27;35-39&#x27;</span>,<span class="string">&#x27;40-44&#x27;</span>,<span class="string">&#x27;45-49&#x27;</span>,<span class="string">&#x27;50-54&#x27;</span>,<span class="string">&#x27;55-59&#x27;</span>,<span class="string">&#x27;60-69&#x27;</span>,<span class="string">&#x27;70+&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">2200</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">0</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;age group&#x27;</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Age Distribution&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q1_order = [<span class="string">&#x27;18-21&#x27;</span>,<span class="string">&#x27;22-24&#x27;</span>,<span class="string">&#x27;25-29&#x27;</span>,<span class="string">&#x27;30-34&#x27;</span>,<span class="string">&#x27;35-39&#x27;</span>,<span class="string">&#x27;40-44&#x27;</span>,<span class="string">&#x27;45-49&#x27;</span>,<span class="string">&#x27;50-54&#x27;</span>,<span class="string">&#x27;55-59&#x27;</span>,<span class="string">&#x27;60-69&#x27;</span>,<span class="string">&#x27;70+&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q1&quot;</span></span><br><span class="line">countplot_(df, col_name, q1_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_16_0.png" alt="png"></p><p>You can easily notice in USA the distribution of age groups is more regular than India. It represents the cording fever especially among young Indians. It can be interpreted that India has a craze for coding.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">q3_order = df[<span class="string">&#x27;Q3&#x27;</span>].value_counts()[:<span class="number">11</span>].index</span><br><span class="line">df_q2q3 = df[[<span class="string">&#x27;Q2&#x27;</span>,<span class="string">&#x27;Q3&#x27;</span>]].groupby(<span class="string">&#x27;Q3&#x27;</span>)[<span class="string">&#x27;Q2&#x27;</span>].value_counts().unstack().loc[q3_order]</span><br><span class="line">df_q2q3[<span class="string">&#x27;sum&#x27;</span>] = df_q2q3.<span class="built_in">sum</span>(axis=<span class="number">1</span>)</span><br><span class="line">df_q2q3_ratio = (df_q2q3.T / df_q2q3[<span class="string">&#x27;sum&#x27;</span>]).T[[<span class="string">&#x27;Man&#x27;</span>,<span class="string">&#x27;Woman&#x27;</span>]][::<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>,<span class="number">1</span>,figsize=(<span class="number">10</span>, <span class="number">1.5</span>))</span><br><span class="line"></span><br><span class="line">ax.barh(df_q2q3_ratio.index, df_q2q3_ratio[<span class="string">&#x27;Man&#x27;</span>], alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;Man&#x27;</span>)</span><br><span class="line">ax.barh(df_q2q3_ratio.index, df_q2q3_ratio[<span class="string">&#x27;Woman&#x27;</span>],</span><br><span class="line">        left=df_q2q3_ratio[<span class="string">&#x27;Man&#x27;</span>], alpha=<span class="number">0.5</span>, label=<span class="string">&#x27;Woman&#x27;</span>)</span><br><span class="line"></span><br><span class="line">ax.set_xlim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">ax.set_xticks([])</span><br><span class="line">ax.set_yticklabels(df_q2q3_ratio.index, fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># male percentage</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_q2q3_ratio.index:</span><br><span class="line">    ax.annotate(<span class="string">f&quot;<span class="subst">&#123;df_q2q3_ratio[<span class="string">&#x27;Man&#x27;</span>][i]*<span class="number">100</span>:<span class="number">.3</span>&#125;</span>%&quot;</span>, </span><br><span class="line">                   xy=(df_q2q3_ratio[<span class="string">&#x27;Man&#x27;</span>][i]/<span class="number">2</span>, i), fontsize=<span class="number">12</span>, va=<span class="string">&#x27;center&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> df_q2q3_ratio.index:</span><br><span class="line">    ax.annotate(<span class="string">f&quot;<span class="subst">&#123;df_q2q3_ratio[<span class="string">&#x27;Woman&#x27;</span>][i]*<span class="number">100</span>:<span class="number">.3</span>&#125;</span>%&quot;</span>, </span><br><span class="line">                   xy=(df_q2q3_ratio[<span class="string">&#x27;Man&#x27;</span>][i]+df_q2q3_ratio[<span class="string">&#x27;Woman&#x27;</span>][i]/<span class="number">2</span>, i),</span><br><span class="line">                   va=<span class="string">&#x27;center&#x27;</span>, ha=<span class="string">&#x27;center&#x27;</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    </span><br><span class="line">plt.title(<span class="string">&#x27;Gender Distribution&#x27;</span>, fontsize=<span class="number">20</span>)   </span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> [<span class="string">&#x27;top&#x27;</span>, <span class="string">&#x27;left&#x27;</span>, <span class="string">&#x27;right&#x27;</span>, <span class="string">&#x27;bottom&#x27;</span>]:</span><br><span class="line">    ax.spines[s].set_visible(<span class="literal">False</span>)</span><br><span class="line">    </span><br><span class="line">ax.legend(loc=<span class="string">&#x27;lower center&#x27;</span>, ncol=<span class="number">2</span>, bbox_to_anchor=(<span class="number">0.5</span>, <span class="number">-0.40</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_18_0.png" alt="png"></p><p>The gender ratio of two countries is alike. In both countries, male respondents are about 3.4 times more than female respondents.</p><h2 id="3-Education-amp-Job"><a href="#3-Education-amp-Job" class="headerlink" title="3. Education &amp; Job "></a>3. Education &amp; Job <a class="anchor" id="chapter3"></a></h2><p>Then I checked their education level and their current job.</p><h3 id="3-1-Level-of-Education"><a href="#3-1-Level-of-Education" class="headerlink" title="3-1. Level of Education"></a>3-1. Level of Education</h3><p>Please notice that I can just confirm their degree level, not major. However, many respondents study above bachelor’s degree, so I think it is okay to assume that their major is related to coding or programming.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;Q4&#x27;</span>] = df[<span class="string">&#x27;Q4&#x27;</span>].<span class="built_in">str</span>.replace(<span class="string">&quot;[^A-Za-z0-9-\s]+&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">df[<span class="string">&#x27;Q4&#x27;</span>].replace(&#123;<span class="string">&#x27;No formal education past high school&#x27;</span>:<span class="string">&#x27;~ High school&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;I prefer not to answer&#x27;</span>:<span class="string">&#x27;Not answer&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;Some collegeuniversity study without earning a bachelors degree&#x27;</span>:<span class="string">&#x27;Study without a BD&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;Masters degree&#x27;</span>:<span class="string">&quot;Master&#x27;s degree&quot;</span>,</span><br><span class="line">                   <span class="string">&#x27;Bachelors degree&#x27;</span>:<span class="string">&quot;Bachelor&#x27;s degree&quot;</span>,</span><br><span class="line">                   <span class="string">&#x27; High school&#x27;</span>:<span class="string">&#x27;~ High school&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;~ High school&#x27;</span>, <span class="string">&#x27;Professional degree&#x27;</span>, <span class="string">&#x27;Study without a BD&#x27;</span>, <span class="string">&quot;Bachelor&#x27;s degree&quot;</span>,<span class="string">&quot;Master&#x27;s degree&quot;</span>,<span class="string">&#x27;Doctoral degree&#x27;</span>, <span class="string">&#x27;Not answer&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">3200</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">15</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Level of Education &#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q4_order = [<span class="string">&#x27;~ High school&#x27;</span>, <span class="string">&#x27;Professional degree&#x27;</span>, <span class="string">&#x27;Study without a BD&#x27;</span>, <span class="string">&quot;Bachelor&#x27;s degree&quot;</span>,<span class="string">&quot;Master&#x27;s degree&quot;</span>,<span class="string">&#x27;Doctoral degree&#x27;</span>, <span class="string">&#x27;Not answer&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q4&quot;</span></span><br><span class="line">countplot_(df, col_name, q4_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_21_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_normalize4 = df.groupby([<span class="string">&#x27;Q3&#x27;</span>])[<span class="string">&#x27;Q4&#x27;</span>].value_counts(dropna=<span class="literal">False</span>, normalize=<span class="literal">True</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">df_normalize4 = pd.DataFrame(df_normalize4)</span><br><span class="line">df_normalize4</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>Q4</th>    </tr>    <tr>      <th>Q3</th>      <th>Q4</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="8" valign="top">India</th>      <th>Bachelor's degree</th>      <td>0.518602</td>    </tr>    <tr>      <th>Master's degree</th>      <td>0.316145</td>    </tr>    <tr>      <th>Doctoral degree</th>      <td>0.046548</td>    </tr>    <tr>      <th>Professional degree</th>      <td>0.036512</td>    </tr>    <tr>      <th>Study without a BD</th>      <td>0.035127</td>    </tr>    <tr>      <th>NaN</th>      <td>0.021284</td>    </tr>    <tr>      <th>Not answer</th>      <td>0.020938</td>    </tr>    <tr>      <th>~ High school</th>      <td>0.004845</td>    </tr>    <tr>      <th rowspan="8" valign="top">USA</th>      <th>Master's degree</th>      <td>0.469227</td>    </tr>    <tr>      <th>Bachelor's degree</th>      <td>0.256826</td>    </tr>    <tr>      <th>Doctoral degree</th>      <td>0.184174</td>    </tr>    <tr>      <th>Study without a BD</th>      <td>0.056918</td>    </tr>    <tr>      <th>Professional degree</th>      <td>0.017122</td>    </tr>    <tr>      <th>NaN</th>      <td>0.006941</td>    </tr>    <tr>      <th>~ High school</th>      <td>0.004627</td>    </tr>    <tr>      <th>Not answer</th>      <td>0.004165</td>    </tr>  </tbody></table></div><p>In the case of master and doctor, the percentage of Americans(46.9%, 18.4%) is higher than Indians (31.6%, 4.7%). Many Indians obtained their bachelor’s degree(51.9%) and I also noticed that 3.7% of the Indians had a professional degree.  </p><p>It shows that USA already has many IT technicians that are highly educated, and India’s level of coding education is growing.</p><h3 id="3-2-Current-Job"><a href="#3-2-Current-Job" class="headerlink" title="3-2. Current Job"></a>3-2. Current Job</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;Q5&#x27;</span>].replace(&#123;<span class="string">&#x27;Currently not employed&#x27;</span>:<span class="string">&#x27;Not employed&#x27;</span>&#125;,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;Business Analyst&#x27;</span>,<span class="string">&#x27;Data Analyst&#x27;</span>,<span class="string">&#x27;Data Engineer&#x27;</span>,<span class="string">&#x27;Data Scientist&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;DBA/Database Engineer&#x27;</span>,<span class="string">&#x27;Machine Learning Engineer&#x27;</span>,<span class="string">&#x27;Product/Project Manager&#x27;</span>,</span><br><span class="line">                             <span class="string">&#x27;Research Scientist&#x27;</span>,<span class="string">&#x27;Software Engineer&#x27;</span>,<span class="string">&#x27;Statistician&#x27;</span>,<span class="string">&#x27;Student&#x27;</span>,<span class="string">&#x27;Not employed&#x27;</span>,<span class="string">&#x27;Other&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">2400</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">20</span>, fontsize=<span class="number">6</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper left&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Current Job&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q5_order = [<span class="string">&#x27;Business Analyst&#x27;</span>,<span class="string">&#x27;Data Analyst&#x27;</span>,<span class="string">&#x27;Data Engineer&#x27;</span>,<span class="string">&#x27;Data Scientist&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;DBA/Database Engineer&#x27;</span>,<span class="string">&#x27;Machine Learning Engineer&#x27;</span>,<span class="string">&#x27;Product/Project Manager&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;Research Scientist&#x27;</span>,<span class="string">&#x27;Software Engineer&#x27;</span>,<span class="string">&#x27;Statistician&#x27;</span>,<span class="string">&#x27;Student&#x27;</span>,<span class="string">&#x27;Not employed&#x27;</span>,<span class="string">&#x27;Other&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q5&quot;</span></span><br><span class="line">countplot_(df, col_name, q5_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_25_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_normalize5 = df.groupby([<span class="string">&#x27;Q3&#x27;</span>])[<span class="string">&#x27;Q5&#x27;</span>].value_counts(dropna=<span class="literal">False</span>, normalize=<span class="literal">True</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">df_normalize5 = pd.DataFrame(df_normalize5)</span><br><span class="line">df_normalize5</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>Q5</th>    </tr>    <tr>      <th>Q3</th>      <th>Q5</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="14" valign="top">India</th>      <th>Student</th>      <td>0.383803</td>    </tr>    <tr>      <th>Software Engineer</th>      <td>0.102094</td>    </tr>    <tr>      <th>Data Scientist</th>      <td>0.099844</td>    </tr>    <tr>      <th>Not employed</th>      <td>0.097076</td>    </tr>    <tr>      <th>Other</th>      <td>0.069389</td>    </tr>    <tr>      <th>Data Analyst</th>      <td>0.058661</td>    </tr>    <tr>      <th>Machine Learning Engineer</th>      <td>0.050874</td>    </tr>    <tr>      <th>NaN</th>      <td>0.035473</td>    </tr>    <tr>      <th>Business Analyst</th>      <td>0.033916</td>    </tr>    <tr>      <th>Research Scientist</th>      <td>0.023707</td>    </tr>    <tr>      <th>Product/Project Manager</th>      <td>0.021457</td>    </tr>    <tr>      <th>Data Engineer</th>      <td>0.013497</td>    </tr>    <tr>      <th>Statistician</th>      <td>0.005883</td>    </tr>    <tr>      <th>DBA/Database Engineer</th>      <td>0.004326</td>    </tr>    <tr>      <th rowspan="14" valign="top">USA</th>      <th>Data Scientist</th>      <td>0.173994</td>    </tr>    <tr>      <th>Student</th>      <td>0.156872</td>    </tr>    <tr>      <th>Other</th>      <td>0.132346</td>    </tr>    <tr>      <th>Software Engineer</th>      <td>0.097640</td>    </tr>    <tr>      <th>Data Analyst</th>      <td>0.086997</td>    </tr>    <tr>      <th>Not employed</th>      <td>0.074965</td>    </tr>    <tr>      <th>Research Scientist</th>      <td>0.061083</td>    </tr>    <tr>      <th>Product/Project Manager</th>      <td>0.052753</td>    </tr>    <tr>      <th>Business Analyst</th>      <td>0.047200</td>    </tr>    <tr>      <th>Machine Learning Engineer</th>      <td>0.043961</td>    </tr>    <tr>      <th>Data Engineer</th>      <td>0.031004</td>    </tr>    <tr>      <th>Statistician</th>      <td>0.017584</td>    </tr>    <tr>      <th>NaN</th>      <td>0.016659</td>    </tr>    <tr>      <th>DBA/Database Engineer</th>      <td>0.006941</td>    </tr>  </tbody></table></div><p>Most noticeably, more than 38% of Indians are students. That’s because 61.2% of the respondents in India are under 25. In the USA, there are lots of Data Scientists(17.4%), and Software Engineers(9.8%). Likewise, India has a lot of Sotfware Engineers(10.2%), and Data Scientists(10.0%). The results of the fewest jobs are the same, too. Both of the countries have few DBA/Database Engineers and Statisticians.</p><h2 id="4-Development-Environment"><a href="#4-Development-Environment" class="headerlink" title="4. Development Environment "></a>4. Development Environment <a class="anchor" id="chapter4"></a></h2><p>In this part, I examined a more detailed part of the development environment. Compared to the period of coding, I discovered each country’s interest in writing code or programming. Furthermore, I will check their basic programming languages and the languages recommended by people who already used it.</p><h3 id="4-1-Period-of-coding"><a href="#4-1-Period-of-coding" class="headerlink" title="4-1. Period of coding"></a>4-1. Period of coding</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;Q6&#x27;</span>].replace(&#123;<span class="string">&#x27;I have never written code&#x27;</span>:<span class="string">&#x27;Never&#x27;</span>,</span><br><span class="line">                  <span class="string">&#x27;&lt; 1 years&#x27;</span>:<span class="string">&#x27;&lt; 1 year&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;&lt; 1 year&#x27;</span>,<span class="string">&#x27;1-2 years&#x27;</span>,<span class="string">&#x27;3-5 years&#x27;</span>,<span class="string">&#x27;5-10 years&#x27;</span>,<span class="string">&#x27;10-20 years&#x27;</span>,<span class="string">&#x27;20+ years&#x27;</span>,<span class="string">&#x27;Never&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">1850</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">0</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Period of Coding&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q6_order = [<span class="string">&#x27;&lt; 1 year&#x27;</span>,<span class="string">&#x27;1-2 years&#x27;</span>,<span class="string">&#x27;3-5 years&#x27;</span>,<span class="string">&#x27;5-10 years&#x27;</span>,<span class="string">&#x27;10-20 years&#x27;</span>,<span class="string">&#x27;20+ years&#x27;</span>,<span class="string">&#x27;Never&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q6&quot;</span></span><br><span class="line">countplot_(df, col_name, q6_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_normalize6 = df.groupby([<span class="string">&#x27;Q3&#x27;</span>])[<span class="string">&#x27;Q6&#x27;</span>].value_counts(dropna=<span class="literal">False</span>, normalize=<span class="literal">True</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">df_normalize6 = pd.DataFrame(df_normalize6)</span><br><span class="line">df_normalize6</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>Q6</th>    </tr>    <tr>      <th>Q3</th>      <th>Q6</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="8" valign="top">India</th>      <th>1-2 years</th>      <td>0.305416</td>    </tr>    <tr>      <th>3-5 years</th>      <td>0.243295</td>    </tr>    <tr>      <th>&lt; 1 year</th>      <td>0.198131</td>    </tr>    <tr>      <th>5-10 years</th>      <td>0.091711</td>    </tr>    <tr>      <th>Never</th>      <td>0.061429</td>    </tr>    <tr>      <th>10-20 years</th>      <td>0.045856</td>    </tr>    <tr>      <th>NaN</th>      <td>0.043087</td>    </tr>    <tr>      <th>20+ years</th>      <td>0.011075</td>    </tr>    <tr>      <th rowspan="8" valign="top">USA</th>      <th>3-5 years</th>      <td>0.229986</td>    </tr>    <tr>      <th>5-10 years</th>      <td>0.174456</td>    </tr>    <tr>      <th>20+ years</th>      <td>0.164276</td>    </tr>    <tr>      <th>1-2 years</th>      <td>0.143915</td>    </tr>    <tr>      <th>10-20 years</th>      <td>0.136974</td>    </tr>    <tr>      <th>&lt; 1 year</th>      <td>0.089311</td>    </tr>    <tr>      <th>Never</th>      <td>0.037483</td>    </tr>    <tr>      <th>NaN</th>      <td>0.023600</td>    </tr>  </tbody></table></div><p>In the case of the USA respondents, the number of people who have never coded nor programmed is less than that of other groups. It show us that Americans have always been interested in it. More than 30.1% of the respondents are experts who have coded for more than 10 years. Additionally, it is noticeable that the number of beginners is decreasing.</p><p>India, on the other hand, has about 6.1% of respondents who have never done coding nor programming. Plus, there are numerous beginners. Almost 74.6% of Indians studied coding for less than 5 years, and about half of the respondents in India coded for less than 3 years.</p><p>This graph also shows that there are already lots of coding masters in the USA, and that there are many beginners in India.</p><h3 id="4-2-Basic-programming-language-multiple"><a href="#4-2-Basic-programming-language-multiple" class="headerlink" title="4-2. Basic programming language (multiple)"></a>4-2. Basic programming language (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess Basic programming language data</span></span><br><span class="line">USA7 = (USA[<span class="string">&#x27;Q7_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_3&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_4&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q7_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_6&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_7&#x27;</span>], USA[<span class="string">&#x27;Q7_Part_8&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q7_OTHER&#x27;</span>])</span><br><span class="line">USA7 = pd.concat(USA7)</span><br><span class="line"></span><br><span class="line">India7 = (India[<span class="string">&#x27;Q7_Part_1&#x27;</span>], India[<span class="string">&#x27;Q7_Part_2&#x27;</span>], India[<span class="string">&#x27;Q7_Part_3&#x27;</span>], India[<span class="string">&#x27;Q7_Part_4&#x27;</span>],</span><br><span class="line">          India[<span class="string">&#x27;Q7_Part_5&#x27;</span>], India[<span class="string">&#x27;Q7_Part_6&#x27;</span>], India[<span class="string">&#x27;Q7_Part_7&#x27;</span>], India[<span class="string">&#x27;Q7_Part_8&#x27;</span>],</span><br><span class="line">          India[<span class="string">&#x27;Q7_OTHER&#x27;</span>])</span><br><span class="line">India7 = pd.concat(India7)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">USA7.replace(&#123;<span class="string">&#x27;General purpose image/video tools (PIL, cv2, skimage, etc)&#x27;</span>:<span class="string">&#x27;General image/video&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Image segmentation methods (U-Net, Mask R-CNN, etc)&#x27;</span>:<span class="string">&#x27;Image segmentation methods&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Object detection methods (YOLOv3, RetinaNet, etc)&#x27;</span>:<span class="string">&#x27;Object detection methods&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)&#x27;</span>:<span class="string">&#x27;Image classification &amp; networks&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Generative Networks (GAN, VAE, etc)&#x27;</span>:<span class="string">&#x27;Generative Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">               </span><br><span class="line">India7.replace(&#123;<span class="string">&#x27;General purpose image/video tools (PIL, cv2, skimage, etc)&#x27;</span>:<span class="string">&#x27;General image/video&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Image segmentation methods (U-Net, Mask R-CNN, etc)&#x27;</span>:<span class="string">&#x27;Image segmentation methods&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Object detection methods (YOLOv3, RetinaNet, etc)&#x27;</span>:<span class="string">&#x27;Object detection methods&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)&#x27;</span>:<span class="string">&#x27;Image classification &amp; networks&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Generative Networks (GAN, VAE, etc)&#x27;</span>:<span class="string">&#x27;Generative Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)   </span><br><span class="line"></span><br><span class="line">USA7.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Basic Program Language in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India7.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Basic Program Language in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_34_0.png" alt="png"></p><p>Both countries use Python the most. The next is SQL. A noticeable difference is shown in the 3rd most used language. In the USA, 14.6% of respondents use R, but in India the 3rd most used programming language is C. In India, R is the third- lowest programming language used whereas in the USA, C is the second-lowest.</p><p>Nevertheless, it is clear that the most commonly used languages of both countries are Python and SQL.</p><h3 id="4-3-Recommend-programming-language-for-starter"><a href="#4-3-Recommend-programming-language-for-starter" class="headerlink" title="4-3. Recommend programming language for starter"></a>4-3. Recommend programming language for starter</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">4500</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">0</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Recommend Language for Starter&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q8_order = []</span><br><span class="line">col_name = <span class="string">&quot;Q8&quot;</span></span><br><span class="line">countplot_(df, col_name, q8_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_37_0.png" alt="png"></p><p>The graph shows that Python has an overwhelming proportion. What is unusual is that Indians who use relatively less R recommended R.</p><h3 id="4-4-Integrated-development-environments-multiple"><a href="#4-4-Integrated-development-environments-multiple" class="headerlink" title="4-4. Integrated development environments (multiple)"></a>4-4. Integrated development environments (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess integrated development environments data</span></span><br><span class="line">USA9 = (USA[<span class="string">&#x27;Q9_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_3&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_4&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q9_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_6&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_7&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_8&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q9_Part_9&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_10&#x27;</span>], USA[<span class="string">&#x27;Q9_Part_11&#x27;</span>], USA[<span class="string">&#x27;Q9_OTHER&#x27;</span>])</span><br><span class="line">USA9 = pd.concat(USA9)</span><br><span class="line"></span><br><span class="line">India9 = (India[<span class="string">&#x27;Q9_Part_1&#x27;</span>], India[<span class="string">&#x27;Q9_Part_2&#x27;</span>], India[<span class="string">&#x27;Q9_Part_3&#x27;</span>], India[<span class="string">&#x27;Q9_Part_4&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q9_Part_5&#x27;</span>], India[<span class="string">&#x27;Q9_Part_6&#x27;</span>], India[<span class="string">&#x27;Q9_Part_7&#x27;</span>], India[<span class="string">&#x27;Q9_Part_8&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q9_Part_9&#x27;</span>], India[<span class="string">&#x27;Q9_Part_10&#x27;</span>], India[<span class="string">&#x27;Q9_Part_11&#x27;</span>], India[<span class="string">&#x27;Q9_OTHER&#x27;</span>])</span><br><span class="line">India9 = pd.concat(India9)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">USA9.replace(&#123;<span class="string">&#x27;Jupyter (JupyterLab, Jupyter Notebooks, etc) &#x27;</span>:<span class="string">&#x27;Jupyter&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;Visual Studio Code (VSCode)&#x27;</span>:<span class="string">&#x27;VSCode&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">India9.replace(&#123;<span class="string">&#x27;Jupyter (JupyterLab, Jupyter Notebooks, etc) &#x27;</span>:<span class="string">&#x27;Jupyter&#x27;</span>,</span><br><span class="line">              <span class="string">&#x27;Visual Studio Code (VSCode)&#x27;</span>:<span class="string">&#x27;VSCode&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    </span><br><span class="line">USA9.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Using IDE in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India9.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Using IDE in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_41_0.png" alt="png"></p><p>The development environment used frequently is similar except for RStudio. As we saw in 4-2, the reason why the ratio of RStudio in India is not high is Indian less use R.</p><ul><li><h2 id="5-Basic-of-Machine-Learning"><a href="#5-Basic-of-Machine-Learning" class="headerlink" title="5. Basic of Machine Learning "></a>5. Basic of Machine Learning <a class="anchor" id="chapter5"></a></h2></li></ul><p>It’s time to check the machine learning section. First, I check to period of using machine learning in each country. And then, let’s look at the machine learning framework and algorithms that they use.</p><h3 id="5-1-Period-of-Using-Machine-Learning"><a href="#5-1-Period-of-Using-Machine-Learning" class="headerlink" title="5-1. Period of Using Machine Learning"></a>5-1. Period of Using Machine Learning</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">&#x27;Q15&#x27;</span>].replace(&#123;<span class="string">&#x27;Under 1 year&#x27;</span>:<span class="string">&#x27;&lt; 1 year&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;20 or more years&#x27;</span>:<span class="string">&#x27;20+ years&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;I do not use machine learning methods&#x27;</span>:<span class="string">&#x27;Never&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;&lt; 1 year&#x27;</span>,<span class="string">&#x27;1-2 years&#x27;</span>,<span class="string">&#x27;2-3 years&#x27;</span>,<span class="string">&#x27;3-4 years&#x27;</span>,<span class="string">&#x27;4-5 years&#x27;</span>,<span class="string">&#x27;5-10 years&#x27;</span>,<span class="string">&#x27;10-20 years&#x27;</span>,<span class="string">&#x27;20+ years&#x27;</span>,<span class="string">&#x27;Never&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">2400</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">0</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Period of Using Machine Learning&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q15_order = [<span class="string">&#x27;&lt; 1 year&#x27;</span>,<span class="string">&#x27;1-2 years&#x27;</span>,<span class="string">&#x27;2-3 years&#x27;</span>,<span class="string">&#x27;3-4 years&#x27;</span>,<span class="string">&#x27;4-5 years&#x27;</span>,<span class="string">&#x27;5-10 years&#x27;</span>,<span class="string">&#x27;10-20 years&#x27;</span>,<span class="string">&#x27;20+ years&#x27;</span>,<span class="string">&#x27;Never&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q15&quot;</span></span><br><span class="line">countplot_(df, col_name, q15_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_44_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_normalize15 = df.groupby([<span class="string">&#x27;Q3&#x27;</span>])[<span class="string">&#x27;Q15&#x27;</span>].value_counts(dropna=<span class="literal">False</span>, normalize=<span class="literal">True</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">df_normalize15 = pd.DataFrame(df_normalize15)</span><br><span class="line">df_normalize15</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>Q15</th>    </tr>    <tr>      <th>Q3</th>      <th>Q15</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="10" valign="top">India</th>      <th>&lt; 1 year</th>      <td>0.395224</td>    </tr>    <tr>      <th>NaN</th>      <td>0.201938</td>    </tr>    <tr>      <th>1-2 years</th>      <td>0.179270</td>    </tr>    <tr>      <th>Never</th>      <td>0.100536</td>    </tr>    <tr>      <th>2-3 years</th>      <td>0.060391</td>    </tr>    <tr>      <th>3-4 years</th>      <td>0.025091</td>    </tr>    <tr>      <th>4-5 years</th>      <td>0.017996</td>    </tr>    <tr>      <th>5-10 years</th>      <td>0.015747</td>    </tr>    <tr>      <th>10-20 years</th>      <td>0.003461</td>    </tr>    <tr>      <th>20+ years</th>      <td>0.000346</td>    </tr>    <tr>      <th rowspan="10" valign="top">USA</th>      <th>&lt; 1 year</th>      <td>0.213790</td>    </tr>    <tr>      <th>1-2 years</th>      <td>0.159186</td>    </tr>    <tr>      <th>NaN</th>      <td>0.127256</td>    </tr>    <tr>      <th>Never</th>      <td>0.116613</td>    </tr>    <tr>      <th>2-3 years</th>      <td>0.109671</td>    </tr>    <tr>      <th>5-10 years</th>      <td>0.084683</td>    </tr>    <tr>      <th>3-4 years</th>      <td>0.071263</td>    </tr>    <tr>      <th>4-5 years</th>      <td>0.068024</td>    </tr>    <tr>      <th>10-20 years</th>      <td>0.027765</td>    </tr>    <tr>      <th>20+ years</th>      <td>0.021749</td>    </tr>  </tbody></table></div><p>The results of this graph shows the two countries similar. In the USA, many people have been coding for a long time, but there are many beginners in machine learning. Expecially in India, there are about 57.4% of respondents using machine learning. It can be said that <strong>India’s coding craze began with the development of machine learning.</strong></p><h3 id="5-2-Using-machine-learning-framework-multiple"><a href="#5-2-Using-machine-learning-framework-multiple" class="headerlink" title="5-2. Using machine learning framework (multiple)"></a>5-2. Using machine learning framework (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess machine learning framework data</span></span><br><span class="line">USA16 = (USA[<span class="string">&#x27;Q16_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_3&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_4&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q16_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_6&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_7&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_8&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q16_Part_9&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_10&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_11&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_12&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q16_Part_13&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_14&#x27;</span>], USA[<span class="string">&#x27;Q16_Part_15&#x27;</span>], USA[<span class="string">&#x27;Q16_OTHER&#x27;</span>])</span><br><span class="line">USA16 = pd.concat(USA16)</span><br><span class="line"></span><br><span class="line">India16 = (India[<span class="string">&#x27;Q16_Part_1&#x27;</span>], India[<span class="string">&#x27;Q16_Part_2&#x27;</span>], India[<span class="string">&#x27;Q16_Part_3&#x27;</span>], India[<span class="string">&#x27;Q16_Part_4&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q16_Part_5&#x27;</span>], India[<span class="string">&#x27;Q16_Part_6&#x27;</span>], India[<span class="string">&#x27;Q16_Part_7&#x27;</span>], India[<span class="string">&#x27;Q16_Part_8&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q16_Part_9&#x27;</span>], India[<span class="string">&#x27;Q16_Part_10&#x27;</span>], India[<span class="string">&#x27;Q16_Part_11&#x27;</span>], India[<span class="string">&#x27;Q16_Part_12&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q16_Part_13&#x27;</span>], India[<span class="string">&#x27;Q16_Part_14&#x27;</span>], India[<span class="string">&#x27;Q16_Part_15&#x27;</span>], India[<span class="string">&#x27;Q16_OTHER&#x27;</span>])</span><br><span class="line">India16 = pd.concat(India16)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">13</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">    </span><br><span class="line">USA16.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Machine Learning Framework in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India16.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Machine Learning Framework in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_49_0.png" alt="png"></p><p>The ranking of machine learning framework of both countries is almost similar. There is a difference between the rank 4th and 5th graphs  which is PyTorch and Xgboost. The most popular framework used is Scikit-learn, with about 25.9% in the USA and about 27.6% in India. TensorFlow and Keras showed about 14.5%, and about 13.4% in the USA and in India about 19.0% and about 17.3% are observed. It shows that more than three quarters of people uses 5 types of framework and one quarter of people uses various framework which occupy less than about 10% each.</p><h3 id="5-3-Using-machine-learning-algorithm-multiple"><a href="#5-3-Using-machine-learning-algorithm-multiple" class="headerlink" title="5-3. Using machine learning algorithm (multiple)"></a>5-3. Using machine learning algorithm (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess machine learning algorithm data</span></span><br><span class="line">USA17 = (USA[<span class="string">&#x27;Q17_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_3&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_4&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q17_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_6&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_7&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_8&#x27;</span>],</span><br><span class="line">        USA[<span class="string">&#x27;Q17_Part_9&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_10&#x27;</span>], USA[<span class="string">&#x27;Q17_Part_11&#x27;</span>], USA[<span class="string">&#x27;Q17_OTHER&#x27;</span>])</span><br><span class="line">USA17 = pd.concat(USA17)</span><br><span class="line"></span><br><span class="line">India17 = (India[<span class="string">&#x27;Q17_Part_1&#x27;</span>], India[<span class="string">&#x27;Q17_Part_2&#x27;</span>], India[<span class="string">&#x27;Q17_Part_3&#x27;</span>], India[<span class="string">&#x27;Q17_Part_4&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q17_Part_5&#x27;</span>], India[<span class="string">&#x27;Q17_Part_6&#x27;</span>], India[<span class="string">&#x27;Q17_Part_7&#x27;</span>], India[<span class="string">&#x27;Q17_Part_8&#x27;</span>],</span><br><span class="line">        India[<span class="string">&#x27;Q17_Part_9&#x27;</span>], India[<span class="string">&#x27;Q17_Part_10&#x27;</span>], India[<span class="string">&#x27;Q17_Part_11&#x27;</span>], India[<span class="string">&#x27;Q17_OTHER&#x27;</span>])</span><br><span class="line">India17 = pd.concat(India17)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">USA17.replace(&#123;<span class="string">&#x27;Gradient Boosting Machines (xgboost, lightgbm, etc)&#x27;</span>:<span class="string">&#x27;Gradient Boosting Machines&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Dense Neural Networks (MLPs, etc)&#x27;</span>:<span class="string">&#x27;Dense Neural Networks&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Transformer Networks (BERT, gpt-3, etc)&#x27;</span>:<span class="string">&#x27;Transformer Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">India17.replace(&#123;<span class="string">&#x27;Gradient Boosting Machines (xgboost, lightgbm, etc)&#x27;</span>:<span class="string">&#x27;Gradient Boosting Machines&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Dense Neural Networks (MLPs, etc)&#x27;</span>:<span class="string">&#x27;Dense Neural Networks&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Transformer Networks (BERT, gpt-3, etc)&#x27;</span>:<span class="string">&#x27;Transformer Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">USA17.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Machine Learning Algorithm in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India17.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Machine Learning Algorithm in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_53_0.png" alt="png"></p><p>The ranking of used machine learning algorithm of both countries is almost similar, too. Linear or logistic regression and decision trees or random forests are most popular in two countries. And there are no significant thing because the result of both countries is alike.</p><h2 id="6-CV-amp-NLP"><a href="#6-CV-amp-NLP" class="headerlink" title="6. CV &amp; NLP "></a>6. CV &amp; NLP <a class="anchor" id="chapter6"></a></h2><p>Then, I check about Computer Vision and Natural Language Processing because it is importance part of machine learning. Therefore I assumed lots of beginners and exports are interested in this field. In this part, the difference between the USA and India is not visible clearly, so I just show you the ratio graphs.</p><h3 id="6-1-Computer-Vision-multiple"><a href="#6-1-Computer-Vision-multiple" class="headerlink" title="6-1. Computer Vision (multiple)"></a>6-1. Computer Vision (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess CV data</span></span><br><span class="line">USA18 = (USA[<span class="string">&#x27;Q18_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q18_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q18_Part_3&#x27;</span>],</span><br><span class="line">                  USA[<span class="string">&#x27;Q18_Part_4&#x27;</span>], USA[<span class="string">&#x27;Q18_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q18_Part_6&#x27;</span>],</span><br><span class="line">                  USA[<span class="string">&#x27;Q18_OTHER&#x27;</span>])</span><br><span class="line">USA18 = pd.concat(USA18)</span><br><span class="line"></span><br><span class="line">India18 = (India[<span class="string">&#x27;Q18_Part_1&#x27;</span>], India[<span class="string">&#x27;Q18_Part_2&#x27;</span>], India[<span class="string">&#x27;Q18_Part_3&#x27;</span>],</span><br><span class="line">                  India[<span class="string">&#x27;Q18_Part_4&#x27;</span>], India[<span class="string">&#x27;Q18_Part_5&#x27;</span>], India[<span class="string">&#x27;Q18_Part_6&#x27;</span>],</span><br><span class="line">                  India[<span class="string">&#x27;Q18_OTHER&#x27;</span>])</span><br><span class="line">India18 = pd.concat(India18)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Preprocess NLP data</span></span><br><span class="line">USA19 = (USA[<span class="string">&#x27;Q19_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q19_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q19_Part_3&#x27;</span>],</span><br><span class="line">                  USA[<span class="string">&#x27;Q19_Part_4&#x27;</span>], USA[<span class="string">&#x27;Q19_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q19_OTHER&#x27;</span>])</span><br><span class="line">USA19 = pd.concat(USA19)</span><br><span class="line"></span><br><span class="line">India19 = (India[<span class="string">&#x27;Q19_Part_1&#x27;</span>], India[<span class="string">&#x27;Q19_Part_2&#x27;</span>], India[<span class="string">&#x27;Q19_Part_3&#x27;</span>],</span><br><span class="line">                  India[<span class="string">&#x27;Q19_Part_4&#x27;</span>], India[<span class="string">&#x27;Q19_Part_5&#x27;</span>], India[<span class="string">&#x27;Q19_OTHER&#x27;</span>])</span><br><span class="line">India19 = pd.concat(India19)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">USA18.replace(&#123;<span class="string">&#x27;General purpose image/video tools (PIL, cv2, skimage, etc)&#x27;</span>:<span class="string">&#x27;General image/video&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Image segmentation methods (U-Net, Mask R-CNN, etc)&#x27;</span>:<span class="string">&#x27;Image segmentation methods&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Object detection methods (YOLOv3, RetinaNet, etc)&#x27;</span>:<span class="string">&#x27;Object detection methods&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)&#x27;</span>:<span class="string">&#x27;Image classification &amp; networks&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Generative Networks (GAN, VAE, etc)&#x27;</span>:<span class="string">&#x27;Generative Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line">               </span><br><span class="line">India18.replace(&#123;<span class="string">&#x27;General purpose image/video tools (PIL, cv2, skimage, etc)&#x27;</span>:<span class="string">&#x27;General image/video&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Image segmentation methods (U-Net, Mask R-CNN, etc)&#x27;</span>:<span class="string">&#x27;Image segmentation methods&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Object detection methods (YOLOv3, RetinaNet, etc)&#x27;</span>:<span class="string">&#x27;Object detection methods&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)&#x27;</span>:<span class="string">&#x27;Image classification &amp; networks&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Generative Networks (GAN, VAE, etc)&#x27;</span>:<span class="string">&#x27;Generative Networks&#x27;</span>&#125;, inplace=<span class="literal">True</span>)   </span><br><span class="line">    </span><br><span class="line">USA18.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;CV Method in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India18.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;CV Method in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_57_0.png" alt="png"></p><h3 id="6-2-Natural-Language-Processing-multiple"><a href="#6-2-Natural-Language-Processing-multiple" class="headerlink" title="6-2. Natural Language Processing (multiple)"></a>6-2. Natural Language Processing (multiple)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">USA19.replace(&#123;<span class="string">&#x27;Word embeddings/vectors (GLoVe, fastText, word2vec)&#x27;</span>:<span class="string">&#x27;Word embeddings/vectors&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Encoder-decorder models (seq2seq, vanilla transformers)&#x27;</span>:<span class="string">&#x27;Encoder-decoder models&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Contextualized embeddings (ELMo, CoVe)&#x27;</span>:<span class="string">&#x27;Contextualized embeddings&#x27;</span>,</span><br><span class="line">               <span class="string">&#x27;Transformer language models (GPT-3, BERT, XLnet, etc)&#x27;</span>:<span class="string">&#x27;Transformer language models&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">India19.replace(&#123;<span class="string">&#x27;Word embeddings/vectors (GLoVe, fastText, word2vec)&#x27;</span>:<span class="string">&#x27;Word embeddings/vectors&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Encoder-decorder models (seq2seq, vanilla transformers)&#x27;</span>:<span class="string">&#x27;Encoder-decoder models&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Contextualized embeddings (ELMo, CoVe)&#x27;</span>:<span class="string">&#x27;Contextualized embeddings&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;Transformer language models (GPT-3, BERT, XLnet, etc)&#x27;</span>:<span class="string">&#x27;Transformer language models&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">USA19.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;NLP Method in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India19.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;NLP Method in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_59_0.png" alt="png"></p><h2 id="7-Machine-Learning-in-the-business"><a href="#7-Machine-Learning-in-the-business" class="headerlink" title="7. Machine Learning in the business "></a>7. Machine Learning in the business <a class="anchor" id="chapter7"></a></h2><p>I wondered if the company actually uses machine learning in their business, so I choose this question. Unfortunately, there are few answers that could be used because there are many missing values.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">df = df.dropna(subset=[<span class="string">&#x27;Q22&#x27;</span>]) <span class="comment"># drop NaN</span></span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;Q22&#x27;</span>].replace(&#123;<span class="string">&#x27;We are exploring ML methods (and may one day put a model into production)&#x27;</span>:<span class="string">&#x27;Exploring step&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;We use ML methods for generating insights (but do not put working models into production)&#x27;</span>:<span class="string">&#x27;Use for insights&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;We recently started using ML methods (i.e., models in production for less than 2 years)&#x27;</span>:<span class="string">&#x27;Started using ML methods&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;We have well established ML methods (i.e., models in production for more than 2 years)&#x27;</span>:<span class="string">&#x27;Have well established ML methods&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;No (we do not use ML methods)&#x27;</span>:<span class="string">&#x27;Do not use&#x27;</span>,</span><br><span class="line">                   <span class="string">&#x27;I do not know&#x27;</span>:<span class="string">&#x27;Do not know&#x27;</span>&#125;, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countplot_</span>(<span class="params">data, col_name, q_order</span>):</span></span><br><span class="line">  values = data[col_name].value_counts()[q_order].values</span><br><span class="line">  ax = sns.countplot(x = col_name, hue=data.columns[<span class="number">3</span>], data=data, hue_order = legend_list, palette = <span class="string">&quot;husl&quot;</span>,</span><br><span class="line">                    order = [<span class="string">&#x27;Exploring step&#x27;</span>,<span class="string">&#x27;Use for insights&#x27;</span>,<span class="string">&#x27;Started using ML methods&#x27;</span>,<span class="string">&#x27;Have well established ML methods&#x27;</span>,<span class="string">&#x27;Do not use&#x27;</span>,<span class="string">&#x27;Do not know&#x27;</span>])</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> ax.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    ax.text(p.get_x() + p.get_width()/<span class="number">2.</span>,</span><br><span class="line">            height+<span class="number">3</span>, height, ha=<span class="string">&#x27;center&#x27;</span>, size=<span class="number">6</span>)</span><br><span class="line">  ax.set_ylim([<span class="number">0</span>, <span class="number">550</span>])</span><br><span class="line">  plt.xticks(rotation=<span class="number">20</span>, fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.xlabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.yticks(fontsize=<span class="number">8</span>)</span><br><span class="line">  plt.ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">  plt.legend(fontsize=<span class="number">8</span>, loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line">  plt.title(<span class="string">&#x27;Using ML in the business&#x27;</span>, fontsize=<span class="number">15</span>)</span><br><span class="line">  plt.show()</span><br><span class="line">q22_order = [<span class="string">&#x27;Exploring step&#x27;</span>,<span class="string">&#x27;Use for insights&#x27;</span>,<span class="string">&#x27;Started using ML methods&#x27;</span>,<span class="string">&#x27;Have well established ML methods&#x27;</span>,<span class="string">&#x27;Do not use&#x27;</span>,<span class="string">&#x27;Do not know&#x27;</span>]</span><br><span class="line">col_name = <span class="string">&quot;Q22&quot;</span></span><br><span class="line">countplot_(df, col_name, q22_order)</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_61_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">df_normalize22 = df.groupby([<span class="string">&#x27;Q3&#x27;</span>])[<span class="string">&#x27;Q22&#x27;</span>].value_counts(dropna=<span class="literal">False</span>, normalize=<span class="literal">True</span>, ascending=<span class="literal">False</span>)</span><br><span class="line">df_normalize22 = pd.DataFrame(df_normalize22)</span><br><span class="line">df_normalize22</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th></th>      <th>Q22</th>    </tr>    <tr>      <th>Q3</th>      <th>Q22</th>      <th></th>    </tr>  </thead>  <tbody>    <tr>      <th rowspan="6" valign="top">India</th>      <th>Exploring step</th>      <td>0.217248</td>    </tr>    <tr>      <th>Have well established ML methods</th>      <td>0.179466</td>    </tr>    <tr>      <th>Do not know</th>      <td>0.177823</td>    </tr>    <tr>      <th>Started using ML methods</th>      <td>0.165092</td>    </tr>    <tr>      <th>Do not use</th>      <td>0.156468</td>    </tr>    <tr>      <th>Use for insights</th>      <td>0.103901</td>    </tr>    <tr>      <th rowspan="6" valign="top">USA</th>      <th>Have well established ML methods</th>      <td>0.262517</td>    </tr>    <tr>      <th>Exploring step</th>      <td>0.165088</td>    </tr>    <tr>      <th>Started using ML methods</th>      <td>0.163058</td>    </tr>    <tr>      <th>Do not use</th>      <td>0.146143</td>    </tr>    <tr>      <th>Do not know</th>      <td>0.132612</td>    </tr>    <tr>      <th>Use for insights</th>      <td>0.130582</td>    </tr>  </tbody></table></div><p>There are so many NaN. I think they didn’t know about it, so they just skipped that question. I dropped all NaN in this question.</p><p>In the USA, there are about 72.3% of respondents’s firm use machine learning for their business. In India there are about 66.4% of respondents’s firm use it for their business. It is noticeable that the ratio of using machine learning in the business in both counrty is not big different.</p><p>However, it may be an uncertain value because all missing values are deleted. Therefore if you wondered about it, I strongly recommend find some useful reports.</p><h2 id="8-Want-to-learn-machine-learning-product-multiple"><a href="#8-Want-to-learn-machine-learning-product-multiple" class="headerlink" title="8. Want to learn machine learning product (multiple) "></a>8. Want to learn machine learning product (multiple) <a class="anchor" id="chapter8"></a></h2><p>Finally, I comfirm that the machine learning products what Americans and Indians want to learn. Because the importance and potential of machine learning is continuosly emphasized, I feel like it help us to understand the trend of indusrty.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Preprocess data</span></span><br><span class="line">USA28b = (USA[<span class="string">&#x27;Q28_B_Part_1&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_2&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_3&#x27;</span>],</span><br><span class="line">          USA[<span class="string">&#x27;Q28_B_Part_4&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_5&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_6&#x27;</span>],</span><br><span class="line">          USA[<span class="string">&#x27;Q28_B_Part_7&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_8&#x27;</span>], USA[<span class="string">&#x27;Q28_B_Part_9&#x27;</span>], USA[<span class="string">&#x27;Q28_B_OTHER&#x27;</span>])</span><br><span class="line">USA28b = pd.concat(USA28b)</span><br><span class="line"></span><br><span class="line">India28b = (India[<span class="string">&#x27;Q28_B_Part_1&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_2&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_3&#x27;</span>],</span><br><span class="line">            India[<span class="string">&#x27;Q28_B_Part_4&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_5&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_6&#x27;</span>],</span><br><span class="line">            India[<span class="string">&#x27;Q28_B_Part_7&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_8&#x27;</span>], India[<span class="string">&#x27;Q28_B_Part_9&#x27;</span>], India[<span class="string">&#x27;Q28_B_OTHER&#x27;</span>])</span><br><span class="line">India28b = pd.concat(India28b)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mpl.rcParams[<span class="string">&#x27;font.size&#x27;</span>] = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">f, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize = (<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">USA28b.replace(&#123;<span class="string">&#x27; Azure Machine Learning Studio &#x27;</span>:<span class="string">&#x27;Azure ML Studio&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27; Google Cloud AI Platform / Google Cloud ML Engine&#x27;</span>:<span class="string">&#x27;Google Cloud AI Platform/ML Engine&#x27;</span>&#125;, inplace=<span class="literal">True</span>)    </span><br><span class="line">India28b.replace(&#123;<span class="string">&#x27; Azure Machine Learning Studio &#x27;</span>:<span class="string">&#x27;Azure ML Studio&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27; Google Cloud AI Platform / Google Cloud ML Engine&#x27;</span>:<span class="string">&#x27;Google Cloud AI Platform/ML Engine&#x27;</span>&#125;, inplace=<span class="literal">True</span>)                </span><br><span class="line">    </span><br><span class="line">USA28b.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">0</span>])</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">&#x27;Want to learn in USA&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">India28b.value_counts().plot.pie(autopct=<span class="string">&#x27;%.1f%%&#x27;</span>, ax = ax[<span class="number">1</span>])</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">&#x27;Want to learn in India&#x27;</span>, fontsize=<span class="number">20</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_ylabel(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/images/kaggle_survey_2020/output_66_0.png" alt="png"></p><p>Everything was chosen by similarly ratio. Above all, Google Cloud AI Platform / Google Cloud ML Engine is the most popular among beginners in two countries.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h3><p>Through exploratory data analysis of 《2020 Kaggle Machine Learning &amp; df Science Survey》, we were able to learn about trends of coding and machine learning. Comparing with an obvious IT powerhouse country, the USA and the rising IT powerhouse India is very meaningful to explore trends of it. Most noticeable is that the USA has been interested in coding and programs from long ago, and India is a country with coding craze especially among young ages. It is amazing that the two countries’ response are very similar in the part of machine learning surveys. I think it is proven the importance of machine learning these days. It was an interesting work.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/nechoi/the-future-of-ml-americans-vs-indians&quot;&gt;View to Kaggle&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;The-future-of-ML-Americans-v</summary>
      
    
    
    
    <category term="Project" scheme="https://ne-choi.github.io/categories/Project/"/>
    
    <category term="Kaggle" scheme="https://ne-choi.github.io/categories/Project/Kaggle/"/>
    
    
    <category term="EDA" scheme="https://ne-choi.github.io/tags/EDA/"/>
    
    <category term="Kaggle" scheme="https://ne-choi.github.io/tags/Kaggle/"/>
    
    <category term="Kaggle beginner" scheme="https://ne-choi.github.io/tags/Kaggle-beginner/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 1: Ⅲ. 데이터 시각화와 통계적 해석</title>
    <link href="https://ne-choi.github.io/2021/01/06/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A2_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%8B%9C%EA%B0%81%ED%99%94%EC%99%80_%ED%86%B5%EA%B3%84%EC%A0%81_%ED%95%B4%EC%84%9D/"/>
    <id>https://ne-choi.github.io/2021/01/06/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A2_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%8B%9C%EA%B0%81%ED%99%94%EC%99%80_%ED%86%B5%EA%B3%84%EC%A0%81_%ED%95%B4%EC%84%9D/</id>
    <published>2021-01-06T00:00:00.000Z</published>
    <updated>2021-02-01T00:58:40.495Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><figcaption><span>setup, include</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knitr::opts_chunk$set(echo &#x3D; TRUE)</span><br></pre></td></tr></table></figure><ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.</li></ul><h1 id="Ⅲ-데이터-시각화와-통계적-해석"><a href="#Ⅲ-데이터-시각화와-통계적-해석" class="headerlink" title="Ⅲ. 데이터 시각화와 통계적 해석"></a>Ⅲ. 데이터 시각화와 통계적 해석</h1><h2 id="1-데이터-시각화"><a href="#1-데이터-시각화" class="headerlink" title="1. 데이터 시각화"></a>1. 데이터 시각화</h2><h3 id="데이터-분석단계"><a href="#데이터-분석단계" class="headerlink" title="데이터 분석단계"></a>데이터 분석단계</h3><ul><li>수집 → 정제 → 시각화 → 예측모형/분석</li></ul><h2 id="2-그래프의-유용성과-오류"><a href="#2-그래프의-유용성과-오류" class="headerlink" title="2. 그래프의 유용성과 오류"></a>2. 그래프의 유용성과 오류</h2><h3 id="그래프의-유용성"><a href="#그래프의-유용성" class="headerlink" title="그래프의 유용성"></a>그래프의 유용성</h3><ul><li>그래프는 데이터 시각화의 일종</li><li>그래프의 올바른 해석은 데이터사이언티스트의 필수 능력이자 커뮤니케이션 도구</li></ul><ul><li>히스토그램<ul><li>같은 분산이라도 데이터 분포를 더 잘 파악할 수 있음</li><li>계급 구간 설정에 따라 히스토그램 그래프가 완전히 달라짐</li></ul></li></ul><h3 id="데이터-시각화-주의할-점"><a href="#데이터-시각화-주의할-점" class="headerlink" title="데이터 시각화 주의할 점"></a>데이터 시각화 주의할 점</h3><ul><li>그래프 목적은 데이터를 분명하게 표현하는 것<ol><li>그래프 작성 시, 축의 범위와 간격 등을 잘 정해야 함</li><li>그래프를 보는 사람의 수준을 고려해야 함</li><li>그래프 종류별 장단점을 정확히 파악하고 사용해야 함</li></ol></li></ul><h3 id="퀴즈-오답"><a href="#퀴즈-오답" class="headerlink" title="퀴즈 오답"></a>퀴즈 오답</h3><ol><li>히스토그램을 통해 알 수 있는 것<ul><li>데이터의 분포 범위</li><li>이상치(Outlier)의 존재 유무</li></ul></li></ol><h2 id="3-상자그림이-주는-정보와-해석"><a href="#3-상자그림이-주는-정보와-해석" class="headerlink" title="3. 상자그림이 주는 정보와 해석"></a>3. 상자그림이 주는 정보와 해석</h2><h3 id="상자그림이-필요한-이유"><a href="#상자그림이-필요한-이유" class="headerlink" title="상자그림이 필요한 이유?"></a>상자그림이 필요한 이유?</h3><ul><li>평균과 분산(기술통계치)만으로는 부족함<ul><li>평균, 분산을 안다고 데이터가 어느 쪽에 더 많이 분포하는지 알 수 없음</li><li>이상치 존재 여부를 알 수 없음</li><li>데이터 분포 범위(최대값, 최소값)를 한눈에 알기 어려움</li></ul></li></ul><h3 id="상자그림이-주는-정보"><a href="#상자그림이-주는-정보" class="headerlink" title="상자그림이 주는 정보"></a>상자그림이 주는 정보</h3><ul><li>한눈에 5가지 정보를 제공<ul><li>중앙값, 일사분위수, 삼사분위수, 최대값, 최소값</li><li>데이터 분포의 대칭성, 치우침, 이상치를 쉽게 파악할 수 있음</li></ul></li></ul><h3 id="상자그림-그리는-방법"><a href="#상자그림-그리는-방법" class="headerlink" title="상자그림 그리는 방법"></a>상자그림 그리는 방법</h3><ol><li><p>데이터의 중앙값(median)을 찾는다.</p><ul><li>중앙값이란?<ul><li>n개의 관측치를 오름차순으로 배열했을 때, 중앙 위치에 놓이는 값</li><li>데이터 수가 작고 이상치가 있을 때, 평균보다 더 정확한 모집단의 중심값이 됨</li></ul></li></ul></li><li><p>일사분위수(Q1)와 삼사분위수(Q3)을 찾는다.</p><ul><li>일사분위수(Q1)<ul><li>데이터를 크기 순서로 배열했을 때, 25% 지점 값</li></ul></li><li>삼사분위수(Q3)<ul><li>데이터를 크기 순서로 배열했을 때, 75% 지점 값</li></ul></li></ul></li><li><p>일사분위수 ~ 삼사분위수를 상자로 그린다. (사분위범위)</p></li><li><p>최소값 ~ 일사분위수, 삼사분위수 ~ 최대값을 그린다.</p></li><li><p>이상치를 표시한다.</p><ul><li>일사분위로부터 -(1.5)*사분위범위를 넘는 관측치는 이상치로 표시</li><li>삼사분위로부터 +(1.5)*사분위범위를 넘는 관측치는 이상치로 표시</li></ul></li></ol><h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul><li>상자그림은 다섯 가지 숫자로 데이터를 요약한 그래프<ul><li>가운데 상자는 Q1에서 Q1까지 그림</li><li>상자 안의 선은 중앙값을 나타냄</li><li>상자 밖 선은 최대값과 최소값까지 이어짐</li><li>상자와 수염 밖 데이터는 이상치</li></ul></li></ul><h2 id="4-산점도와-상관관계-트렌드-분석"><a href="#4-산점도와-상관관계-트렌드-분석" class="headerlink" title="4. 산점도와 상관관계: 트렌드 분석"></a>4. 산점도와 상관관계: 트렌드 분석</h2><h3 id="산점도-필요-이유"><a href="#산점도-필요-이유" class="headerlink" title="산점도 필요 이유"></a>산점도 필요 이유</h3><ul><li>지금까지 히스토그램, 상자그림으로 변수 1개의 데이터 분포를 살펴봄</li><li><strong>두 변수 사이 관계</strong>를 아는 방법은?</li></ul><h3 id="산점도-Scatter-plot"><a href="#산점도-Scatter-plot" class="headerlink" title="산점도(Scatter plot)"></a>산점도(Scatter plot)</h3><ul><li><p>변수 간 관계 방향, 트렌드, 강도를 알 수 있음</p></li><li><p>산점도의 x축과 y축은 독립변수와 종속변수로 이루어짐</p><ul><li>독립변수: 원인 역할을 하는 변수, X</li><li>종속변수: 결과 관측 변수, Y</li><li>ex. 학점 - 공부 시간</li></ul></li><li><p>산점도로부터 알 수 있는 3가지</p><ul><li>트렌드: linear, curved, clusters, no pattern</li><li>방향: positie, negative, no direction</li><li>강도: how closely the points fit the trend</li></ul></li></ul><h3 id="산점도-해석-방향"><a href="#산점도-해석-방향" class="headerlink" title="산점도 해석: 방향"></a>산점도 해석: 방향</h3><ul><li><p>양의 상관관계(Positively associated)</p><ul><li>두 변수 X와 Y가 X값이 클 때 Y값도 큰 경향이 있고, X값이 작을 때 Y값도 작은 경향</li></ul></li><li><p>음의 상관관계(Negatively associated)</p><ul><li>두 변수 X와 Y가 X값이 클 때 Y값은 작은 경향이 있고, X값이 작을 때 Y값은 큰 경향</li></ul></li></ul><h3 id="산점도-해석-강도"><a href="#산점도-해석-강도" class="headerlink" title="산점도 해석: 강도"></a>산점도 해석: 강도</h3><ul><li>상관계수(Correlation, r)<ul><li>r은 -1부터 +1까지 존재</li><li>+1에 가까울수록 강한 양의 상관관계</li><li>-1에 가까울수록 강한 음의 상관관계</li><li>0은 가장 약한 상관관계(상관관계가 없음)</li></ul></li><li>산점도에서 의미하는 상관관계: <strong>선형적인</strong> 상관관계만을 말함</li></ul><h3 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h3><ul><li>산점도는 두 변수간 관계 방향, 형태, 강도를 살펴볼 수 있는 그래프</li><li>상관계수(r)는 두 변수간 선형적인 상관관계의 강도를 나타냄</li><li>산점도에서 선형모형(선형함수식)을 구현할 수 있음</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight plain&quot;&gt;&lt;figcaption&gt;&lt;span&gt;setup, include&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="통계" scheme="https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"/>
    
    <category term="데이터시각화" scheme="https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 1: Ⅱ. 빅데이터 탐색의 첫걸음</title>
    <link href="https://ne-choi.github.io/2021/01/05/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A1_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89%EC%9D%98_%EC%B2%AB%EA%B1%B8%EC%9D%8C/"/>
    <id>https://ne-choi.github.io/2021/01/05/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A1_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89%EC%9D%98_%EC%B2%AB%EA%B1%B8%EC%9D%8C/</id>
    <published>2021-01-05T00:00:00.000Z</published>
    <updated>2021-02-01T00:58:44.309Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.</li></ul><h1 id="Ⅱ-빅데이터-탐색의-첫걸음"><a href="#Ⅱ-빅데이터-탐색의-첫걸음" class="headerlink" title="Ⅱ. 빅데이터 탐색의 첫걸음"></a>Ⅱ. 빅데이터 탐색의 첫걸음</h1><h2 id="1-데이터의-평균-중심위치"><a href="#1-데이터의-평균-중심위치" class="headerlink" title="1. 데이터의 평균(중심위치)"></a>1. 데이터의 평균(중심위치)</h2><h3 id="평균"><a href="#평균" class="headerlink" title="평균"></a>평균</h3><ul><li>평균은 데이터를 하나의 값으로 표현한 요약된 정보: 추정치</li><li>평균 = 데이터 값의 총합 / 데이터 개수</li></ul><h3 id="평균-다룰-때-주의할-점"><a href="#평균-다룰-때-주의할-점" class="headerlink" title="평균 다룰 때 주의할 점"></a>평균 다룰 때 주의할 점</h3><ul><li>평균은 혼자 존재하는 개념이 아님</li></ul><ol><li><p>평균과 표본선정</p><ul><li><p>표본선정에 따라 평균값이 달라짐</p></li><li><p>ex. 대기업 평균 연봉 조사</p><ul><li>표본 A: 연령대별로 각 50명씩 선정</li><li>표본 B: 50대에서 200명을 선정</li></ul></li><li><p>조사된 평균값이 모집단을 대표하는 통계치라고 할 수 있는가?</p></li><li><p>표본이 적합하게 추출되었는지 평가하는 법</p><ul><li>편의(bias)가 적은가: 표본으로부터 얻어지는 통계치(표본평균)의 기대값이 모수의 참값과 유사한가?</li><li>정확도(precision)가 높은가: 반복해서 표본 추출 시, 얼마나 유사한 값이 나오는가?</li></ul></li></ul></li><li><p>평균과 분산</p><ul><li>같은 평균이라도 분산이 다르면 데이터 특성이 달라짐</li></ul></li><li><p>평균값은 그 집단에서 가장 많이 존재하는 것이 아님</p><ul><li>데이터: 1, 2, 2, 7</li><li>평균: 3</li></ul></li></ol><h3 id="데이터의-중심척도"><a href="#데이터의-중심척도" class="headerlink" title="데이터의 중심척도"></a>데이터의 중심척도</h3><ul><li><p>평균(mean)</p><ul><li>표본이 적은 경우, 아주 큰 값이나 작은 값(outlier)에 민감한 추정치</li><li>중앙값이 평균보다 더 적합한 중심척도인 경우도 있음</li></ul></li><li><p>중앙값(median)</p><ul><li>n개의 관측치를 크기순으로 배열했을 때, 중앙 위치에 놓이는 값</li><li>데이터 수가 작고 이상치(outlier)가 있을 때, 평균보다 더 정확한 모집단의 중심값이 됨</li></ul></li><li><p>최빈값(mode)</p><ul><li>전체 데이터 중, 가장 빈도(frequency)가 높은 값</li><li>데이터 수가 많아질수록 평균과 가까워짐</li></ul></li></ul><h2 id="2-데이터의-분산-산포정도"><a href="#2-데이터의-분산-산포정도" class="headerlink" title="2. 데이터의 분산(산포정도)"></a>2. 데이터의 분산(산포정도)</h2><h3 id="어느-집단-분산이-클까"><a href="#어느-집단-분산이-클까" class="headerlink" title="어느 집단 분산이 클까?"></a>어느 집단 분산이 클까?</h3><ul><li>평균만 아는 사람 vs 평균과 표준편차를 아는 사람</li><li>평균 연봉은 같지만 편차가 큰 경우, 편차가 적은 기업에 비해 초봉이 낮고 승진 시 월급이 높아짐</li></ul><h3 id="분산-공식"><a href="#분산-공식" class="headerlink" title="분산 공식"></a>분산 공식</h3><ul><li>데이터 평균과 데이터간 거리 합으로 분산 계산<ul><li>데이터: x<del>1</del>, x<del>2</del>, …, x<del>n</del></li><li>평균: Xbar</li><li>편차: (x<del>1</del> - xbar),(x<del>2</del> - xbar),…,(x<del>n</del> - xbar)</li><li>편차들의 합: (x<del>1</del> - xbar) + (x<del>2</del> - xbar) + … + (x<del>n</del> - xbar) = ?</li></ul></li></ul><br>- 데이터가 평균으로부터 대칭적으로 존재할 경우, 편차들의 합이 0 → **편차를 제곱하여 더함**- 분산 = 편차들의 제곱합을 (n-1)*로 나눔  - (n-1)로 나누는 이유: 자유도와 관련, 평균값으로 표본평균을 사용하므로 1개의 자유도를 잃게 되어 (n-1)로 나눔<h3 id="표준편차"><a href="#표준편차" class="headerlink" title="표준편차"></a>표준편차</h3><ul><li>(개별데이터값 - 평균값) 차이를 제곱하여 더하였으므로 값이 커지고 단위가 달라짐<br>→ 분산에 <strong>제곱근</strong>을 취하여 원래 단위로 복원 → <strong>표준편차</strong>라고 부름</li></ul><h3 id="분산의-의미"><a href="#분산의-의미" class="headerlink" title="분산의 의미"></a>분산의 의미</h3><ul><li>분산: 데이터가 분포되어있는 정도<ul><li>데이터에 대한 요약 정보 보완</li><li>평균값만으로는 데이터 상상이 어려움</li></ul></li></ul><h2 id="3-데이터와-빅데이터"><a href="#3-데이터와-빅데이터" class="headerlink" title="3. 데이터와 빅데이터"></a>3. 데이터와 빅데이터</h2><h3 id="데이터란"><a href="#데이터란" class="headerlink" title="데이터란?"></a>데이터란?</h3><ul><li>모든 숫자를 데이터라고 할 수 있을까? No</li><li>데이터: <strong>구조화된 데이터</strong><ul><li>다차원 배열(매트릭스)</li><li>각 열의 형식이 다른 표 or 스프레드시트</li><li>탭이나 텍스트파일 형식으로 저장</li></ul></li></ul><h3 id="데이터화-Datafication"><a href="#데이터화-Datafication" class="headerlink" title="데이터화(Datafication)"></a>데이터화(Datafication)</h3><ul><li>기계가 읽어들일 수 있는 모든 것(숫자, 이미지, 텍스트)을 데이터로 변환하는 것</li><li>개인의 활동을 실시간으로 추적해 이를 예측분석이 가능한 수량화된 온라인 데이터로 변환하는 것을 의미</li></ul><h3 id="빅데이터란"><a href="#빅데이터란" class="headerlink" title="빅데이터란"></a>빅데이터란</h3><ul><li>Volume(양)</li><li>Velocity(속도)</li><li>Variety(다양성)</li></ul><h2 id="4-데이터-탐색의-첫걸음"><a href="#4-데이터-탐색의-첫걸음" class="headerlink" title="4. 데이터 탐색의 첫걸음"></a>4. 데이터 탐색의 첫걸음</h2><h3 id="통계치로-인사이트-얻기"><a href="#통계치로-인사이트-얻기" class="headerlink" title="통계치로 인사이트 얻기"></a>통계치로 인사이트 얻기</h3><ul><li>ㅇㅇ회사 공채에 합격하기 위해 합격자 평균 분석</li><li>사람들이 선호하는 기업 문화 알아보기</li></ul><h3 id="최적의-의사결정-데이터탐색"><a href="#최적의-의사결정-데이터탐색" class="headerlink" title="최적의 의사결정: 데이터탐색"></a>최적의 의사결정: 데이터탐색</h3><ul><li>공정에 대한 평균, 산포, 불량률 추정<ul><li>품질 변동상황을 관리도(control chart)로 표현</li><li>공정에 발생하는 이상요인을 빨리 탐지하여 수정조치 → 불량 사전 예방</li><li>공정에서 정상범위 관리도 차트</li><li>중심선, 관리상한선, 관리하한선을 어떻게 정할 것인가?</li></ul></li></ul><h3 id="숨겨진-패턴-분석-분류"><a href="#숨겨진-패턴-분석-분류" class="headerlink" title="숨겨진 패턴 분석: 분류"></a>숨겨진 패턴 분석: 분류</h3><ul><li><p>암과 정상 뇌 영상을 숫자화(데이터화)함</p><ul><li>분류(암/정상)를 가장 잘 구분하는 변수를 찾고, 범주간 차이를 가장 잘 표현하는 새로운 함수를 구함</li><li>새로운 환자 영상을 보고 어느 범주에 더 가까운지를 판별하여 암 여부를 진단</li></ul></li><li><p>두 범주가 잘 분류된다는 것: 두 범주가 겹치지 않으면서 두 범주 중심위치가 가능한 먼 것</p></li></ul><h3 id="트렌드-분석-웹-마이닝"><a href="#트렌드-분석-웹-마이닝" class="headerlink" title="트렌드 분석: 웹 마이닝"></a>트렌드 분석: 웹 마이닝</h3><ul><li>1년간 검색어 트렌드 분석</li><li>Moving Average를 통한 트렌드 파악</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅱ-빅데이터-탐색의-첫걸음&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="평균" scheme="https://ne-choi.github.io/tags/%ED%8F%89%EA%B7%A0/"/>
    
    <category term="분산" scheme="https://ne-choi.github.io/tags/%EB%B6%84%EC%82%B0/"/>
    
  </entry>
  
  <entry>
    <title>데이터사이언스를 위한 통계학입문 1: Ⅰ. 데이터과학과 통계</title>
    <link href="https://ne-choi.github.io/2021/01/04/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A0_%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99%EA%B3%BC_%ED%86%B5%EA%B3%84/"/>
    <id>https://ne-choi.github.io/2021/01/04/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A0_%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99%EA%B3%BC_%ED%86%B5%EA%B3%84/</id>
    <published>2021-01-04T00:00:00.000Z</published>
    <updated>2021-02-01T00:58:48.715Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.</li></ul><h1 id="Ⅰ-데이터과학과-통계"><a href="#Ⅰ-데이터과학과-통계" class="headerlink" title="Ⅰ 데이터과학과 통계"></a>Ⅰ 데이터과학과 통계</h1><h2 id="1-데이터과학이란-무엇인가"><a href="#1-데이터과학이란-무엇인가" class="headerlink" title="1. 데이터과학이란 무엇인가"></a>1. 데이터과학이란 무엇인가</h2><h3 id="데이터과학이란"><a href="#데이터과학이란" class="headerlink" title="데이터과학이란?"></a>데이터과학이란?</h3><ul><li><p>빅데이터: 통계학, 데이터마이닝, 인공지능, 딥러닝  </p></li><li><p>Data → Data Analytics → Insight  </p></li><li><p>필요한 기술</p><ul><li>통계적 개념과 지식: 샘플링, 확률분포, 가설검정, p-value</li><li>데이터 다루는 기술(데이터 큐레이션): 빅데이터 다루기</li><li>데이터 요약된 정보 전달 기술: 데이터 시각화(공간지도분석, 다차원그래픽)</li><li>데이터윤리, 데이터보안</li><li>데이터 도메인 지식, 분석 능력: 현실 문제 해결 능력</li></ul></li></ul><h3 id="데이터-과학-예시"><a href="#데이터-과학-예시" class="headerlink" title="데이터 과학 예시"></a>데이터 과학 예시</h3><ul><li>핀란드 의료데이터 프로젝트: FinnGen<ul><li>핀란드인(Finnish) + 유전자(Genome)</li><li>자발적 참여자의 유전자정보 수집, 확자의 의료정보까지 통합 구축</li><li>50만명 목표, 23만명 수집, 15만명 유전자 정보 보호</li><li>6개월마다 데이터 업데이트: 전세계 연구자와 공유</li><li>관절염/당뇨병 등 자가면역질환 연구 수행</li></ul></li></ul><h3 id="공부할-내용"><a href="#공부할-내용" class="headerlink" title="공부할 내용"></a>공부할 내용</h3><ul><li>데이터과학을 위한 통계 개념과 지식</li><li>공유데이터와 오픈소스</li><li>빅데이터분석을 위한 첫걸음: 데이터 중심 위치, 산포 정도 확인</li><li>데이터 시각화</li><li>데이터과학에서 확률분포 의미</li></ul><h2 id="2-통계가-상식이-된-사회"><a href="#2-통계가-상식이-된-사회" class="headerlink" title="2. 통계가 상식이 된 사회"></a>2. 통계가 상식이 된 사회</h2><h3 id="통계가-왜-필요한가"><a href="#통계가-왜-필요한가" class="headerlink" title="통계가 왜 필요한가?"></a>통계가 왜 필요한가?</h3><ul><li>통계는 올바른 의사결정을 도움<ul><li>빅데이터 → 요약된 정보 제공, 과거 데이터로 미래 예측, 데이터에 숨겨진 패턴 발견 → 올바른 의사결정</li></ul></li></ul><h3 id="의사결정에서-통계의-역할"><a href="#의사결정에서-통계의-역할" class="headerlink" title="의사결정에서 통계의 역할"></a>의사결정에서 통계의 역할</h3><ul><li><p>ex. 고객 이동경로 분석 결과 활용 예시</p><ul><li>고객 금융 검색 경로 추적</li><li>고객 이동경로(customer journey) 분석</li><li>이탈 가능성 높은 고객을 붙잡고 신규 고객 유입하는 데 사용</li></ul></li><li><p>정부 정책 근거자료: 영국의회 노령연금 도입</p><ul><li>1886년 영국 사회학자 찰스 부스</li><li>산업혁명으로 부유해진 런던에서 시민 빈곤 상황을 12년간 조사</li><li>가난을 8단계로 분류하여 절대빈곤이 30.7%에 달한다는 결과 발표</li><li>1908년 영국의회 노령연금 도입</li></ul></li><li><p>서울시 심야버스 노선정책</p><ul><li>자정 ~ 새벽 5시까지의 시민 휴대폰 전화 데이터 수집</li><li>유동인구 분포 및 밀도 파악하여 심야버스 노선 수립</li></ul></li></ul><h3 id="요약"><a href="#요약" class="headerlink" title="요약"></a>요약</h3><ul><li>개인의 일상활동은 데이터화되어 예측분석이 가능하도록 수량화, 객관화됨</li><li>통계는 수많은 데이터로부터 요약된 정보 제공, 미래 데이터 예측, 숨겨진 패턴 발견 → 올바른 의사결정을 하게 함</li><li>통계치는 금융권 관리전략, 정부 정책 수립, 법정소송 시 근거자료로 활용될 수 있음</li></ul><h2 id="3-데이터-분선과-윤리"><a href="#3-데이터-분선과-윤리" class="headerlink" title="3. 데이터 분선과 윤리"></a>3. 데이터 분선과 윤리</h2><h3 id="데이터-정직성"><a href="#데이터-정직성" class="headerlink" title="데이터 정직성"></a>데이터 정직성</h3><ul><li><p>데이터 정직성?</p><ul><li>데이터가 잘못되지 않았는지를 나타냄</li></ul></li><li><p>ex. 한강 수질 검사</p><ul><li><p>한강에서 물을 한웅큼 채취</p></li><li><p>방 안에 있는 보온병에 보관</p></li><li><p>수질 검사를 하면 그 결과를 신뢰할 수 있는가?</p></li><li><p>데이터를 잘못 수집했다</p><ul><li>한강이 넓은데 어디서 수집?: 표본의 수</li><li>어제 산성비가 내렸을 수 있음: 데이터 수집 기간/시기</li><li>손으로 채집하여 오염 가능성: 수집 방법</li></ul></li><li><p>데이터를 잘못 보관했다</p><ul><li>보온병 세균이 옮으면?</li><li>차가운 데 보관해야 하는 것 아닌지?</li><li>다른 이가 손대지 못하게 안전히 보관해야 함: data storage</li></ul></li></ul></li></ul><h3 id="기사-내-통계치-해석-평가"><a href="#기사-내-통계치-해석-평가" class="headerlink" title="기사 내 통계치 해석/평가"></a>기사 내 통계치 해석/평가</h3><ul><li><p>ex. 비정규직 vs 정규직 임금 격차</p><ul><li>동등한 조건으로 비교되었는지</li><li>성, 연령, 근속년수 등 요인이 통제된 상태의 시급으로 비교되었는지</li></ul></li><li><p>ex. 세무사 월 최고 소득</p><ul><li>편향된 표본 추출</li><li>세무사 39명 조사, 우연히 연봉 3~4억원 대인 자영업 세무사가 많았음</li></ul></li></ul><h3 id="요약-1"><a href="#요약-1" class="headerlink" title="요약"></a>요약</h3><ul><li>데이터과학 윤리<br>-데이터를 올바르게 분석할 뿐 아니라 올바른 방법으로 수집해야 함</li><li>정직하지 못한 데이터의 주요 원인<ul><li>데이터분석자의 무지함, 비윤리성, 환경의 제약</li></ul></li><li>데이터 수집 시, 너무 적은 양의 데이터, 편향된 표본 추출, 데이터의 왜곡 및 훼손에 주의</li><li>결측치 문제 고려</li></ul><h2 id="4-공유데이터와-오픈소스-github"><a href="#4-공유데이터와-오픈소스-github" class="headerlink" title="4. 공유데이터와 오픈소스(github)"></a>4. 공유데이터와 오픈소스(github)</h2><h3 id="공유데이터"><a href="#공유데이터" class="headerlink" title="공유데이터"></a>공유데이터</h3><ul><li><p>공유데이터?</p><ul><li>모든사람이 자유롭게 사용/재사용/재배포 가능한 데이터</li><li>이용성 및 접근성</li><li>재사용과 재배포</li><li>보편적 참여</li></ul></li><li><p>공유데이터 서비스</p><ul><li>머신러닝 기법 분석에 활용 가능한 데이터 저장소<ul><li><a href="https://archive.ics.uci.edu/ml/index.php">Machine Learning Repository in UC, Irvine</a></li></ul></li><li>정부 제공<ul><li><a href="kostat.go.kr">통계청</a></li><li><a href="www.data.go.kr">공공데이터포털</a></li><li><a href="data.seoul.go.kr">서울열린데이터광장</a></li></ul></li><li>네이버<ul><li>네이버 데이터랩: 국내 공공데이터를 기관별로 분류하여 접근성을 높임</li></ul></li></ul></li></ul><h3 id="오픈소스"><a href="#오픈소스" class="headerlink" title="오픈소스"></a>오픈소스</h3><ul><li><p>오픈소스?</p><ul><li>저작권자가 소스코드를 공개하여 누구나 복제, 개작, 배포할 수 있는 소프트웨어</li><li>R, Python: 오픈소스 통계분석 프로그램</li><li>C++, JAVA, Python 등 다른 프로그래밍 언어와 쉽게 연동</li><li>빅데이터 시스템인 스파크와도 일부 기능을 연동함으로써 응용범위가 더욱 넓어짐</li></ul></li><li><p>인공지능에서의 오픈소스</p><ul><li>텐서플로우: 구글에서 머신러닝과 신경망 연구를 위해 만튼 소프트웨어</li><li>딥마인드랩: 구글 딥마인드에서 공개한 인공지능 개발 플랫폼</li></ul></li><li><p>공유데이터 vs 오픈소스</p><ul><li>공유데이터: 단순히 수치로 표현되는 측정치 또는 결괏값</li><li>오픈소스: 단순 데이터가 아닌 지적 창작물</li></ul></li></ul><h3 id="github"><a href="#github" class="headerlink" title="github"></a>github</h3><ul><li>git: 프로그램 등 소스 코드 관리를 위한 분산 관리 툴</li><li>github은 git에 프로젝트 관리지원기능을 확장한 웹 호스팅 서비스</li><li>2008년 미국 github사에서 서비스 시작</li><li>2018년 마이크로소프트가 인수</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅰ-데이터과학과-통계&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Statistics" scheme="https://ne-choi.github.io/categories/Study/Postech/Statistics/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="통계" scheme="https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 2: ⅩⅥ. 딥러닝과 텍스트 마이닝</title>
    <link href="https://ne-choi.github.io/2020/12/24/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A5_%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%A7%88%EC%9D%B4%EB%8B%9D/"/>
    <id>https://ne-choi.github.io/2020/12/24/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A5_%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%A7%88%EC%9D%B4%EB%8B%9D/</id>
    <published>2020-12-24T00:00:00.000Z</published>
    <updated>2021-02-01T01:09:58.633Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="ⅩⅥ-딥러닝과-텍스트-마이닝"><a href="#ⅩⅥ-딥러닝과-텍스트-마이닝" class="headerlink" title="ⅩⅥ. 딥러닝과 텍스트 마이닝"></a>ⅩⅥ. 딥러닝과 텍스트 마이닝</h1><h2 id="1-Neural-Networks"><a href="#1-Neural-Networks" class="headerlink" title="1. Neural Networks"></a>1. Neural Networks</h2><h3 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h3><ul><li>인공신경망은 기계학습(Machine Learning)의 통계적 학습 알고리즘 중 하나</li><li>컴퓨터 비전, 자연어 처리, 음석 인식 등 영역에서 사용</li><li>AI ⊂ Machine Learning ⊂ Neural Network  </li></ul><ul><li><strong>신경망 모델(Neural Network)</strong><ul><li>Perceptron을 한 단위로 하는 네트워크를 구축하여, 인간의 신경세포(Neuron)와 유사한 기능을 하도록 제안</li></ul></li></ul><h3 id="Perceptron-Single-Layer"><a href="#Perceptron-Single-Layer" class="headerlink" title="Perceptron - Single Layer"></a>Perceptron - Single Layer</h3><ul><li>하나의 Perceptron은 단순히 다수의 입력과 가중치의 선형 결합을 계산하는 역할을 수행</li><li>Activation 함수에 따라 선형결합으로 생성되는 출력값이 결정<ul><li>Binary Threshold</li><li>Sigmoid</li><li>ReLU(Rectified Linear Unit)</li><li>tanh</li></ul></li></ul><h3 id="Multi-layer-perceptron"><a href="#Multi-layer-perceptron" class="headerlink" title="Multi-layer perceptron"></a>Multi-layer perceptron</h3><ul><li><p>Perceptron으로 구성된 Single Layer들이 Multi-layer를 만듦</p><ul><li>Input layer와 Output layer 사이에는 Hidden layer가 존재하여 Non-linear transformation 수행</li></ul></li><li><p>Output layer에서 Softmax 함수를 통해 가장 큰 값을 손쉽게 알 수 있음</p><ul><li>Exponential 함수로 항상 양수 결과치가 도출되고 이를 통해 확률값을 도출</li></ul></li></ul><h3 id="Neural-Network-수행"><a href="#Neural-Network-수행" class="headerlink" title="Neural Network 수행"></a>Neural Network 수행</h3><ul><li>mxnet 패키지는 R4.0 버전 이후부터 실행 불가</li><li>R 3.6.2버전 설치해서 실행 필요<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># require mxnet</span><br><span class="line">install.packages(&quot;https:&#x2F;&#x2F;github.com&#x2F;jeremiedb&#x2F;mxnet_winbin&#x2F;raw&#x2F;master&#x2F;mxnet.zip&quot;,repos &#x3D; NULL)</span><br><span class="line">library(mxnet)</span><br><span class="line"></span><br><span class="line"># If you have Error message &quot;no package called XML or DiagrmmeR&quot;, then install</span><br><span class="line">install.packages(&quot;XML&quot;)</span><br><span class="line">install.packages(&quot;DiagrammeR&quot;)</span><br><span class="line">#library(XML)</span><br><span class="line">#library(DiagrammeR)</span><br><span class="line"></span><br><span class="line">#devtools::install_github(&quot;rich-iannone&#x2F;DiagrammeR&quot;)</span><br><span class="line"></span><br><span class="line">#install.packages(&#39;devtools&#39;)</span><br><span class="line">#library(devtools)</span><br></pre></td></tr></table></figure></li><li>데이터 불러오기<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week16_1&#x2F;iris.csv&quot;)</span><br><span class="line">attach(iris)</span><br><span class="line"></span><br><span class="line"># change to numeric from character variable : Species</span><br><span class="line">iris[,5] &#x3D; as.numeric(iris[,5])-1</span><br><span class="line">table(Species)</span><br><span class="line"># check the data</span><br><span class="line">head(iris, n&#x3D;10)</span><br></pre></td></tr></table></figure></li><li>학습데이터와 검증데이터<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># split train &amp; test dataset</span><br><span class="line"># training (n&#x3D;100)&#x2F; test data(n&#x3D;50) </span><br><span class="line">set.seed(1000,sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&lt;-nrow(iris)</span><br><span class="line">tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line"># split train data and test data</span><br><span class="line">train&lt;-data.matrix(iris[tr.idx,])</span><br><span class="line">test&lt;-data.matrix(iris[-tr.idx,])</span><br><span class="line"></span><br><span class="line"># feature &amp; Labels</span><br><span class="line"># 객체별 Feature와 Label로 분리</span><br><span class="line"># Label은 5번째 열에 위치</span><br><span class="line">train_feature&lt;-train[,-5]</span><br><span class="line">trainLabels&lt;-train[,5]</span><br><span class="line">test_feature&lt;-test[,-5]</span><br><span class="line">testLabels &lt;-test[,5]</span><br></pre></td></tr></table></figure></li><li>Hidden Layer 구성<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Build nn model</span><br><span class="line"># first layers</span><br><span class="line">require(mxnet)</span><br><span class="line">my_input &#x3D; mx.symbol.Variable(&#39;data&#39;)</span><br><span class="line">fc1 &#x3D; mx.symbol.FullyConnected(data&#x3D;my_input, num.hidden &#x3D; 200, name&#x3D;&#39;fc1&#39;) # 200개의 뉴런 형성</span><br><span class="line">relu1 &#x3D; mx.symbol.Activation(data&#x3D;fc1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu1&#39;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># second layers</span><br><span class="line">fc2 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu1, num.hidden &#x3D; 100, name&#x3D;&#39;fc2&#39;) # 100개의 뉴런 형성</span><br><span class="line">relu2 &#x3D; mx.symbol.Activation(data&#x3D;fc2, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu2&#39;)</span><br></pre></td></tr></table></figure></li><li>Output Layer 구성<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># third layers</span><br><span class="line">fc3 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu2, num.hidden &#x3D; 3, name&#x3D;&#39;fc3&#39;) # 3개로 분류(0,1,2)해야 하므로 3개의 Output 뉴런 생성</span><br><span class="line"></span><br><span class="line"># softmax</span><br><span class="line">softmax &#x3D; mx.symbol.SoftmaxOutput(data&#x3D;fc3, name&#x3D;&#39;sm&#39;)</span><br><span class="line"># sofrmax 결과를 통해 가장 큰 값 선택</span><br></pre></td></tr></table></figure></li><li>모델 학습<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># training</span><br><span class="line">mx.set.seed(123, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">device &lt;- mx.cpu()</span><br><span class="line">model &lt;- mx.model.FeedForward.create(softmax,</span><br><span class="line">                                     optimizer &#x3D; &quot;sgd&quot;, # Stochastic Gradient Descent</span><br><span class="line">                                     array.batch.size&#x3D;10, # 총 10개 그룹</span><br><span class="line">                                     num.round &#x3D; 500, learning.rate&#x3D;0.1,</span><br><span class="line">                                     X&#x3D;train_feature, y&#x3D;trainLabels, ctx&#x3D;device,</span><br><span class="line">                                     eval.metric &#x3D; mx.metric.accuracy,</span><br><span class="line">                                     array.layout &#x3D; &quot;rowmajor&quot;,</span><br><span class="line">                                     epoch.end.callback&#x3D;mx.callback.log.train.metric(100))</span><br><span class="line">graph.viz(model$symbol)</span><br></pre></td></tr></table></figure></li><li>모델 테스트<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># testing</span><br><span class="line">predict_probs &lt;- predict(model, test_feature, array.layout &#x3D; &quot;rowmajor&quot;)</span><br><span class="line">predicted_labels &lt;- max.col(t(predict_probs)) - 1</span><br><span class="line">table(testLabels, predicted_labels)</span><br><span class="line">sum(diag(table(testLabels, predicted_labels)))&#x2F;length(predicted_labels)</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-Convolutional-Neural-Networks"><a href="#2-Convolutional-Neural-Networks" class="headerlink" title="2. Convolutional Neural Networks"></a>2. Convolutional Neural Networks</h2><h3 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h3><ul><li>신경망 모델(Neural Net)<ul><li>입력값으로 객체의 특성(feature)을 받고, 출력된 값과 실제 값을 비교하는 과정을 거침(지도학습; Supervised Learning)</li><li>하나의 이미지는 수많은 픽셀들이 모여 형성하며, 특정 색에 해당하는 특정 값을 가짐<ul><li>이미지의 모든 픽셀값을 입력값으로 갖는 신경망 모델을 만들 수 있음</li></ul></li></ul></li></ul><h3 id="Intuitions"><a href="#Intuitions" class="headerlink" title="Intuitions"></a>Intuitions</h3><ul><li><p>고해상도 이미지의 경우 특성 feature 수가 너무 많아짐</p><ul><li>모든 뉴런이 모든 픽셀과 연결(fully connected)되어 있을 경우, 모델 학습에 큰 어려움이 있음</li><li>각 뉴런들이 이미지의 일부 특성 feature에만 연결될 수 있는 구조가 더 더 적합함</li><li>Convolution operation을 통해 구현 가능</li></ul></li><li><p>Convolutional Neural Network</p><ul><li>Feed forward Network: x<del>i</del>^n^을 구함<ul><li>Convilution</li><li>Max Pooling</li><li>Activation function</li></ul></li><li>Backpropagation: Error 최소화</li></ul></li></ul><h3 id="Concolution-Operation"><a href="#Concolution-Operation" class="headerlink" title="Concolution Operation"></a>Concolution Operation</h3><ul><li>임의의 값으로 설정된 filter가 전체 이미지 중 일부의 선형 결합을 계산<ul><li>각각 결괏값은 하나의 Neuron이 되며, filter는 해당 Neuron의 가중치과 됨</li><li>결괏값의 사이즈를 정하기 위해서는 Stride, Padding, Depth 고려 필요<ul><li>Stride: filter를 몇 칸 이동할지 결정</li><li>Padding: input 주변에 0으로 padding을 삽입</li><li>Depth number of filter: 3차원상의 neuron의 깊이를 결정</li></ul></li></ul></li></ul><h3 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h3><ul><li>Convolutional Layer 사이에 Pooling Layer를 넣는 방법이 많이 사용됨<ul><li>추출한 이미지에서 지역적인 부분 특징만 뽑아 다음 layer로 넘겨줌</li><li>이를 통해 ① 가중치의 수를 줄일 수 있으며, ② 과적합(overfitting)을 방지할 수 있음</li><li>대표적으로 가장 큰 값(Local Maxima)만을 뽑아내는 Max Pooling이 많이 사용됨</li></ul></li></ul><h3 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h3><ul><li>Modified National Institute of Standards and Technology</li><li>손으로 쓴 숫자를 인식하기 위해 사용되는 데이터<ul><li>28x28 pixel(784)의 흑백 이미지(0-255)들이 있음</li><li>0부터 9까지 총 70,000개의 손글씨 이미지가 있음</li></ul></li></ul><h3 id="CNN-in-R"><a href="#CNN-in-R" class="headerlink" title="CNN in R"></a>CNN in R</h3><ul><li>신경망 모델 생성을 위한 패키지: mxnet<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># Load MNIST mn1</span><br><span class="line"># 28*28, 1 channel images</span><br><span class="line">mn1 &lt;- read.csv(&quot;mini_mnist.csv&quot;)</span><br><span class="line">set.seed(123,sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&lt;-nrow(mn1)</span><br><span class="line">tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line"># split train data and test data</span><br><span class="line">train_data&lt;-data.matrix(mn1[tr.idx,])</span><br><span class="line">test_data&lt;-data.matrix(mn1[-tr.idx,])</span><br><span class="line"></span><br><span class="line"># 0과 1 사이에 분포하도록 Nomalized(0: 검정색 &#x2F; 255: 흰색)</span><br><span class="line">test&lt;-t(test_data[,-1]&#x2F;255)</span><br><span class="line">features&lt;-t(train_data[,-1]&#x2F;255)</span><br><span class="line">labels&lt;-train_data[,1]</span><br><span class="line"></span><br><span class="line"># data preprocession</span><br><span class="line">features_array &lt;- features</span><br><span class="line"># 입력 데이터의 차원을 설정(픽셀*객체 개수)</span><br><span class="line"># ncol(features): 학습 데이터 수(866)</span><br><span class="line">dim(features_array) &lt;- c(28,28,1,ncol(features))</span><br><span class="line">test_array &lt;- test</span><br><span class="line">dim(test_array) &lt;- c(28,28,1,ncol(test))</span><br><span class="line"></span><br><span class="line">ncol(features)</span><br><span class="line">table(labels)</span><br></pre></td></tr></table></figure></li><li>Convolutional Layer 구성<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># Build cnn model</span><br><span class="line"># first conv layers</span><br><span class="line">my_input &#x3D; mx.symbol.Variable(&#39;data&#39;)</span><br><span class="line">conv1 &#x3D; mx.symbol.Convolution(data&#x3D;my_input, kernel&#x3D;c(4,4), stride&#x3D;c(2,2), pad&#x3D;c(1,1), num.filter &#x3D; 20, name&#x3D;&#39;conv1&#39;)</span><br><span class="line">relu1 &#x3D; mx.symbol.Activation(data&#x3D;conv1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu1&#39;)</span><br><span class="line">mp1 &#x3D; mx.symbol.Pooling(data&#x3D;relu1, kernel&#x3D;c(2,2), stride&#x3D;c(2,2), pool.type&#x3D;&#39;max&#39;, name&#x3D;&#39;pool1&#39;)</span><br><span class="line"></span><br><span class="line"># second conv layers</span><br><span class="line">conv2 &#x3D; mx.symbol.Convolution(data&#x3D;mp1, kernel&#x3D;c(3,3), stride&#x3D;c(2,2), pad&#x3D;c(1,1), num.filter &#x3D; 40, name&#x3D;&#39;conv2&#39;)</span><br><span class="line">relu2 &#x3D; mx.symbol.Activation(data&#x3D;conv2, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu2&#39;)</span><br><span class="line">mp2 &#x3D; mx.symbol.Pooling(data&#x3D;relu2, kernel&#x3D;c(2,2), stride&#x3D;c(2,2), pool.type&#x3D;&#39;max&#39;, name&#x3D;&#39;pool2&#39;)</span><br><span class="line"></span><br><span class="line"># fully connected</span><br><span class="line">fc1 &#x3D; mx.symbol.FullyConnected(data&#x3D;mp2, num.hidden &#x3D; 1000, name&#x3D;&#39;fc1&#39;)</span><br><span class="line">relu3 &#x3D; mx.symbol.Activation(data&#x3D;fc1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu3&#39;)</span><br><span class="line">fc2 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu3, num.hidden &#x3D; 3, name&#x3D;&#39;fc2&#39;)</span><br><span class="line"></span><br><span class="line"># softmax</span><br><span class="line">sm &#x3D; mx.symbol.SoftmaxOutput(data&#x3D;fc2, name&#x3D;&#39;sm&#39;)</span><br><span class="line"></span><br><span class="line"># training</span><br><span class="line">mx.set.seed(100,sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">device &lt;- mx.cpu()</span><br><span class="line">model &lt;- mx.model.FeedForward.create(symbol&#x3D;sm, </span><br><span class="line">                                     optimizer &#x3D; &quot;sgd&quot;,</span><br><span class="line">                                     array.batch.size&#x3D;30,</span><br><span class="line">                                     num.round &#x3D; 70, learning.rate&#x3D;0.1,</span><br><span class="line">                                     X&#x3D;features_array, y&#x3D;labels, ctx&#x3D;device,</span><br><span class="line">                                     eval.metric &#x3D; mx.metric.accuracy,</span><br><span class="line">                                     epoch.end.callback&#x3D;mx.callback.log.train.metric(100))</span><br><span class="line">graph.viz(model$symbol)</span><br><span class="line"></span><br><span class="line"># test</span><br><span class="line">predict_probs &lt;- predict(model, test_array)</span><br><span class="line">predicted_labels &lt;- max.col(t(predict_probs)) - 1</span><br><span class="line">table(test_data[, 1], predicted_labels)</span><br><span class="line">sum(diag(table(test_data[, 1], predicted_labels)))&#x2F;length(predicted_labels)</span><br></pre></td></tr></table></figure></li><li>네트워크 시각화 함수: graph.viz(model$symbol)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">graph.viz(model$symbol)</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-텍스트-마이닝"><a href="#3-텍스트-마이닝" class="headerlink" title="3. 텍스트 마이닝"></a>3. 텍스트 마이닝</h2><h3 id="텍스트-마이닝이란"><a href="#텍스트-마이닝이란" class="headerlink" title="텍스트 마이닝이란?"></a>텍스트 마이닝이란?</h3><ul><li>텍스트마이닝(Text mining)<ul><li>웹페이지, 이메일, 소셜네트워크 기록 등 전자문서 파일로부터 특정 연관성(동시적으로 빈도 높은 단어 추출)을 분석하는 방법</li><li>다양한 방식의 알고리즘을 이용해 대용량의 텍스트로부터 트렌드와 관심어를 찾아내는 기법으로 사용</li></ul></li></ul><h3 id="텍스트-마이닝-필요-패키지"><a href="#텍스트-마이닝-필요-패키지" class="headerlink" title="텍스트 마이닝 필요 패키지"></a>텍스트 마이닝 필요 패키지</h3><ul><li>Natural language processing<ul><li>install.packages(‘NLP’)</li></ul></li><li>text mining package<ul><li>install.packages(‘tm’)</li></ul></li><li>visualizing<ul><li>install.packages(‘wordcloud’)</li></ul></li><li>color displaying<ul><li>install.packages(‘RColorBrewer’)</li></ul></li><li>Korean processing<ul><li>install.packages(‘KoNLP’)</li></ul></li><li>import twitter data<ul><li>install.packages(‘twitteR’)</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># set library (set in order)</span><br><span class="line">library(NLP)</span><br><span class="line">library(tm)</span><br><span class="line">library(RColorBrewer)</span><br><span class="line">library(wordcloud)</span><br></pre></td></tr></table></figure><ul><li>사용 데이터: tm에 포함된 crude<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 20 new articles from Reuter- 21578 data set</span><br><span class="line">data(crude)</span><br><span class="line"># To know abour crude data</span><br><span class="line">help(crude)</span><br></pre></td></tr></table></figure><h3 id="텍스트-마이닝-함수"><a href="#텍스트-마이닝-함수" class="headerlink" title="텍스트 마이닝- 함수"></a>텍스트 마이닝- 함수</h3></li></ul><p>함수|설명 및 예제코드<br>|—|—|<br>str(x[[1]])|데이터 구조 정보(첫 번째 파일 구조) / str(crude[[1]])<br>content(x[[1]])|문서 내용(첫 번째 분서 내용)<br>meta(X)|메타정보(x에 기록된 저자, 날짜, id 등 정보) / meta(ctude[[1]])<br>inspect(X)|코퍼스, 텍스트, 문서행렬 등 정보 제공 / data(crude), inspect(crude[1:3]), inspect(crude[[1]]), tdm &lt;- TermDocumentMatrix(crude), inspect(tdm)<br>lapply(x, content)|파일 내용을 보여줌 / lapply(crude, content)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># information about the first file in crude data</span><br><span class="line">str(crude[[1]])</span><br><span class="line">content(crude[[1]])</span><br><span class="line">meta(crude[[1]])</span><br><span class="line">lapply(crude, content)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># inspect function</span><br><span class="line">inspect(crude[1:3]) </span><br><span class="line">inspect(crude[[1]]) </span><br></pre></td></tr></table></figure><h3 id="텍스트-마이닝-전처리-함수"><a href="#텍스트-마이닝-전처리-함수" class="headerlink" title="텍스트 마이닝- 전처리 함수"></a>텍스트 마이닝- 전처리 함수</h3><p>함수|설명<br>|—|—|<br>tm_map(x, removePunctuation)|문장부호 제거(. , “” ‘’)<br>tm_map(x, stripWhitespace)|공백문자 제거<br>tm_map(x, removeNumbers)|숫자 제거</p><h3 id="텍스트-마이닝-실습"><a href="#텍스트-마이닝-실습" class="headerlink" title="텍스트 마이닝 실습"></a>텍스트 마이닝 실습</h3><ul><li><p>문장부호 없애기: tm_map(x, removePunctuation)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 1. remove punctuation in documnet</span><br><span class="line">crude&lt;-tm_map(crude, removePunctuation)</span><br><span class="line">content(crude[[1]])</span><br></pre></td></tr></table></figure></li><li><p>숫자 제거: tm_map(x, removeNumbers)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 2. remove numbers</span><br><span class="line">crude&lt;-tm_map(crude, removeNumbers)</span><br><span class="line">content(crude[[1]])</span><br></pre></td></tr></table></figure></li><li><p>스톱워즈 제거: 언어별로 다르며 별도 지정 가능</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 3. remove stopwords</span><br><span class="line">crude&lt;-tm_map(crude, function(x) removeWords(x,stopwords()))</span><br><span class="line">content(crude[[1]])</span><br></pre></td></tr></table></figure></li><li><p>참고: 스톱워즈 리스트</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stopwords() #stopwords(&quot;en&quot;) 174개, stopwords(&quot;SMART&quot;)&quot; 517개</span><br></pre></td></tr></table></figure></li><li><p>문서 행렬 구성: TermDocumentMatrix(문서이름)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 4. contruct term-doucument matrix </span><br><span class="line">tdm&lt;-TermDocumentMatrix(crude)</span><br><span class="line">inspect(tdm)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>DOCS 144: crude 문서 번호 144에 나오는 단어들의 빈도</li></ul></li><li><p>문서 행렬을 행렬로 변환</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 5. read tdm as a matrix</span><br><span class="line">m&lt;-as.matrix(tdm)</span><br><span class="line">head(m)</span><br></pre></td></tr></table></figure></li><li><p>단어의 빈도 순서로 정렬</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 6. sorting in high frequency to low frequency </span><br><span class="line">v&lt;-sort(rowSums(m), decreasing&#x3D;TRUE)</span><br><span class="line">v[1:10] # 가장 높은 것부터 [1:10]번까지</span><br></pre></td></tr></table></figure></li><li><p>텍스트마이닝 수행</p><ul><li>단어 이름과 빈도를 결합한 행렬을 데이터 프레임으로 저장</li><li>crude 관련 기사 파일로부터 962개 단어를 분류해 빈도 계산<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 7. match with freq and word names</span><br><span class="line">d&lt;-data.frame(word&#x3D;names(v), freq&#x3D;v)</span><br><span class="line">head(d) #가장 빈도 높은 단어 6개</span><br><span class="line">d[957:962, ] #가장 빈도 낮은 단어 6개</span><br></pre></td></tr></table></figure></li></ul></li><li><p>텍스트마이닝 결과 그리기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 7. plot a word cloud</span><br><span class="line">wordcloud(d$word, d$freq)</span><br><span class="line"># help(wordcloud)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 7-1. Now lets try it with frequent words plotted first</span><br><span class="line"># par(mfrow &#x3D; c(1, 1),mar&#x3D;c(1,2,3,1))</span><br><span class="line">wordcloud(d$word,d$freq,c(8,.3),2,,FALSE,.1)</span><br><span class="line"></span><br><span class="line"># 7-2. color plot with frequent words plotted first</span><br><span class="line">pal &lt;- brewer.pal(9,&quot;BuGn&quot;)</span><br><span class="line">pal &lt;- pal[-(1:4)]</span><br><span class="line">wordcloud(d$word,d$freq,c(8,.3),2,,FALSE,,.15,pal)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;ⅩⅥ-딥러닝과-텍스트-마이닝&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="딥러닝" scheme="https://ne-choi.github.io/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"/>
    
    <category term="텍스트마이닝" scheme="https://ne-choi.github.io/tags/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 2: ⅩⅤ. 주성분 분석과 부분 최소자승법</title>
    <link href="https://ne-choi.github.io/2020/12/23/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A4_%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D%EA%B3%BC_%EB%B6%80%EB%B6%84_%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/"/>
    <id>https://ne-choi.github.io/2020/12/23/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A4_%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D%EA%B3%BC_%EB%B6%80%EB%B6%84_%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/</id>
    <published>2020-12-23T00:00:00.000Z</published>
    <updated>2021-02-01T00:51:25.404Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="ⅩⅤ-주성분-분석과-부분-최소자승법"><a href="#ⅩⅤ-주성분-분석과-부분-최소자승법" class="headerlink" title="ⅩⅤ. 주성분 분석과 부분 최소자승법"></a>ⅩⅤ. 주성분 분석과 부분 최소자승법</h1><h2 id="1-주성분분석"><a href="#1-주성분분석" class="headerlink" title="1. 주성분분석"></a>1. 주성분분석</h2><h3 id="주성분분석-PCA"><a href="#주성분분석-PCA" class="headerlink" title="주성분분석(PCA)"></a>주성분분석(PCA)</h3><ul><li><p>주성분분석(Principle Component Analysis)</p><ul><li>다변량 분석기법</li><li>‘주성분’이라고 불리는 선형조합으로 표현하는 기법</li><li>주성분은 공분산(X^T^X)로부터 eigenvector와 eigenvalue를 도출하여 계산됨</li></ul></li><li><p>주성분 간의 수직 관계</p><ul><li>1st 주성분(PC1): 독립변수들의 변동(분산)을 가장 많이 설명하는 성분</li><li>2nd 주성분(PC2): PC1과 수직인 주성분(1st 주성분이 설명하지 못하는 변동을 두 번째로 설명하는 성분)</li></ul></li></ul><ul><li>iris 데이터(4개변수)의 주성분 도출: 차원축소&amp;예측력 향상 목적<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week15_1&#x2F;iris.csv&quot;)</span><br><span class="line">iris$Species&lt;-as.factor(iris$Species)</span><br><span class="line">attach(iris)</span><br><span class="line">head(iris)</span><br></pre></td></tr></table></figure></li><li>독립변수 간 상관관계 확인<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Check correlation</span><br><span class="line">cor(iris[1:4])</span><br></pre></td></tr></table></figure></li><li>결과 확인<ul><li>0.96, 0.87 등 높은 상관계수가 관찰됨</li></ul></li></ul><h3 id="주성분분석을-위한-함수"><a href="#주성분분석을-위한-함수" class="headerlink" title="주성분분석을 위한 함수"></a>주성분분석을 위한 함수</h3><ul><li>prcomp(독립변수들, center= , scale= )<ul><li>옵션을 주지 않으면 center=T, scale=F</li><li>center=T, scale=T는 변수들의 평균을 빼고 편차로 나누어 표준화한다는 의미<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 1.PCA(center&#x3D;T-&gt;mean&#x3D;0, scale.&#x3D;T-&gt;variance&#x3D;1)</span><br><span class="line">ir.pca&lt;-prcomp(iris[,1:4],center&#x3D;T,scale.&#x3D;T)</span><br><span class="line">ir.pca</span><br><span class="line">summary(ir.pca)</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>PC1 = 0.5211xSepal.Length - 0.2693xSepal.Width + 0.5804xPetal.Length + 0.5649xPetal.Width</li><li>summary의 Proportion of Variance<ul><li>전체 분산 중 각 주성분이 설명하는 비율</li><li>PC1: 전체 분산의 72.96%를 설명</li><li>PC2: 22.85%, PC3: 3.67%, PC4: 0.5%<br>→ 누적설명비율을 보면, PC1과 PC2 두 성분으로 전체 분산의 95.81%를 설명</li></ul></li></ul></li></ul><h3 id="최적-주성분-수-찾기"><a href="#최적-주성분-수-찾기" class="headerlink" title="최적 주성분 수 찾기"></a>최적 주성분 수 찾기</h3><ul><li>scree plot을 그려보고 급격히 떨어지기 전까지의 PC를 선택<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 2.scree plot : to choose the number of components</span><br><span class="line">plot(ir.pca,type&#x3D;&quot;l&quot;)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>3rd PC에서 설명력이 급격하게 떨어짐</li><li>기울기가 꺾이는 PC3을 elbow point라고 부름<br>→ PC1, PC2까지 사용하는 것을 추천</li></ul></li></ul><ul><li>BAR Chart로 보기: screeplt(pca 결과)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># either way to draw scree plot</span><br><span class="line">screeplot(ir.pca)</span><br></pre></td></tr></table></figure></li><li>PC계산 = X_data(n* p) % * % PCA_weight(p*p)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># %*%: 행렬의 계산</span><br><span class="line"># 3. Calculate component &#x3D; x_data%*% PCA_weight</span><br><span class="line">PRC&lt;-as.matrix(iris[,1:4])%*%ir.pca$rotation</span><br><span class="line">head(PRC)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>PRC는 n*p행렬, 여기서는 150x4</li><li>PC1 = 0.5211xSepal.Length - 0.2693xSepal.Width + 0.5804xPetal.Length + 0.5649xPetal.Width</li></ul></li></ul><h3 id="주성분을-이용한-분류모형"><a href="#주성분을-이용한-분류모형" class="headerlink" title="주성분을 이용한 분류모형"></a>주성분을 이용한 분류모형</h3><ul><li>iris data → iris.pc data 구성<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 4. classification using principal components</span><br><span class="line"># make data with components</span><br><span class="line">iris.pc&lt;-cbind(as.data.frame(PRC), Species)</span><br><span class="line">iris.pc$Species&lt;-as.factor(iris.pc$Species)</span><br><span class="line">head(iris.pc)</span><br></pre></td></tr></table></figure></li><li>주성분을 이용한 서포트벡터머신 수행<ul><li>주성분이 input<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 5. support vector machine</span><br><span class="line"># install.packages(&quot;e1071&quot;)</span><br><span class="line">library (e1071)</span><br><span class="line"></span><br><span class="line"># classify all data using PC1-PC4 using support vector machine</span><br><span class="line">m1&lt;- svm(Species ~., data &#x3D; iris.pc, kernel&#x3D;&quot;linear&quot;)</span><br><span class="line"># m2&lt;- svm(Species ~PC1+PC2, data &#x3D; iris.pc, kernel&#x3D;&quot;linear&quot;)</span><br><span class="line">summary(m1)</span><br></pre></td></tr></table></figure></li></ul></li><li>서포트벡터머신 결과 vs PCA 결과<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># predict class for all data </span><br><span class="line">x&lt;-iris.pc[, -5]</span><br><span class="line">pred &lt;- predict(m1, x)</span><br><span class="line"># check accuracy between true class and predicted class</span><br><span class="line">y&lt;-iris.pc[,5]</span><br><span class="line">table(pred, y)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>주성분을 이용한 분류의 오분류율: 2/150 = 0.013(1.33%)<br>→ 이 데이터에서는 SVM(오분류율: 4/150 = 2.66%)보다 PCA 분류가 오분류율이 적다</li></ul></li></ul><h2 id="2-주성분-회귀분석"><a href="#2-주성분-회귀분석" class="headerlink" title="2. 주성분 회귀분석"></a>2. 주성분 회귀분석</h2><h3 id="주성분회귀"><a href="#주성분회귀" class="headerlink" title="주성분회귀"></a>주성분회귀</h3><ul><li><p>주성분회귀(Principle Component Regression)</p><ul><li><p>독립변수 차원을 줄이기 위해 사용 가능, 주성분을 이용해 타겟변수(Y)의 설명력(예측력)을 높일 수 있음</p><ul><li>Y = b<del>0</del> + b<del>1</del>PC<del>1</del> + b<del>2</del>PC<del>2</del></li></ul></li><li><p>독립변수의 전체분산을 가장 잘 설명하는 component를 사용해 독립변수 간 다중공선성 문제를 해결할 수 있음</p></li><li><p>주요 component score가 Y의 예측력을 보장하지는 않음</p><ul><li>주요 component score는 X의 분산을 가장 잘 설명하는 방향의 축을 기준으로 변환된 것으로 Y와의 관계에는 상관성이 없을 수도 있음</li></ul></li></ul></li><li><p><strong>주성분 회귀 분석 순서</strong></p><ol><li>데이터에 다중공선성이 있는지 체크</li><li>주성분 분석을 위한 데이터 전처리(mean-centering or scaling)</li><li>주성분분석</li><li>주성분 개수 결정</li><li>주성분으로 회귀분석모형 수행  </li></ol></li><li><p><strong>주성분회귀분석</strong></p><ul><li>여러 변수를 주성분이라는 새로운 변수로 축소하여 회귀모형 수행</li><li>다중공선성 문제 해결 가능</li><li>적절한 주성분을 사용해 회귀모형 구현 필요</li><li>주성분 순서대로 Y변수에 대한 설명력이 높은 것은 아님  </li></ul></li><li><p>wine 데이터: 독립변수 9개, 타겟변수 Aroma rating</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">wine&lt;-read.csv(&quot;data&#x2F;week15_2&#x2F;wine_aroma.csv&quot;)</span><br><span class="line">attach(wine)</span><br><span class="line">head(wine)</span><br><span class="line"></span><br><span class="line"># Check correlation</span><br><span class="line">cor(wine[1:9])</span><br></pre></td></tr></table></figure></li><li><p>주성분분석</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 1. PCA(center&#x3D;T → mean&#x3D;0, scale.&#x3D;T → variance&#x3D;1)</span><br><span class="line">wi.pca&lt;-prcomp(wine[1:9], center&#x3D;T, scale.&#x3D;F)</span><br><span class="line">wi.pca</span><br><span class="line">summary(wi.pca)</span><br></pre></td></tr></table></figure></li><li><p>최적 주성분 수 찾기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 2.scree plot : to choose the number of components</span><br><span class="line">plot(wi.pca,type&#x3D;&quot;l&quot;)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>2rd PC에서 설명력이 급격히 떨어짐<br>→ 이 경우 PC1만 사용해도 됨</li></ul></li><li><p>PC 계산</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 3. calculate component&#x3D;x_data%*% PCA weight</span><br><span class="line">PRC&lt;-as.matrix(wine[,1:9])%*%wi.pca$rotation</span><br><span class="line">head(PRC)</span><br></pre></td></tr></table></figure></li><li><p>wine.pc data 구성</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 4. Principal component regression</span><br><span class="line"># make data with components</span><br><span class="line">wine.pc&lt;-cbind(as.data.frame(PRC),Aroma)</span><br><span class="line">head(wine.pc)</span><br></pre></td></tr></table></figure></li></ul><h3 id="주성분을-이용한-회귀모형"><a href="#주성분을-이용한-회귀모형" class="headerlink" title="주성분을 이용한 회귀모형"></a>주성분을 이용한 회귀모형</h3><ul><li><p>다중회귀모형과 주성분회귀분석</p><ul><li><p>다중회귀모형</p><ul><li>Y = β<del>0</del> + β<del>1</del>X<del>1</del> + β<del>2</del>X<del>2</del> + … + β<del>K</del>X<del>K</del></li></ul></li><li><p>주성분회귀모형</p><ul><li>Y = β<del>0</del> + β<del>1</del>PC<del>1</del> + β<del>2</del>PC<del>2</del> + …</li><li>타겟값(Y)를 가장 잘 예측하는 선형모형</li></ul></li></ul></li></ul><ul><li>주성분을 이용한 회귀분석모형 1(WINE DATA: PC1-PC4 포함)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># regression(PC1-PC4)</span><br><span class="line">fit1&lt;-lm(Aroma~PC1+PC2+PC3+PC4, data&#x3D;wine.pc)</span><br><span class="line">fit1</span><br><span class="line">summary(fit1)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>PC3의 p-value는 0.7로 매우 높음 → 별로 중요하지 않음</li><li>R^2^= .494</li></ul></li></ul><ul><li>주성분을 이용한 회귀분석모형 2(WINE DATA: PC1-PC4 포함)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># regression(PC1-PC9)</span><br><span class="line">fit2&lt;-lm(Aroma~., data&#x3D;wine.pc)</span><br><span class="line">fit2</span><br><span class="line">summary(fit2)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>R^2^= .741</li></ul></li></ul><ul><li>일반 회귀분석모형(wine data: raw data)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Multiple regression with the raw data</span><br><span class="line">fit3&lt;-lm(Aroma ~., data&#x3D;wine)</span><br><span class="line">summary(fit3)</span><br></pre></td></tr></table></figure></li><li>잔차에 대한 가정 확인<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># residual diagnostic plot </span><br><span class="line">layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page </span><br><span class="line">plot(fit3)</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-Partial-Least-Square-Regression"><a href="#3-Partial-Least-Square-Regression" class="headerlink" title="3. Partial Least Square Regression"></a>3. Partial Least Square Regression</h2><h3 id="최소자승회귀법-PLS"><a href="#최소자승회귀법-PLS" class="headerlink" title="최소자승회귀법(PLS)"></a>최소자승회귀법(PLS)</h3><ul><li>주성분분석의 component vs 최소자승회귀법의 component<ul><li>PLS는 공정변수의 변동을 설명하는 벡터 t를 구하는 데 X 정보만을 이용하지 않고 타겟변수 y 정보를 동시에 고려하여 도출<ul><li>t<del>1</del>(PLS): Y쪽으로 Shift</li></ul></li><li>Chemometrics, Marketing 분야의 고차원데이터, 독립변수 간 상관성 높은 데이터에 적용</li></ul></li></ul><ul><li>PLS 수행을 위한 패키지 설치<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install package for Partial Least Square </span><br><span class="line">#install.packages(&#39;pls&#39;)</span><br><span class="line">library(pls)</span><br></pre></td></tr></table></figure></li><li>사용 데이터 설명<ul><li>가솔린 데이터(근적외선 흡광고, 60개의 가솔린 표본)<ul><li>독립변수 차원:401</li><li>타겟변수(Y): 옥탄가(octane numbers)</li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data(gasoline)</span><br><span class="line">#help(&quot;gasoline&quot;)</span><br><span class="line">attach(gasoline)</span><br></pre></td></tr></table></figure><ul><li>데이터 요약 설명(타겟변수 Y: 옥탄가)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># descriptive statistics</span><br><span class="line">par(mfrow&#x3D;c(1,1))</span><br><span class="line">hist(octane, col&#x3D;3)</span><br><span class="line">summary(octane)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>옥탄가의 최솟값 83.4, 최댓값 89.6</li><li>히스토그램은 옥탄가의 분포를 보여줌</li></ul></li></ul><ul><li><p>훈련데이터와 검증데이터(50개/10개)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># train and test set</span><br><span class="line">gasTrain &lt;- gasoline[1:50, ]</span><br><span class="line">gasTest &lt;- gasoline[51:60, ]</span><br><span class="line"></span><br><span class="line"># 1.check how many principal components</span><br><span class="line">ga.pca&lt;-prcomp(gasoline$NIR,center&#x3D;T,scale.&#x3D;F)</span><br><span class="line">ga.pca</span><br><span class="line">summary(ga.pca)</span><br><span class="line">plot(ga.pca,type&#x3D;&quot;l&quot;)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>최소 5개 정도 사용하기</li></ul></li><li><p>PLS함수: plsr</p><ul><li>plsr(타겟변수~독립변수, ncomp= , data= )</li><li>옵션사항<ul><li>ncomp: 잠재변수 수</li><li>validation=c(“none”, “CV”, “LOO”) (CV: cross-validaton, LOO: leave-one-out)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># pls model by training set (find LV by leave-one-out) </span><br><span class="line"># 1. start with 10 component PLS model</span><br><span class="line">gas1 &lt;- plsr(octane ~ NIR, ncomp &#x3D; 10, data &#x3D; gasTrain, validation &#x3D; &quot;LOO&quot;)</span><br><span class="line"># NIR에 401차원의 값이 들어있음, ncomp: 잠재변수의 수</span><br><span class="line"></span><br><span class="line">summary(gas1)</span><br></pre></td></tr></table></figure></li></ul></li></ul></li><li><p>결과 해석</p><ul><li>CV<ul><li>1개의 잠재변수 → 10개의 잠재변수</li><li>1개의 잠재변수: 평균오차 1.357, 2개의 잠재변수: 평균오차 0.297 …</li></ul></li><li>TRAINING<ul><li>X들의 분산설명비율: 2개의 LV로 85.58%</li><li>Y값의 변동분 설명비율: 96.85% 설명</li></ul></li></ul></li></ul><ul><li>PLS 모형에서의 최적 잠재변수 수<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 2. to choose the number of components</span><br><span class="line">plot(RMSEP(gas1), legendpos &#x3D; &quot;topright&quot;, pch&#x3D;46, cex&#x3D;1.0, main&#x3D;&quot;Cross-validation for # of LV&quot;)</span><br><span class="line"># for gasoline data, # of LV&#x3D;2</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>최적 잠재변수의 수는 RMSEP가 최저이고 변화가 없는 지점에서 결정<ul><li>2개의 components (LV)를 추천</li><li>예측모형 평가척도: 평균오차</li></ul></li></ul></li></ul><ul><li>최적 PLS 모형의 실제값과 예측값 산점도<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 3. Display the PLS model with LV&#x3D;2</span><br><span class="line"># scatterplot with true and predicted</span><br><span class="line">plot(gas1, ncomp &#x3D; 2, asp &#x3D; 1, line &#x3D; TRUE, cex&#x3D;1.5,main&#x3D;&quot;Measured vs Predicted&quot;, xlab&#x3D;&quot;Measured&quot; )</span><br></pre></td></tr></table></figure></li><li>잠재변수 수에 따른 전체분산의(독립변수들) 설명 정도<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Check explained variances proportion for X</span><br><span class="line">explvar(gas1)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>2개의 잠재변수가 전체 분산의 85.58% 설명</li></ul></li></ul><ul><li><p>검증데이터의 RMSEP 계산</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 4. predicted Y for test data</span><br><span class="line">ypred&lt;-predict(gas1, ncomp &#x3D; 2, newdata &#x3D; gasTest)</span><br><span class="line"></span><br><span class="line">y&lt;-gasoline$octane[51:60]</span><br><span class="line"></span><br><span class="line"># check : RMSEP for test data</span><br><span class="line">sqrt((sum(y-ypred)^2)&#x2F;10)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 5. compare with the one from #4 : RMSEP for test data</span><br><span class="line">RMSEP(gas1, newdata &#x3D; gasTest)</span><br></pre></td></tr></table></figure></li><li><p>PLS 예측값 내보내기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># output of y and predicted y</span><br><span class="line">out1&lt;-cbind(y, ypred)</span><br><span class="line"># data exporting</span><br><span class="line">write.csv(out1,file&#x3D;&quot;out1.csv&quot;, row.names &#x3D; FALSE)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;ⅩⅤ-주성분-분석과-부분-최소자승법&quot;&gt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="주성분분석" scheme="https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"/>
    
    <category term="최소자승법" scheme="https://ne-choi.github.io/tags/%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 2: ⅩⅣ. 연관규칙과 로지스틱회귀분석</title>
    <link href="https://ne-choi.github.io/2020/12/22/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A3_%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99%EA%B3%BC_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/"/>
    <id>https://ne-choi.github.io/2020/12/22/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A3_%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99%EA%B3%BC_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/</id>
    <published>2020-12-22T00:00:00.000Z</published>
    <updated>2021-02-01T01:09:49.587Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="ⅩⅣ-연관규칙과-로지스틱회귀분석"><a href="#ⅩⅣ-연관규칙과-로지스틱회귀분석" class="headerlink" title="ⅩⅣ. 연관규칙과 로지스틱회귀분석"></a>ⅩⅣ. 연관규칙과 로지스틱회귀분석</h1><h2 id="1-연관규칙-분석-Ⅰ"><a href="#1-연관규칙-분석-Ⅰ" class="headerlink" title="1. 연관규칙 분석 Ⅰ"></a>1. 연관규칙 분석 Ⅰ</h2><h3 id="연관규칙"><a href="#연관규칙" class="headerlink" title="연관규칙"></a>연관규칙</h3><ul><li><p>연관규칙(Association Rule)</p><ul><li>대용량 데이터베이스의 트랜잭션에서 빈번하게 발생하는 패턴을 발견</li><li>거래 간의 상호 관련성을 분석</li></ul></li><li><p>연관규칙 예시</p><ul><li>신발을 구매하는 고객의 10%는 양말을 동시에 구매한다.</li><li>빵과 우유를 고매한 고객의 50%가 쥬스도 함께 구매한다.</li></ul></li><li><p>용어 설명</p><ul><li><strong>시장바구니(market basket)</strong><ul><li>고객이 구매한 물품에 관련한 정보(구매 시기, 지불 방법, 매장 정보 포함)</li></ul></li><li><strong>트렌잭션(transaction)</strong><ul><li>고객이 거래한 정보를 하나의 트랜잭션이라고 함</li></ul></li><li><strong>시장바구니 분석(market basket analysis)</strong><ul><li>시장바구니 데이터로부터 연관규칙 탐색 분석</li></ul></li></ul></li></ul><h3 id="연관규칙-평가-척도"><a href="#연관규칙-평가-척도" class="headerlink" title="연관규칙 평가 척도"></a>연관규칙 평가 척도</h3><ul><li><p><strong>지지도(Support)</strong></p><ul><li>$\frac{A와 B를 동시에 포함하는 거래 수}{전체 거래 수}$  </li></ul></li><li><p><strong>신뢰도(Confidence)</strong></p><ul><li>$\frac{A와 B를 동시에 포함하는 거래 수}{A를 포함하는 거래 수}$  </li></ul></li><li><p><strong>향상도(Lift)</strong></p><ul><li>$\frac{A와 B를 동시에 포함하는 거래 수}{A를 포함하는 거래 수 x B를 포함하는 거래 수}$</li></ul></li></ul><br>  - 지지도가 어느 정도 수준에 도달해야 함- 신뢰도가 높을 경우에는 두 항목 A → B에서 항목 B의 확률이 커야 연관규칙이 의미가 있음- 향상도가 1보다 큰 값을 주어야 유용한 정보를 준다고 볼 수 있음<h3 id="향상도-Lift"><a href="#향상도-Lift" class="headerlink" title="향상도(Lift)"></a>향상도(Lift)</h3><ul><li>향상도: A가 거래된 경우, 그 거래가 B를 포함하는 경우와 B가 임의로 거래되는 경우의 비율</li></ul><table><thead><tr><th align="center">향상도</th><th align="center">의미</th></tr></thead><tbody><tr><td align="center">1</td><td align="center">두 항목의 거래 발생이 독립적인 관계</td></tr><tr><td align="center">&lt; 1</td><td align="center">두 항목의 거래 발생이 서로 음의 상관관계</td></tr><tr><td align="center">&gt; 1</td><td align="center">두 항목의 거래 발생이 서로 양의 상관관계</td></tr></tbody></table><ul><li>정리<ul><li>각 항목의 구매가 상호관련이 없다면 P(B|A)가 P(B)와 같아 향상도는 1이 됨</li><li>1보다 크면 결과 예측에 관해 우연적 기회(random chance)보다 우수함을 의미</li><li>향상도 값이 클수록 A의 거래 여부가 B의 거래 여부에 큰 영향을 미침</li></ul></li></ul><h3 id="연관규칙-수행-패키지"><a href="#연관규칙-수행-패키지" class="headerlink" title="연관규칙 수행 패키지"></a>연관규칙 수행 패키지</h3><ul><li>arules</li><li>데이터가 transaction data로 Dataframe과 구조가 다름<ul><li>ID, 거래된 아이템, 거래된 일자 정보 등<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># association rule analysis package</span><br><span class="line">#install.packages(&quot;arules&quot;)</span><br><span class="line">library(arules)</span><br><span class="line"></span><br><span class="line"># data import-&gt; make transaction data</span><br><span class="line">dvd1&lt;-read.csv(&quot;data&#x2F;week14_1&#x2F;dvdtrans.csv&quot;)</span><br><span class="line">dvd1</span><br></pre></td></tr></table></figure></li></ul></li><li>arules package를 통해 transaction 데이터 변환과 연관규칙 분석 수행</li><li>Split을 통해 ID별로 item을 as함수를 통해 transaction 데이터로 변환<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dvd.list&lt;-split(dvd1$Item,dvd1$ID)</span><br><span class="line">dvd.list</span><br><span class="line">dvd.trans&lt;-as(dvd.list,&quot;transactions&quot;)</span><br><span class="line">dvd.trans</span><br><span class="line"></span><br><span class="line">inspect(dvd.trans)</span><br></pre></td></tr></table></figure></li><li>transaction 데이터의 요약<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># summary of dvd.trans</span><br><span class="line">summary(dvd.trans)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>10 트랜잭션 / 10 항목</li><li>밀도(density): 0.3 ← 10*10 cell 중, 30%dml cell에 거래가 발생해 데이터가 있다는 뜻</li><li>거래 항목 중 Gladiator = 7번, Patriot = 6번, Six Sense = 6번 순으로 나왔음을 의미</li></ul></li></ul><h3 id="연관규칙-수행함수"><a href="#연관규칙-수행함수" class="headerlink" title="연관규칙 수행함수"></a>연관규칙 수행함수</h3><ul><li>apriori(transaction, parameter=list(support=0.0#, confidence=0.##))<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># for running dvdtras data</span><br><span class="line">dvd_rule&lt;-apriori(dvd.trans,</span><br><span class="line">                  parameter &#x3D; list(support&#x3D;0.2,confidence &#x3D; 0.20,minlen &#x3D; 2))</span><br><span class="line">dvd_rule</span><br><span class="line"></span><br><span class="line"># same code with short command</span><br><span class="line"># dvd_rule&lt;-apriori(dvd.trans, parameter &#x3D; list(supp&#x3D;0.2,conf&#x3D; 0.20,minlen &#x3D; 2))  </span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>support=0.2, confidence=0.2 이상인 13개의 연관규칙 생성</li></ul></li></ul><h3 id="연관규칙의-해석"><a href="#연관규칙의-해석" class="headerlink" title="연관규칙의 해석"></a>연관규칙의 해석</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(dvd_rule)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">inspect(dvd_rule)</span><br></pre></td></tr></table></figure><ul><li><p>결과 해석</p><ul><li>지지도: Green Mile과 Sixth Sense를 동시에 구매할 확률: 20%</li><li>신뢰도: Green Mile을 구매한 경우는 모두 Sixth Sense를 구매: 100%</li><li>향상도: Green Mile을 구매하면 Six Sense 구매비율이 1.667배 향상됨<br></li></ul></li><li><p>그래프로 표현한 연관규칙: 지지도 &gt; 0.2 항목들의 상대빈도</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Bar chart for support&gt;0.2</span><br><span class="line">itemFrequencyPlot(dvd.trans,support&#x3D;0.2,main&#x3D;&quot;item for support&gt;&#x3D;0.2&quot;, col&#x3D;&quot;green&quot;)</span><br></pre></td></tr></table></figure></li></ul><h2 id="2-연관규칙-분석-Ⅱ"><a href="#2-연관규칙-분석-Ⅱ" class="headerlink" title="2. 연관규칙 분석 Ⅱ"></a>2. 연관규칙 분석 Ⅱ</h2><h3 id="데이터-설명-Groceries"><a href="#데이터-설명-Groceries" class="headerlink" title="데이터 설명(Groceries)"></a>데이터 설명(Groceries)</h3><ul><li>Groceries data(“arules” package에 탑재된 데이터)<ul><li>data(“Groceries”)로 불러옴</li><li>실제 식료품점의 30일치 transaction 데이터</li><li>9835 트랜젝션 / 169 항목</li><li>밀도: 0.026% ← 9835*169 cell 중, 0.026%의 cell에 거래가 발생하여 숫자가 차 있다는 의미</li><li>Element(itemset/transaction) length distribution: 하나의 거래 장바구니(row 1개당)에 item의 개수별로 몇 번의 거래가 있었는지 나타냄</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># association rule analysis package</span><br><span class="line"># install.packages(&quot;arules&quot;)</span><br><span class="line">library(arules)</span><br><span class="line"></span><br><span class="line">#association rule analysis</span><br><span class="line">data(&quot;Groceries&quot;)</span><br><span class="line"></span><br><span class="line">summary(Groceries)</span><br></pre></td></tr></table></figure><h3 id="지지도에-따른-시각화"><a href="#지지도에-따른-시각화" class="headerlink" title="지지도에 따른 시각화"></a>지지도에 따른 시각화</h3><ul><li><p>지지도 5% 이상의 item 막대 그래프</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Bar chart for item with support&gt;&#x3D;5%</span><br><span class="line">itemFrequencyPlot(Groceries,supp&#x3D;0.05,main&#x3D;&quot;item for support&gt;&#x3D;5%&quot;, col&#x3D;&quot;green&quot;, cex&#x3D;0.8)</span><br></pre></td></tr></table></figure></li><li><p>지지도 상위 20개 막대 그래프</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Bar chart for the top support 20 items</span><br><span class="line">itemFrequencyPlot(Groceries,topN&#x3D;20,main&#x3D;&quot;support top 20 items&quot;, col&#x3D;&quot;coral&quot;, cex&#x3D;0.8)</span><br></pre></td></tr></table></figure><h3 id="연관규칙-분석결과"><a href="#연관규칙-분석결과" class="headerlink" title="연관규칙 분석결과"></a>연관규칙 분석결과</h3></li><li><p>연관규칙분석</p><ul><li>support, confidence, length는 minimum값으로 너무 높게 잡을 경우 연관규칙 분석이 되지 않음<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Association rule with support&gt;5%, confidence&gt;20% in minimum length 2</span><br><span class="line">Grocery_rule&lt;-apriori(data&#x3D;Groceries,</span><br><span class="line">                      parameter &#x3D; list(support&#x3D;0.05,</span><br><span class="line">                                       confidence &#x3D; 0.20,</span><br><span class="line">                                       minlen &#x3D; 2))</span><br><span class="line">Grocery_rule</span><br></pre></td></tr></table></figure></li></ul></li><li><p>연관규칙 조회 및 평가</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#analyzing result</span><br><span class="line">summary(Grocery_rule)</span><br><span class="line">inspect(Grocery_rule)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>향상도 최솟값이 1보다 큼</li><li>6개의 rules이 items 2개로 구성되어 있음</li></ul></li><li><p>연관규칙: 향상도(lift) 순서로 정렬</p><ul><li>sort() 함수를 통해 분석가가 보려는 기준으로 정렬하여 조회 가능<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># sorting by Lift</span><br><span class="line">inspect(sort(Grocery_rule,by&#x3D;&quot;lift&quot;))</span><br><span class="line"># inspect(sort(Grocery_rule, by&#x3D;&quot;support&quot;))</span><br></pre></td></tr></table></figure></li></ul></li><li><p>연관규칙: 품목별 연관성 탐색</p><ul><li>subset() 함수를 통해 원하는 item이 포함된 연관규칙만 선별해 조회 가능</li><li>%in%. %pin%, %ain% 등 옵션 이용 가능<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># searching association for interesting items</span><br><span class="line">rule_interest&lt;-subset(Grocery_rule, items %in% c(&quot;yogurt&quot;,&quot;whole milk&quot;))</span><br><span class="line">inspect(rule_interest)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>연관규칙결과를 data.frame으로 저장</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># save as dataframe</span><br><span class="line">Grocery_rule_df&lt;-as(Grocery_rule,&quot;data.frame&quot;)</span><br><span class="line">Grocery_rule_df</span><br><span class="line"></span><br><span class="line">#saving results as csv file</span><br><span class="line">write(Grocery_rule, file&#x3D;&quot;Grocery_rule.csv&quot;,</span><br><span class="line">      sep&#x3D;&quot;,&quot;,</span><br><span class="line">      quote&#x3D;TRUE,</span><br><span class="line">      row.names&#x3D;FALSE)</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-로지스틱회귀분석"><a href="#3-로지스틱회귀분석" class="headerlink" title="3. 로지스틱회귀분석"></a>3. 로지스틱회귀분석</h2><h3 id="로지스틱-회귀모형"><a href="#로지스틱-회귀모형" class="headerlink" title="로지스틱 회귀모형"></a>로지스틱 회귀모형</h3><ul><li><p>로지스틱회귀분석(logistic regression)</p><ul><li><p>종속변수가 범주형인 경우 사용</p><ul><li>2개의 범주(양성/음성, 불량/양품 등) 또는 3개 이상의 범주를 다룸</li><li>3개 이상 범주일 경우, 서열형 데이터(ordinal data), 명목형 데이터(nominal data)로 나누어 다른 모형 사용</li></ul></li><li><p>log it(p) = β<del>0</del> + β<del>1</del>X</p><ul><li>회귀계수 β<del>1</del>의 의미가 선형회귀모형에서와는 다름</li><li>β<del>1</del>: X가 한 단위 증가할 때, logit(P)(승산비) 로그값의 증가분을 이야기함 → 승산비가 배로 증가함을 의미</li></ul></li><li><p>Y가 (0/1, cancer/no cancer, present/absent) 등의 값을 취하는 경우, 독립변수들과 Y 관계를 설명하기 위해 로지스틱 함수 사용</p><ul><li>f(y) = $\frac{1}{1 + e^-y^}$</li></ul></li></ul></li></ul><h3 id="로지스틱-회귀모형-실습"><a href="#로지스틱-회귀모형-실습" class="headerlink" title="로지스틱 회귀모형 실습"></a>로지스틱 회귀모형 실습</h3><ul><li><p>데이터 살펴보기</p><ul><li>암 재발 확률 구하기</li><li>Y: Remiss(0,1)</li><li>6 explanatory variables: risk factor related cancer remission(Cell, Smear, infill, Li, blast, temp)</li></ul></li><li><p>로짓 변환을 통해 p헷(확률값)을 계산한 후, 확률값을 보고 0,5  또는 최적 임계치를 기준으로 구분하는 것</p></li><li><p>데이터 불러오기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># remiss data</span><br><span class="line">re &lt;- read.csv(&quot;data&#x2F;week14_3&#x2F;remiss.csv&quot;)</span><br><span class="line">head(re)</span><br><span class="line">str(re)</span><br><span class="line">attach(re)</span><br></pre></td></tr></table></figure></li><li><p>로지스틱회귀모형 실시(y는 binomial variable, logit function 선택)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#logistic regression (full model)</span><br><span class="line">t1&lt;-glm(remiss~cell+smear+infil+li+blast+temp, data&#x3D;re,family&#x3D;binomial(logit))</span><br><span class="line">summary(t1)</span><br><span class="line"></span><br><span class="line">cor(re)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>smear, infil은 거의 1에 가까운 값을 보임 → 둘 중 하나 제거하는 것이 좋음</li><li>blast의 p-value가 1에 가까움: blast 변수를 추가적으로 설명할 수 있는 부분이 거의 없다 → 제거하는 것이 좋음</li></ul></li><li><p>smear, blast 삭제</p></li><li><p>로지스틱 회귀모형의 평가 척도: -2Log(Deviance), AIC, likelihood ratio test(G^2)</p><ul><li>AIC가 낮은 것이 좋은 모델이라고 평가<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># logisitic regression (reduced model 1)</span><br><span class="line">t2&lt;-glm(remiss~cell+smear+li+temp, data&#x3D;re,family&#x3D;binomial(logit))</span><br><span class="line">summary(t2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># logisitic regression (reduced model 2)</span><br><span class="line">t3&lt;-glm(remiss~cell+li+temp, data&#x3D;re,family&#x3D;binomial(logit))</span><br><span class="line">summary(t3)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>결과 해석</p><ul><li>logit(P) = 67.63 + 9.65Cell + 3.87Li - 82.07Temp<ul><li>e.g. Li의 beta1 해석: li 한 단위 증가 시 재발 확률은 exp(3.867) = 47.79배  </li></ul></li></ul></li></ul><ul><li>예측확률값 출력: 원래 데이터 + 예측확률값<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># output data with predicted probability</span><br><span class="line">dat1_pred&lt;-cbind(re,t3$fitted.values)</span><br><span class="line">write.table(dat1_pred,file&#x3D;&quot;dat1_pred.csv&quot;, row.names&#x3D;FALSE, sep&#x3D;&quot;,&quot;, na&#x3D;&quot; &quot;)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;ⅩⅣ-연관규칙과-로지스틱회귀분석&quot;&gt;&lt;a</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="연관규칙" scheme="https://ne-choi.github.io/tags/%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99/"/>
    
    <category term="로지스틱회귀분석" scheme="https://ne-choi.github.io/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 2: ⅩⅢ. 군집분석</title>
    <link href="https://ne-choi.github.io/2020/12/21/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A2_%EA%B5%B0%EC%A7%91%EB%B6%84%EC%84%9D/"/>
    <id>https://ne-choi.github.io/2020/12/21/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A2_%EA%B5%B0%EC%A7%91%EB%B6%84%EC%84%9D/</id>
    <published>2020-12-21T00:00:00.000Z</published>
    <updated>2021-02-01T00:50:02.752Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="ⅩⅢ-군집분석"><a href="#ⅩⅢ-군집분석" class="headerlink" title="ⅩⅢ. 군집분석"></a>ⅩⅢ. 군집분석</h1><h2 id="1-군집분석과-유사성-척도"><a href="#1-군집분석과-유사성-척도" class="headerlink" title="1. 군집분석과 유사성 척도"></a>1. 군집분석과 유사성 척도</h2><h3 id="군집분석"><a href="#군집분석" class="headerlink" title="군집분석"></a>군집분석</h3><ul><li><p>군집분석은 비지도학습(Unsupervised Learning)</p><ul><li>주어진 데이터(X 변수들)의 속성으로 군집화<ul><li>계층형 군집 분석</li><li>k-means</li></ul></li></ul></li><li><p>유사한 속성을 가진 객체를 군집(cluster)으로 묶는 데이터 마이닝 기법</p><ul><li>예제: 고객 구매패턴을 반영하는 속성 데이터가 수집된다고 할 때, 군집분석을 통해 유사한 구매패턴을 보이는 고객을 군집화하고 판매전략을 도출(소득/충성정도 등)</li></ul></li></ul><h3 id="군집분석-종류"><a href="#군집분석-종류" class="headerlink" title="군집분석 종류"></a>군집분석 종류</h3><ol><li><p>계층적 군집(Hierarchical Clustering)</p><ul><li>사전에 군집 수 k를 정하지 않고 단계적으로 군집 트를 제공</li></ul></li><li><p>비계층적 군집(Non-hierarchical Clustering)</p><ul><li>사전에 군집 수 k를 정한 후 각 객체를 k개 중 하나의 군집에 배정</li></ul></li></ol><h3 id="유사성-척도"><a href="#유사성-척도" class="headerlink" title="유사성 척도"></a>유사성 척도</h3><ul><li>객체 간의 유사성 정도를 정량적으로 나타내기 위해 척도가 필요<ul><li>거리(distance) 척도<ul><li>거리가 가까울수록 유사성이 커짐</li></ul></li><li>상관계수척도<ul><li>객체 간 상관계수가 클수록 유사성이 커짐  </li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># similarity measures - distance</span><br><span class="line">m1 &lt;- matrix(</span><br><span class="line">  c(150, 50, 130, 55, 80, 80, 100, 85, 95, 91),</span><br><span class="line">  nrow &#x3D; 5,</span><br><span class="line">  ncol &#x3D; 2,</span><br><span class="line">  byrow &#x3D; TRUE)</span><br><span class="line"># m1 is a matrix</span><br><span class="line">m1</span><br><span class="line">is.data.frame(m1)</span><br><span class="line"># m1 is defined as dataframe</span><br><span class="line">m1&lt;-as.data.frame(m1)</span><br></pre></td></tr></table></figure><ul><li><p><strong>거리척도</strong></p><ul><li><p>객체 i의 p차원 공간에서의 좌표는 열벡터로 표현</p><ul><li>p개의 속상을 가진 객체 i에 관해, j번째 속성은 X<del>ji</del>로 표현  </li></ul></li><li><p>거리 계산 함수: dist(데이터, method= ), default는 “euclidean”  </p></li></ul><ol><li>유클리디안 거리(Euclidean distance)<ul><li>Distance = $\sqrt{(x<del>12</del> - x<del>11</del>)^2^ + (x<del>21</del> - x<del>22</del>)^2^}$<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 1. Euclidean distance</span><br><span class="line">D1 &lt;- dist(m1)</span><br><span class="line">D1</span><br></pre></td></tr></table></figure></li></ul></li><li>민코프스키 거리(Minkowski distance<ul><li>유클리디안 거리의 일반화된 방법(m = 2일 때는 유클리디안 거리와 동일)</li><li>d(x<del>i</del>,x<del>j</del>) = (시그마|X<del>ki</del> - X<del>kj</del>|^m^)^1/m^<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 2. Minkowski distance</span><br><span class="line">D2&lt;- dist(m1, method&#x3D;&quot;minkowski&quot;, p&#x3D;3) </span><br><span class="line">D2</span><br></pre></td></tr></table></figure></li></ul></li><li>마할라노비스 거리(Mahalanobis distance)<ul><li>변수 간의 상관관계가 존재할 때 사용</li><li>d(x<del>i</del>,x<del>j</del>) = $\sqrt{(x<del>i</del> - x<del>j</del>)^T^S^-1^(x<del>i</del> - x<del>j</del>)}$</li></ul></li></ol></li></ul><h3 id="상관계수를-유사성-척도로-사용"><a href="#상관계수를-유사성-척도로-사용" class="headerlink" title="상관계수를 유사성 척도로 사용"></a>상관계수를 유사성 척도로 사용</h3><ul><li>상관관계가 클수록 두 객체의 유사성이 크다고 추정<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 3. correlation coefficient</span><br><span class="line">m2 &lt;- matrix(</span><br><span class="line">  c(20, 6, 14, 30, 7, 15, 46, 4, 2),</span><br><span class="line">  nrow &#x3D; 3,</span><br><span class="line">  ncol &#x3D; 3,</span><br><span class="line">  byrow &#x3D; TRUE,</span><br><span class="line">  dimnames &#x3D; list(</span><br><span class="line">     c(&quot;obs1&quot;, &quot;obs2&quot;, &quot;obs3&quot;),</span><br><span class="line">     c(&quot;age&quot;, &quot;exp&quot;, &quot;time&quot;)))</span><br><span class="line">m2</span><br></pre></td></tr></table></figure></li><li>상관계쑤 측정<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># correlation between Obs1~Obs2</span><br><span class="line">cor(m2[1,],m2[2,]) </span><br><span class="line"># correlation between Obs1~Obs3</span><br><span class="line">cor(m2[1,],m2[3,])</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>객체 1(obs1)과 객체 2의 유사성이, 객체 1과 객체 3 간 유사성보다 큼(0.9674 &gt; 0.7984)</li></ul></li></ul><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 2. Minkowski distance</span><br><span class="line">D4&lt;- dist(m1, method&#x3D;&quot;manhattan&quot;) </span><br><span class="line">D4</span><br></pre></td></tr></table></figure><h2 id="2-계층적-군집분석"><a href="#2-계층적-군집분석" class="headerlink" title="2. 계층적 군집분석"></a>2. 계층적 군집분석</h2><h3 id="계측적-군집분석"><a href="#계측적-군집분석" class="headerlink" title="계측적 군집분석"></a>계측적 군집분석</h3><ul><li>사전에 군집 수 k를 정하지 않고 단계적으로 군집 형성<ul><li>유사한 객체를 군집으로 묶고, 그 군집을 기반으로 유사한 군집을 새로운 군집으로 묶어가면서 군집을 계층적으로 구성</li><li>단일연결법(single linkage method)</li><li>완전연결법(complete linkage method)</li><li>평균연결법(average linkage method)</li><li>중심연결법(centroid linkage method)  </li></ul></li></ul><p><strong>1. 단일연결법</strong></p><ul><li>군집 i와 군집 j의 유사성 척도로 두 군집의 모든 객체 쌍의 거리 중 <strong>가장 가까운 거리</strong>를 사용<ul><li>객체 쌍이 가장 짧은 거리가 유사할수록 두 군집이 더 유사하다고 평가</li></ul></li></ul><p><strong>2. 완전연결법</strong></p><ul><li>두 군집의 모든 객체 쌍의 거리 중 <strong>가장 먼 거리</strong>를 사용</li></ul><p><strong>3. 평균연결법</strong></p><ul><li>두 군집의 모든 객체 쌍의 <strong>평균 거리</strong>를 사용</li></ul><p><strong>4. 중심연결법</strong></p><ul><li>두 군집의 <strong>중심 좌표</strong> (무게 중심, 객체가 아닌 위치)</li></ul><h3 id="완전연결법-vs-평균연결법"><a href="#완전연결법-vs-평균연결법" class="headerlink" title="완전연결법 vs 평균연결법"></a>완전연결법 vs 평균연결법</h3><ul><li>데이터 설명<ul><li>1833년 영국 Lancashire 방직 공장 임금</li><li>DAAG package built in 데이터</li><li>총 51개 객체</li><li>객체별 5개 속성<ul><li>나이(age), 남성 근로자 수(mnum), 남성 근로자 평균 임금(mwage), 여성 근로자 수(fnum), 여성 근로자 평균 임금(fwage)</li></ul></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># needs &quot;lattice&quot;, &quot;DAAG&quot; package for loading dataset</span><br><span class="line"># install.packages(&quot;lattice&quot;)</span><br><span class="line"># install.packages(&quot;DAAG&quot;)</span><br><span class="line">library(lattice)</span><br><span class="line">library(DAAG)</span><br></pre></td></tr></table></figure><ul><li>데이터 불러오기(DAAG 패키지 안에 든 데이터)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">wages1833&lt;-read.csv(file&#x3D;&quot;data&#x2F;week13_2&#x2F;wages1833.csv&quot;)</span><br><span class="line">head(wages1833)</span><br><span class="line"></span><br><span class="line"># remove observations with the missing values</span><br><span class="line">dat1&lt;-wages1833</span><br><span class="line">dat1&lt;-na.omit(dat1)</span><br><span class="line">str(dat1)</span><br></pre></td></tr></table></figure></li><li>계층적 군집분석: hclust(거리계산결과, method=” “)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># calculate distance between each nodes</span><br><span class="line">dist_data&lt;-dist(dat1)</span><br></pre></td></tr></table></figure></li></ul><ol><li>완전연결법 적용 결과(거리 계산은 유클리디안)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># prepare hierarchical cluster</span><br><span class="line"># complete linkage method</span><br><span class="line">hc_a &lt;- hclust(dist_data, method &#x3D; &quot;complete&quot;)</span><br><span class="line">plot(hc_a, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;complete&quot;)</span><br></pre></td></tr></table></figure></li></ol><ul><li>설명<ul><li>single: 단일, complete: 완전, average: 평균, centriod: 중심  </li></ul></li></ul><ol start="2"><li>평균연결법 적용 결과(거리 계산은 유클리디안)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># average linkage method</span><br><span class="line"># check how different from complete method</span><br><span class="line">hc_c &lt;- hclust(dist_data, method &#x3D; &quot;average&quot;)</span><br><span class="line">plot(hc_c, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;average&quot;)</span><br></pre></td></tr></table></figure></li></ol><h3 id="와드-연결방법"><a href="#와드-연결방법" class="headerlink" title="와드 연결방법"></a>와드 연결방법</h3><ul><li>Ward’s method</li><li>많이 사용됨  </li></ul><ol start="3"><li>와드방법을 적용한 결과(거리계산은 유클리디안)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Ward&#39;s method</span><br><span class="line">hc_c &lt;- hclust(dist_data, method &#x3D; &quot;ward.D2&quot;)</span><br><span class="line">plot(hc_c, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;Ward&#39;s method&quot;)</span><br></pre></td></tr></table></figure></li></ol><h2 id="3-비계층적-군집분석"><a href="#3-비계층적-군집분석" class="headerlink" title="3. 비계층적 군집분석"></a>3. 비계층적 군집분석</h2><h3 id="비계층적-군집분석"><a href="#비계층적-군집분석" class="headerlink" title="비계층적 군집분석"></a>비계층적 군집분석</h3><ul><li>사전에 군집 수 k를 정한 후, 각 객체를 k개 중 하나의 군집에 배정<ul><li>비계층적 군집(Non-hierarchical Clustering)<ul><li>K-means 알고리즘</li><li>K-medoids 알고리즘<ul><li>PAM(Partitioning Around Medoids)</li><li>CLARA(Clustering LARge Applications)</li></ul></li></ul></li></ul></li></ul><h3 id="k-means-군집분석"><a href="#k-means-군집분석" class="headerlink" title="k-means 군집분석"></a>k-means 군집분석</h3><ul><li>비계층적 군집분석 중 가장 널리 사용<ul><li>k개 군집의 중심좌표를 고려하여 각 객체를 가장 가까운 군집에 배정하는 것을 반복<ul><li>0단계. 초기 객체 선정: k개 객체 좌표를 초기 군집 중심좌표로 선정  </li><li>1단계. 객체 군집 배정: 각 객체와 k개 중심좌표와의 거리 산출 후, 가장 가까운 군집에 객체 배정  </li><li>2단계. 군집 중심좌표 산출: 새로운 군집의 중심좌표 산출  </li><li>3단계. 수렴 조건 점검: 새로 산출된 중심 좌표값과 이전 좌표값을 비교하여 수렴 조건 내에 들면 종료, 아니면 단계 1 반복</li></ul></li></ul></li></ul><h3 id="k-means-군집분석-예제"><a href="#k-means-군집분석-예제" class="headerlink" title="k-means 군집분석 예제"></a>k-means 군집분석 예제</h3><ul><li><p>데이터 불러오기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">wages1833&lt;-read.csv(file&#x3D;&quot;data&#x2F;week13_3&#x2F;wages1833.csv&quot;)</span><br><span class="line">head(wages1833)</span><br><span class="line"></span><br><span class="line"># preprocessing</span><br><span class="line">dat1&lt;-wages1833</span><br><span class="line">dat1&lt;-na.omit(dat1)</span><br><span class="line">head(dat1, n&#x3D;5)</span><br></pre></td></tr></table></figure></li><li><p>군집수 k결정</p><ul><li>최적 군집수에 관한 시각화</li><li>최적값은 “silhouette(실루엣)”, “gap_stat”, “wss(그룹내합계제곱)”으로 산출</li><li>그래프가 완만해지는 지점을 k의 값으로 추정<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># to choose the optimal k</span><br><span class="line"># install.packages(&quot;factoextra&quot;)</span><br><span class="line">library(factoextra)</span><br><span class="line">fviz_nbclust(dat1, kmeans, method &#x3D; &quot;wss&quot;)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>결과 해석</p><ul><li>최적 k = 3  </li></ul></li><li><p>k-means (k=3)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># compute kmeans</span><br><span class="line">set.seed(123,sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">km &lt;- kmeans(dat1, 3, nstart &#x3D; 25) # random set의 수 (nstart)</span><br><span class="line">km</span><br><span class="line"></span><br><span class="line"># visualize</span><br><span class="line">fviz_cluster(km, data &#x3D; dat1, </span><br><span class="line">             ellipse.type&#x3D;&quot;convex&quot;, # Convex 모양으로 구역 표시</span><br><span class="line">             repel &#x3D; TRUE) # repel을 통해 관측치 표기</span><br></pre></td></tr></table></figure><h3 id="K-medoids-군집분석"><a href="#K-medoids-군집분석" class="headerlink" title="K-medoids 군집분석"></a>K-medoids 군집분석</h3></li><li><p>K-medoids 군집분석은 각 군집의 대표 객체(medoid)를 고려</p><ul><li>군집의 대표 객체란, 군집 내 다른 객체들과의 거리가 최소가 되는 객체</li><li>K-medoids 군집분석은 객체를 k개의 군집으로 구분하는데, 객체와 속하는 군집의 대표 객체와의 거리 총합을 최소로 하는 방법<ul><li><strong>PAM(Partitioning Around Medoids) 알고리즘</strong>: 모든 객체에 관해 대표 객체가 변했을 때 발생하는 거리 총합의 변화를 계산, 데이터 수가 많아질수록 연산량이 크게 증가</li><li><strong>CLARA 알고리즘</strong>: 적절한 수의 객체를 샘플링한 후, PAM 알고리즘을 적용해 대표 객체 선정, 샘플링을 여러 번 한 후 가장 좋은 결과를 선택, 편향된 샘플링은 잘못된 결괏값을 도출할 수 있음  </li></ul></li></ul></li><li><p>PAM 알고리즘 살펴보기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># compute PAM</span><br><span class="line">library(&quot;cluster&quot;)</span><br><span class="line">pam_out &lt;- pam(dat1, 3)</span><br><span class="line">pam_out</span><br><span class="line"></span><br><span class="line"># freq of each cluster</span><br><span class="line">table(pam_out$clustering)</span><br><span class="line"></span><br><span class="line"># visualize</span><br><span class="line">fviz_cluster(pam_out, data &#x3D; dat1,</span><br><span class="line">             ellipse.type&#x3D;&quot;convex&quot;, </span><br><span class="line">             repel &#x3D; TRUE)</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;ⅩⅢ-군집분석&quot;&gt;&lt;a href=&quot;#ⅩⅢ</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="k-인접기법" scheme="https://ne-choi.github.io/tags/k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95/"/>
    
    <category term="판별분석" scheme="https://ne-choi.github.io/tags/%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 1: Ⅹ. k-인접기법과 판별분석</title>
    <link href="https://ne-choi.github.io/2020/12/14/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9_k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95%EA%B3%BC_%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"/>
    <id>https://ne-choi.github.io/2020/12/14/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9_k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95%EA%B3%BC_%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/</id>
    <published>2020-12-14T00:00:00.000Z</published>
    <updated>2021-02-01T00:49:04.678Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  </li></ul><h1 id="Ⅹ-k-인접기법과-판별분석"><a href="#Ⅹ-k-인접기법과-판별분석" class="headerlink" title="Ⅹ. k-인접기법과 판별분석"></a>Ⅹ. k-인접기법과 판별분석</h1><h2 id="1-k-인접기법"><a href="#1-k-인접기법" class="headerlink" title="1. k-인접기법"></a>1. k-인접기법</h2><ul><li>k-nearest neighbor</li></ul><h3 id="분류-Classification"><a href="#분류-Classification" class="headerlink" title="분류(Classification)"></a>분류(Classification)</h3><ul><li><p>분류(Classification): 지도학습(Supervised Learning)</p><ul><li>타겟범주를 알고 있는 데이터로 분류 규칙을 생성하고 새로운 데이터를 특정 범주에 분류하는 기법</li></ul></li><li><p>군집화(Clustering): 비지도학습(Unsupervised Learning)</p><ul><li>독립변수들의 속성을 기반으로 객체들을 그룹화하는 방법</li></ul></li></ul><h3 id="k-인접기법"><a href="#k-인접기법" class="headerlink" title="k-인접기법"></a>k-인접기법</h3><ul><li><p>k-인접방법 (kNN): k개의 가장 가까운 이웃을 사용해서 분류하는 방법</p><ul><li>거리만 고려하거나, 거리에 따라 가중치를 부여하는 2가지 방법</li><li>사용되는 변수에 결측치가 있는 경우, 미리 처리하고 수행해야 함</li><li>k개의 인접한 관측치의 다수 범주로 할당하는 방법</li></ul></li><li><p>최적 k는?</p><ul><li>k가 너무 크면 데이터 구조를 파악하기 어렵고, 너무 작으면 과적합(overfitting) 위험이 있음</li><li>교차검증(Cross-validation)으로 정확도가 높은 k를 선정</li></ul></li><li><p>장점</p><ul><li>단순하며 효율적</li><li>데이터 분산을 추정할 필요가 없음</li><li>빠른 훈련 단계</li></ul></li><li><p>단점</p><ul><li>모델을 생성하지 않음</li><li>느린 분류 단계</li><li>많은 메모리 필요</li><li>결측치는 추가 작업 필요</li></ul></li><li><p>kNN 수행을 위한 패키지 설치</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># packages</span><br><span class="line"># install.packages(&quot;class&quot;) #no weighted value knn</span><br><span class="line"># install.packages(&quot;gmodels&quot;) #검증에 사용되는 cross table을 위한 패키지</span><br><span class="line"># install.packages(&quot;scales&quot;) #for graph</span><br><span class="line">library(class)</span><br><span class="line">library(gmodels)</span><br><span class="line">library(scales)</span><br></pre></td></tr></table></figure></li><li><p>train/test 데이터 분할(cross-validation)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">iris &lt;- read.csv(&quot;data&#x2F;week10_1&#x2F;iris.csv&quot;)</span><br><span class="line"># head(iris)</span><br><span class="line"># str(iris)</span><br><span class="line">attach(iris)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># training&#x2F; test data : n&#x3D;150</span><br><span class="line">set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&#x3D;nrow(iris)</span><br><span class="line">tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line"># attributes in training and test</span><br><span class="line">iris.train&lt;-iris[tr.idx,-5]</span><br><span class="line">iris.test&lt;-iris[-tr.idx,-5]</span><br><span class="line"># target value in training and test</span><br><span class="line">trainLabels&lt;-iris[tr.idx,5]</span><br><span class="line">testLabels&lt;-iris[-tr.idx,5]</span><br><span class="line"></span><br><span class="line">train&lt;-iris[tr.idx,]</span><br><span class="line">test&lt;-iris[-tr.idx,]</span><br></pre></td></tr></table></figure></li><li><p>kNN 수행과 결과</p><ul><li>kNN 함수: knn(train=학습데이터, test=검증데이터, cl=타겟변수, k= )<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># knn (5-nearest neighbor)</span><br><span class="line">md1 &lt;- knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;5)</span><br><span class="line">md1</span><br></pre></td></tr></table></figure></li></ul></li><li><p>kNN(k=5)의 결과: 정확도</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># accuracy of 5-nearest neighbor classification</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;md1, prop.chisq&#x3D;FALSE)</span><br><span class="line"># x: 타겟변수의 실제값, y: 타겟변수의 예측값</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>정확도: 47/50 → 94%</li><li>versicolor를 virginica로 오분류(2개)</li><li>virginica를 versicolor로 오분류(1개)</li><li>오분류율: 3/50 → 6%  </li></ul></li></ul><h2 id="2-k-인접기법-가중치"><a href="#2-k-인접기법-가중치" class="headerlink" title="2. k-인접기법: 가중치"></a>2. k-인접기법: 가중치</h2><h3 id="kNN에서-최적-k-탐색"><a href="#kNN에서-최적-k-탐색" class="headerlink" title="kNN에서 최적 k 탐색"></a>kNN에서 최적 k 탐색</h3><ul><li>최적 k의 탐색: 1 to nrow(tran_data)/2 (여기서는 1 to 50)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># optimal k selection (1 to n&#x2F;2)</span><br><span class="line">accuracy_k &lt;- NULL</span><br><span class="line"># try k&#x3D;1 to nrow(train)&#x2F;2, may use nrow(train)&#x2F;3(or 4,5) depending the size of n in train data</span><br><span class="line">nnum&lt;-nrow(iris.train)&#x2F;2</span><br><span class="line">for(kk in c(1:nnum))</span><br><span class="line">&#123;</span><br><span class="line">  set.seed(1234, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">  knn_k&lt;-knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;kk)</span><br><span class="line">  accuracy_k&lt;-c(accuracy_k,sum(knn_k&#x3D;&#x3D;testLabels)&#x2F;length(testLabels))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># plot for k&#x3D;(1 to n&#x2F;2) and accuracy</span><br><span class="line">test_k&lt;-data.frame(k&#x3D;c(1:nnum), accuracy&#x3D;accuracy_k[c(1:nnum)])</span><br><span class="line">plot(formula&#x3D;accuracy~k, data&#x3D;test_k,type&#x3D;&quot;o&quot;,ylim&#x3D;c(0.5,1), pch&#x3D;20, col&#x3D;3, main&#x3D;&quot;validation-optimal k&quot;)</span><br><span class="line">with(test_k,text(accuracy~k,labels &#x3D; k,pos&#x3D;1,cex&#x3D;0.7))</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># minimum k for the highest accuracy</span><br><span class="line">min(test_k[test_k$accuracy %in% max(accuracy_k),&quot;k&quot;])</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>k=7에서 정확도(.98)가 가장 높음</li></ul></li></ul><ul><li>최종 kNN 모형 (k=7)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#k&#x3D;7 knn</span><br><span class="line">md1&lt;-knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;7)</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;md1, prop.chisq&#x3D;FALSE)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>정확도: 49/50 → 98%</li><li>versicolor를 virginica로 오분류 (1개)</li><li>오분류율: 1/50 → 2%</li></ul></li></ul><ul><li>kNN(k=7)의 결과: 그래픽<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># graphic display</span><br><span class="line">plot(formula&#x3D;Petal.Length ~ Petal.Width,</span><br><span class="line">     data&#x3D;iris.train,col&#x3D;alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7)[trainLabels],</span><br><span class="line">     main&#x3D;&quot;knn(k&#x3D;7)&quot;)</span><br><span class="line">points(formula &#x3D; Petal.Length~Petal.Width,</span><br><span class="line">       data&#x3D;iris.test,</span><br><span class="line">       pch &#x3D; 17,</span><br><span class="line">       cex&#x3D; 1.2,</span><br><span class="line">       col&#x3D;alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7)[md1]</span><br><span class="line">)</span><br><span class="line">legend(&quot;bottomright&quot;,</span><br><span class="line">       c(paste(&quot;train&quot;,levels(trainLabels)),paste(&quot;test&quot;,levels(testLabels))),</span><br><span class="line">       pch&#x3D;c(rep(1,3),rep(17,3)),</span><br><span class="line">       col&#x3D;c(rep(alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7),2)),</span><br><span class="line">       cex&#x3D;0.9</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>Petal.width와 Petal.length에 산점도를 그리면 setosa는 잘 분류됨</li><li>virginica와 versicolor는 분류가 잘 되지 않음</li></ul></li></ul><h3 id="Weighted-kNN"><a href="#Weighted-kNN" class="headerlink" title="Weighted kNN"></a>Weighted kNN</h3><ul><li><p>거리에 따라 가중치를 부여하는 두 가지 알고리즘이 존재</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Weighted KNN packages</span><br><span class="line"># install.packages(&quot;kknn&quot;) # weighted value knn</span><br><span class="line">library(kknn)</span><br></pre></td></tr></table></figure></li><li><p>k=5, distance=1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># weighted knn</span><br><span class="line">md2 &lt;- kknn(Species~., train&#x3D;train, test&#x3D;iris.test, k&#x3D;5, distance&#x3D;1, kernel&#x3D;&quot;triangular&quot;)</span><br><span class="line">md2</span><br><span class="line"></span><br><span class="line"># to see results for weighted knn</span><br><span class="line">md2_fit&lt;-fitted(md2)</span><br><span class="line">md2_fit</span><br></pre></td></tr></table></figure></li><li><p>cross table로 오분류율 보기</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># accuracy of weighted knn</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;md2_fit,prop.chisq&#x3D;FALSE,prop.c&#x3D;FALSE)</span><br></pre></td></tr></table></figure></li><li><p>k=7, distance=2로 옵션 변경 결과</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># weighted knn (k&#x3D;7, distance&#x3D;2)</span><br><span class="line">md3&lt;-kknn(Species~., train&#x3D;train,test&#x3D;iris.test,k&#x3D;7,distance&#x3D;2,kernel&#x3D;&quot;triangular&quot;)</span><br><span class="line">md3</span><br><span class="line"># to see results for weighted knn</span><br><span class="line">md3_fit&lt;-fitted(md3)</span><br><span class="line">md3_fit</span><br><span class="line"># accuracy of weighted knn</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;md3_fit,prop.chisq&#x3D;FALSE,prop.c&#x3D;FALSE)</span><br></pre></td></tr></table></figure><h2 id="3-판별분석-Ⅰ"><a href="#3-판별분석-Ⅰ" class="headerlink" title="3. 판별분석 Ⅰ"></a>3. 판별분석 Ⅰ</h2><h3 id="판별분석"><a href="#판별분석" class="headerlink" title="판별분석"></a>판별분석</h3></li><li><p>데이터 마이닝 분류 기법 중 하나</p></li><li><p>객체를 몇 개의 <strong>범주로 분류</strong></p></li><li><p>범주를 가장 잘 구분하는 변수 파악 및 범주간 차이를 가장 잘 표현하는 함수 도출</p><ul><li>피셔(Fisher) 방법</li><li>의사결정이론<ul><li>선형판별분석(LDA; Liner DA): 정규분포 분산-공분산 행렬이 범주에 관계 없이 동일한 경우</li><li>이차판별분석(QDA; Quadratic DA): 정규분포의 분산-공분산 행렬이 범주별로 다른 경우</li></ul></li></ul></li><li><p>예제 데이터</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;gmodels&quot;) #crosstable</span><br><span class="line">library(gmodels)</span><br></pre></td></tr></table></figure></li><li><p>train / test 분할</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week10_3&#x2F;iris.csv&quot;)</span><br><span class="line">attach(iris)</span><br><span class="line"></span><br><span class="line"># training&#x2F; test data : n&#x3D;150</span><br><span class="line"></span><br><span class="line">set.seed(1000,sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line"></span><br><span class="line">N&#x3D;nrow(iris)</span><br><span class="line">tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line"># attributes in training and test</span><br><span class="line">iris.train&lt;-iris[tr.idx,-5] # 독립변수 4개 포함한 100개 데이터</span><br><span class="line">iris.test&lt;-iris[-tr.idx,-5] # 독립변수 4개 포함한 50개 데이터</span><br><span class="line"># target value in training and test</span><br><span class="line">trainLabels&lt;-iris[tr.idx,5] # 학습데이터의 타겟변수</span><br><span class="line">testLabels&lt;-iris[-tr.idx,5] # 검증데이터의 타겟변수</span><br><span class="line"></span><br><span class="line">train&lt;-iris[tr.idx,]</span><br><span class="line">test&lt;-iris[-tr.idx,]</span><br></pre></td></tr></table></figure></li><li><p>LDA함수: lda(종속변수~독립변수, data=학습데이터이름, prior=사전확률)</p><ul><li>사전확률(prioir probability): 원인 A가 발생할 확률인 P(A)와 같이, 결과가 나타나기 전에 결정되어 있는 확률<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install the MASS package for LDA</span><br><span class="line"># install.packages(&quot;MASS&quot;)</span><br><span class="line">library(MASS)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Linear Discriminant Analysis (LDA) with training data n&#x3D;100</span><br><span class="line">iris.lda &lt;- lda(Species ~ ., data&#x3D;train, prior&#x3D;c(1&#x2F;3,1&#x2F;3,1&#x2F;3)) # .의 의미: 전체 데이터를 사용하겠다(4개)</span><br><span class="line">iris.lda</span><br></pre></td></tr></table></figure></li></ul></li><li><p>결과 해석</p><ul><li><p>Coefficients ~</p></li><li><p>첫 번째 범주 판별 함수: LD1 = 0.89 Sepal.Length + 1.71 Sepal.Width - 2.15 Petal.Length - 2.91 Petal.Width</p></li><li><p>두 번째 범주 판별 함수: LD2 = -0.11 Sepal.Length - 2.23 Sepal.Width + 0.74 Petal.Length - 2.39 Petal.Width</p></li><li><p>LD1이 between-group variance의 99%를 설명</p></li><li><p>LD2가 between-group variance의 1%를 설명</p></li></ul></li><li><p>검증 데이터에 LDA 결과를 적용하여 범주 추정</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># predict test data set n&#x3D;50</span><br><span class="line">testpred &lt;- predict(iris.lda, test)</span><br><span class="line">testpred</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>$class: 추정 범주</li><li>세 개 범주의 사후 확률(posterior probability)을 구한 후, max 값의 범주로 할당</li></ul></li><li><p>정확도 산정: 오분류율(검증데이터)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># accuracy of LDA</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;testpred$class, prop.chisq&#x3D;FALSE)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>정확도: 49/50 → 98%</li><li>오분류율: 1/50 → 2%</li></ul></li></ul><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><p>iris.lda &lt;- lda(Species ~ ., data=train, prior=c(1/2,1/4,1/4)) # .의 의미: 전체 데이터를 사용하겠다(4개)<br>iris.lda</p><p>testpred &lt;- predict(iris.lda, test)<br>testpred</p><p>CrossTable(x=testLabels,y=testpred$class, prop.chisq=FALSE)</p><p>testpred1 &lt;- round(testpred$posterior, 2) id=150</p><h2 id="4-판별분석-Ⅱ"><a href="#4-판별분석-Ⅱ" class="headerlink" title="4. 판별분석 Ⅱ"></a>4. 판별분석 Ⅱ</h2><h3 id="선형판별분석-vs-이차판별분석"><a href="#선형판별분석-vs-이차판별분석" class="headerlink" title="선형판별분석 vs 이차판별분석"></a>선형판별분석 vs 이차판별분석</h3><table><thead><tr><th>LDA</th><th>QDA</th></tr></thead><tbody><tr><td>분산-공분산 행렬이 범주 관계 없이 동일한 경우</td><td>분산-공분산 행렬이 범주별로 다른 경우</td></tr><tr><td>(+) 적은 파라미터 사용, 낮은 분산</td><td>(-) 많은 파라미터 사용, 높은 분산</td></tr><tr><td>(-) 낮은 유연성</td><td>(=) 높은 유연성</td></tr></tbody></table><h3 id="이차판별분석-QDA"><a href="#이차판별분석-QDA" class="headerlink" title="이차판별분석(QDA)"></a>이차판별분석(QDA)</h3><ul><li><p>모집단 등분산 검정</p><ul><li>분산-공분산 행렬이 범주별로 다른 경우, 이차판별분석 실시  </li></ul></li><li><p>Box’s M-test</p><ul><li>귀무가설: 모집단의 분산-공분산 행렬이 동일</li><li>대립가설: 모집단의 분산-공분산 행렬이 동일하지 X  </li></ul></li><li><p>등분산 검정을 위한 패키지: biotools  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;biotools&quot;)</span><br><span class="line">library(biotools)</span><br><span class="line">boxM(iris[1:4], iris$Species)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>p-value~0: p-value는 거의 0에 가까움 </li><li>귀무가설(등분산 가정)이 기각 → QDA 실시</li></ul></li></ul><h3 id="QDA-함수"><a href="#QDA-함수" class="headerlink" title="QDA 함수"></a>QDA 함수</h3><ul><li><p>qda(종속변수~독립변수, data=학습데이터이름, prior=사전확률)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Quadratic Discriminant Analysis (QDA)</span><br><span class="line">iris.qda &lt;- qda(Species ~ ., data&#x3D;train, prior&#x3D;c(1&#x2F;3,1&#x2F;3,1&#x2F;3))</span><br><span class="line">iris.qda</span><br></pre></td></tr></table></figure></li><li><p>추가 설명</p><ul><li>prior은 경우에 따라 다르게 줄 수 있음</li><li>독립변수에 대한 그룹별 평균값  </li></ul></li><li><p>검증 데이터에 QDA 결과를 적용하여 범주 추정  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># predict test data set n&#x3D;50</span><br><span class="line">testpredq &lt;- predict(iris.qda, test)</span><br><span class="line">testpredq</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>$class: 추정 범주</li><li>세 개 범주의 사후 확률(posterior probability)을 구한 후, max 값의 범주로 할당  </li></ul></li></ul><ul><li>정확도 산정: 오분류율(검증데이터)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># accuracy of QDA</span><br><span class="line">CrossTable(x&#x3D;testLabels,y&#x3D;testpredq$class, prop.chisq&#x3D;FALSE)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>선형판별로 했을 때와 동일(단, 데이터가 많아지면 결과가 다르게 나올 수 있음)</li></ul></li></ul><ul><li>Partition Plot<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># partimat() function for LDA &amp; QDA</span><br><span class="line"># install.packages(&quot;klaR&quot;)</span><br><span class="line">library(klaR)</span><br><span class="line">partimat(as.factor(iris$Species) ~ ., data&#x3D;iris, method&#x3D;&quot;lda&quot;)</span><br><span class="line">partimat(as.factor(iris$Species) ~ ., data&#x3D;iris, method&#x3D;&quot;qda&quot;)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅹ-k-인접기법과-판별분석&quot;&gt;&lt;a hr</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="k-인접기법" scheme="https://ne-choi.github.io/tags/k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95/"/>
    
    <category term="판별분석" scheme="https://ne-choi.github.io/tags/%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 1: XⅡ. 의사결정나무와 랜덤 포레스트</title>
    <link href="https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/X%E2%85%A1_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4%EC%99%80_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/"/>
    <id>https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/X%E2%85%A1_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4%EC%99%80_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/</id>
    <published>2020-12-13T00:00:00.000Z</published>
    <updated>2021-02-01T00:44:48.244Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><figcaption><span>setup, include</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knitr::opts_chunk$set(echo &#x3D; TRUE)</span><br></pre></td></tr></table></figure><ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  </li></ul><h1 id="XⅡ-의사결정나무와-랜덤-포레스트"><a href="#XⅡ-의사결정나무와-랜덤-포레스트" class="headerlink" title="XⅡ. 의사결정나무와 랜덤 포레스트"></a>XⅡ. 의사결정나무와 랜덤 포레스트</h1><h2 id="1-의사결정나무-Ⅰ"><a href="#1-의사결정나무-Ⅰ" class="headerlink" title="1. 의사결정나무 Ⅰ"></a>1. 의사결정나무 Ⅰ</h2><h3 id="의사결정나무"><a href="#의사결정나무" class="headerlink" title="의사결정나무"></a>의사결정나무</h3><ul><li><p>Decision Tree</p><ul><li>기계학습 중 하나로, 의사결정 규칙을 나무 형태로 분류해가는 분석 기법<ul><li>분석과정이 직관적이고 이해하기 쉬움</li><li>연속형/범주형 변수를 모두 사용할 수 있음</li><li>분지규칙은 불순도(범주가 섞여 있는 정도)를 최소화함</li></ul></li></ul></li><li><p>과정</p><ol><li>tree 형성(Growing tree): 너무 많이 키워서 분류하면 과적합 문제가 생김</li><li>tree 가지치기(Pruning tree): 과적합 예방을 위해</li><li>최적 tree로 분류(Classification)</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#decision tree packages download</span><br><span class="line">#install.packages(&quot;tree&quot;)</span><br><span class="line">#load library</span><br><span class="line">library(tree)</span><br><span class="line"></span><br><span class="line">#package for confusion matrix</span><br><span class="line">#install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line"></span><br><span class="line"># read csv file</span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week12_1&#x2F;iris.csv&quot;)</span><br><span class="line">attach(iris)</span><br></pre></td></tr></table></figure><ul><li>학습데이터 / 검증데이터 분할<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># training (n&#x3D;100)&#x2F; test data(n&#x3D;50) </span><br><span class="line">set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&lt;-nrow(iris)</span><br><span class="line">tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"># split train data and test data</span><br><span class="line">train&lt;-iris[tr.idx,]</span><br><span class="line">test&lt;-iris[-tr.idx,]</span><br><span class="line">#dim(train)</span><br><span class="line">#dim(test)</span><br></pre></td></tr></table></figure><h3 id="의사결정나무-함수"><a href="#의사결정나무-함수" class="headerlink" title="의사결정나무 함수"></a>의사결정나무 함수</h3>tree(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용</li></ul><h3 id="step-1-Growing-tree"><a href="#step-1-Growing-tree" class="headerlink" title="step 1: Growing tree"></a>step 1: Growing tree</h3><ul><li>학습데이터의 tree 결과<ul><li>마디 6에서는 더이상 분지할 필요 없음<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">treemod &lt;- tree(Species~., data&#x3D;train)</span><br><span class="line">treemod</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plot(treemod)</span><br><span class="line">text(treemod,cex&#x3D;1.5)</span><br><span class="line"></span><br><span class="line">table(train$Species)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="step-2-Pruning"><a href="#step-2-Pruning" class="headerlink" title="step 2. Pruning"></a>step 2. Pruning</h3><ul><li>cv.tree(tree모형결과, FUN= )</li><li>최적 tree 모형을 위한 가지치기</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cv.trees &lt;- cv.tree(treemod, FUN&#x3D;prune.misclass)</span><br><span class="line">cv.trees</span><br><span class="line">plot(cv.trees)</span><br></pre></td></tr></table></figure><ul><li>최종 트리모형<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># final tree model with the optimal node </span><br><span class="line">prune.trees&lt;-prune.misclass(treemod, best&#x3D;3)</span><br><span class="line">plot(prune.trees)</span><br><span class="line">text(prune.trees,pretty&#x3D;0, cex&#x3D;1.5)</span><br><span class="line">#help(prune.misclass)</span><br></pre></td></tr></table></figure></li></ul><h3 id="step-3"><a href="#step-3" class="headerlink" title="step 3."></a>step 3.</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># step 3: classify test data </span><br><span class="line">treepred&lt;-predict(prune.trees,test,type&#x3D;&#39;class&#39;)</span><br><span class="line"># classification accuracy</span><br><span class="line">confusionMatrix(treepred,test$Species)</span><br></pre></td></tr></table></figure><h2 id="2-의사결정나무-Ⅱ"><a href="#2-의사결정나무-Ⅱ" class="headerlink" title="2. 의사결정나무 Ⅱ"></a>2. 의사결정나무 Ⅱ</h2><h3 id="rpart-패키지"><a href="#rpart-패키지" class="headerlink" title="rpart 패키지"></a>rpart 패키지</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># other package for tree</span><br><span class="line"># install.packages(&quot;rpart&quot;)</span><br><span class="line"># install.packages(&quot;party&quot;)</span><br><span class="line">library(rpart)</span><br><span class="line">library(party)</span><br></pre></td></tr></table></figure><ul><li>rpart(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cl1&lt;-rpart(Species~., data&#x3D;train)</span><br><span class="line">plot(cl1)</span><br><span class="line">text(cl1, cex&#x3D;1.5)</span><br></pre></td></tr></table></figure></li><li>Prunning<ul><li>rpart 패키지는 과적합 우려가 있음(iris의 경우에는 필요 x)</li><li>printcp에서 xerror(cross validation error) 값이 최소가 되는 마디를 선택<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#pruning (cross-validation)-rpart</span><br><span class="line">printcp(cl1)</span><br><span class="line">plotcp(cl1)</span><br></pre></td></tr></table></figure></li></ul></li><li>rpart를 사용한 최종 tree 모형<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pcl1&lt;-prune(cl1, cp&#x3D;cl1$cptable[which.min(cl1$cptable[,&quot;xerror&quot;]),&quot;CP&quot;])</span><br><span class="line">plot(pcl1)</span><br><span class="line">text(pcl1)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>tree 함수를 이용했을 때와 동일한 결과</li></ul></li></ul><ul><li>정확도<ul><li>test data에 대한 정확도<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#measure accuracy(rpart)</span><br><span class="line">#package for confusion matrix</span><br><span class="line">#install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line"></span><br><span class="line">pred2&lt;- predict(cl1,test, type&#x3D;&#39;class&#39;)</span><br><span class="line">confusionMatrix(pred2,test$Species)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h3 id="party-패키지"><a href="#party-패키지" class="headerlink" title="party 패키지"></a>party 패키지</h3><ul><li>ctree(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용<ul><li>p-value를 기준으로 분지되는 포인트를 잡음<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#decision tree(party)-unbiased recursive partioning based on permutation test</span><br><span class="line">partymod&lt;-ctree(Species~.,data&#x3D;train)</span><br><span class="line">plot(partymod)</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="3-랜덤-포레스트"><a href="#3-랜덤-포레스트" class="headerlink" title="3. 랜덤 포레스트"></a>3. 랜덤 포레스트</h2><h3 id="랜덤-포레스트"><a href="#랜덤-포레스트" class="headerlink" title="랜덤 포레스트"></a>랜덤 포레스트</h3><ul><li>Random Forest<ul><li>2001년 Leo Breiman이 제안</li><li>의사결정나무의 단점(과적합)을 개선한 알고리즘</li><li>앙상블(Ensemble, 결합) 기법을 사용한 모델로, 주어진 데이터를 리샘플링하여 다수의 의사결정나무를 만든 다음, 여러 모델의 예측 결과를 종합해 정확도를 높이는 방법</li></ul></li></ul><h3 id="Bagging"><a href="#Bagging" class="headerlink" title="Bagging"></a>Bagging</h3><ul><li>Bagging(Bootstrap Aggregating)<ul><li>전체 데이터에서 학습데이터를 복원추출(resampling)하여 트리 구성</li><li>Training Data에서 Random Sampling</li><li>클래스 값 중 가장 많이 voting된 값이 결정</li></ul></li></ul><h3 id="랜덤-포레스트-패키지"><a href="#랜덤-포레스트-패키지" class="headerlink" title="랜덤 포레스트 패키지"></a>랜덤 포레스트 패키지</h3><ul><li><p>randomForest</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;randomForest&quot;)</span><br><span class="line">library(randomForest)</span><br></pre></td></tr></table></figure></li><li><p>iris 데이터 사용</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week12_3&#x2F;iris.csv&quot;)</span><br><span class="line">attach(iris)</span><br><span class="line"></span><br><span class="line"># training&#x2F; test data : n &#x3D; 150</span><br><span class="line">set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&lt;-nrow(iris)</span><br><span class="line">tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line"># split training and test data</span><br><span class="line">train&lt;-iris[tr.idx,]</span><br><span class="line">test&lt;-iris[-tr.idx,]</span><br><span class="line">#dim(train)</span><br><span class="line">#dim(test)</span><br></pre></td></tr></table></figure></li><li><p>랜덤포레스트 함수</p><ul><li>ramdomForest(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Random Forest : mtry&#x3D;2 (default&#x3D;sqrt(p))</span><br><span class="line">iris$Species &lt;- as.numeric(iris@Species)</span><br><span class="line"></span><br><span class="line">rf_out1&lt;-randomForest(Species~.,data&#x3D;train,importance&#x3D;T)</span><br><span class="line">rf_out1</span><br></pre></td></tr></table></figure></li></ul></li><li><p>변수의 중요도: ramdom forest 결과로부터 중요 변수 확인</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># important variables for RF</span><br><span class="line">round(importance(rf_out1), 2)</span><br></pre></td></tr></table></figure></li><li><p>추가</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#Random Forest : mtry&#x3D;4</span><br><span class="line">rf_out2&lt;-randomForest(Species~.,data&#x3D;train,importance&#x3D;T, mtry&#x3D;4)</span><br><span class="line">rf_out2</span><br><span class="line"></span><br><span class="line"># important variables for RF</span><br><span class="line">round(importance(rf_out2), 2)</span><br></pre></td></tr></table></figure></li><li><p>정확도</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#measuring accuracy(rf)</span><br><span class="line">rfpred&lt;-predict(rf_out2,test)</span><br><span class="line">confusionMatrix(rfpred,test$Species)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;figure class=&quot;highlight plain&quot;&gt;&lt;figcaption&gt;&lt;span&gt;setup, include&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="의사결정나무" scheme="https://ne-choi.github.io/tags/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4/"/>
    
    <category term="랜덤포레스트" scheme="https://ne-choi.github.io/tags/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 1: Ⅸ. 데이터 마이닝 기초</title>
    <link href="https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%A7%88%EC%9D%B4%EB%8B%9D_%EA%B8%B0%EC%B4%88/"/>
    <id>https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%A7%88%EC%9D%B4%EB%8B%9D_%EA%B8%B0%EC%B4%88/</id>
    <published>2020-12-13T00:00:00.000Z</published>
    <updated>2021-02-01T00:46:10.533Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  </li></ul><h1 id="Ⅸ-데이터-마이닝-기초"><a href="#Ⅸ-데이터-마이닝-기초" class="headerlink" title="Ⅸ. 데이터 마이닝 기초"></a>Ⅸ. 데이터 마이닝 기초</h1><h2 id="1-다중-회귀-분석-Ⅰ"><a href="#1-다중-회귀-분석-Ⅰ" class="headerlink" title="1. 다중 회귀 분석 Ⅰ"></a>1. 다중 회귀 분석 Ⅰ</h2><h3 id="데이터-마이닝-기법"><a href="#데이터-마이닝-기법" class="headerlink" title="데이터 마이닝 기법"></a>데이터 마이닝 기법</h3><ul><li><p>데이터 마이닝</p><ul><li><p>예측(prediction)</p><ul><li>야구선수의 연봉(차기 년도)</li><li>주식 변동(t+1 시점)</li><li>일기예보(비 올 확률)</li><li>수질오염(오염 수치)<br>→ 회귀분석, 선형모형, 비선형모형</li></ul></li><li><p>분류(classification)</p><ul><li>대출심사(허가/불가)</li><li>신용등급(A, B, C 등급)</li><li>고객 분류(구매빈도, 구매액)</li><li>품종분류<br>→ 의사결정나무, 서포트벡터머신, 판별분석, 로지스틱회귀모형</li></ul></li></ul></li></ul><h3 id="다중회귀모형"><a href="#다중회귀모형" class="headerlink" title="다중회귀모형"></a>다중회귀모형</h3><ul><li><p>다중회귀모형(multiple regression)</p><ul><li>종속변수 Y를 설명하는 데 k개의 독립변수 X<del>1</del>, …, X<del>k</del>가 있을 때, 다중회귀모형은 아래와 같이 정의</li><li>Y<del>i</del> = β<del>0</del> + β<del>1</del>X<del>1i</del> + β<del>2</del>X<del>2i</del> + … + β<del>k</del>X<del>ki</del> + ε<del>i</del>,  i = 1, 2, …, n<br>ε<del>i</del>N(0, σ^2^)</li><li>회귀계수 β<del>k</del>의 해석: 다른 독립변수가 일정할 때, X<del>k</del>의 한 단위 변화에 따른 평균 변화량</li></ul></li><li><p>autompg 데이터</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># autompg data</span><br><span class="line">car&lt;-read.csv(&quot;data&#x2F;week9_1&#x2F;autompg.csv&quot;)</span><br><span class="line">head(car)</span><br><span class="line">str(car)</span><br><span class="line">attach(car)</span><br></pre></td></tr></table></figure></li><li><p>다중회귀모형: lm(y변수~x1+x2+x3, data= )</p><ul><li>1st model: 전체변수를 모두 포함한 회귀모형</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># multiple regression : 1st full model </span><br><span class="line">r1&lt;-lm(mpg ~ disp+hp+wt+accler, data&#x3D;car)</span><br><span class="line">summary(r1)</span><br></pre></td></tr></table></figure><ul><li><p>결과 해석</p><ul><li>선형회귀식: mpg = 40.88 - 0.011 disp + 0.0048 hp - 0.0061 wt + 0.17 accler</li><li>선형회귀식의 결정계수: R^2^ = 0.7006</li></ul></li><li><p>추가: 의문점 확인</p><ul><li>check point 1: 마력(hp)가 높을수록 연비가 좋은가? (상식적으로는 음의 선형관계여야 함)<br>→ 데이터 탐색 필요</li></ul></li><li><p>데이터 탐색(Explanatory Data Analysis)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># pariwise plot - Explanatory Data Analysis</span><br><span class="line">var1&lt;-c(&quot;mpg&quot;,&quot;disp&quot;,&quot;hp&quot;,&quot;wt&quot;, &quot;accler&quot; )</span><br><span class="line">pairs(car[var1], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car$cyl))</span><br></pre></td></tr></table></figure><h2 id="2-다중회귀분석-Ⅱ"><a href="#2-다중회귀분석-Ⅱ" class="headerlink" title="2. 다중회귀분석 Ⅱ"></a>2. 다중회귀분석 Ⅱ</h2><h3 id="다중회귀분석-변수선택방법"><a href="#다중회귀분석-변수선택방법" class="headerlink" title="다중회귀분석: 변수선택방법"></a>다중회귀분석: 변수선택방법</h3></li><li><p>변수선택방법: 다수의 독립변수들이 있을 때 최종모형은?</p><ol><li>전진선택벅(forward selection): 독립변수 중, 종속변수에 가장 큰 영향을 주는 변수부터 모형에 포함</li><li>후진제거법(backward elimination): 독립변수를 모두 포함한 모형에서 가장 영향이 적은(중요하지 않은) 변수부터 제거</li><li>단계별방법(stepwise method): 전진선택법에 의해 변수 추가, 변수 추가 시 기존 변수의 중요도가 정해진 유의수준(threshold)에 포함되지 않으면 앞에서 넣은 변수 제거할 수 있음</li></ol></li></ul><ul><li>단계별 방법<ul><li>step(모형, direction = “both”)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 2rd model using variable selection method</span><br><span class="line"># step(r1, direction&#x3D;&quot;forward&quot;)</span><br><span class="line"># step(r1, direction&#x3D;&quot;backward&quot;)</span><br><span class="line"></span><br><span class="line"># stepwise selection</span><br><span class="line">step(r1, direction&#x3D;&quot;both&quot;)</span><br><span class="line">#step(lm(mpg ~ disp+hp+wt+accler, data&#x3D;car), direction&#x3D;&quot;both&quot;)</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>변수 제거: hp</li><li>최종 변수 선택: disp, wt, accler</li></ul></li></ul><ul><li>단계별 방법에 따른 최종 다중회귀모형<ul><li>2nd model: 단계별 선택 방법에 의한 회귀모형<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># final multiple regression</span><br><span class="line">r2&lt;-lm(mpg ~ disp+wt+accler, data&#x3D;car)</span><br><span class="line">summary(r2)</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>선형회귀식: mpg = 41.30 = 0.011 disp - 0.0062 wt + 0.17 accler</li><li>선형회귀식의 결정계수: R^2^ = 0.7004</li></ul></li></ul><h3 id="회귀분석의-가정과-진단"><a href="#회귀분석의-가정과-진단" class="headerlink" title="회귀분석의 가정과 진단"></a>회귀분석의 가정과 진단</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># residual diagnostic plot </span><br><span class="line">layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page </span><br><span class="line">plot(r2)</span><br></pre></td></tr></table></figure><h3 id="다중공선성"><a href="#다중공선성" class="headerlink" title="다중공선성"></a>다중공선성</h3><ul><li><p>다중공선성(Multicollinearity)</p><ul><li>독립변수들 사이에 상관관계가 있는 현상</li><li>다중공선성이 존재하는 경우 회귀계수 해석 불가능</li></ul></li><li><p>독립변수들간의 상관계수</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># check correlation between independent variables</span><br><span class="line">var2&lt;-c(&quot;disp&quot;,&quot;hp&quot;,&quot;wt&quot;, &quot;accler&quot; )</span><br><span class="line">cor(car[var2])</span><br><span class="line"></span><br><span class="line"># get correlation for each pair</span><br><span class="line"># cor(disp, wt)</span><br><span class="line"># cor(disp, accler)</span><br><span class="line"># cor(wt, accler)</span><br></pre></td></tr></table></figure></li><li><p>분산팽창계수(VIF; Variance Inflation Factor): 다중공선성의 척도</p><ul><li>VIF<del>j</del> = $\frac{1}{1 - R^2^<del>j</del>}$,  j = 1, 2, …, k</li><li>VIF는 다중공산성으로 인한 분산의 증가를 의미</li><li>R^2^<del>j</del>은 X<del>j</del>를 종속변수로 하고 나머지 변수를 독립변수로 하는 회귀모형에서의 결정계쑤</li><li>VIF<del>j</del>값 &gt; 10이면 다중공선성을 고려</li></ul></li><li><p>정리</p><ul><li>변수 선택 과정에서 상관계수가 높은 두 변수 중 하나만을 선택</li><li>더 많은 데이터 수집</li><li>능형회귀(ridge regression), 주성분회귀(principal components regression)</li></ul></li><li><p>car 패키지 내장 함수</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># check multicollinearity </span><br><span class="line"># variance inflation factor(VIF)</span><br><span class="line"># install.packages(&quot;car&quot;)</span><br><span class="line">library(car)</span><br><span class="line">vif(lm(mpg ~ disp+hp+wt+accler, data&#x3D;car))</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li><p>check point 1: coefficients &amp; R^2^</p><ul><li>선형회귀식: mpg = 41.30 - 0.011 disp - 0.0062 wt + 0.17 accler</li><li>선형회귀식의 결정계수: R^2^ = 0.7004</li></ul></li><li><p>check point 2: multi-collinearlity</p><ul><li>disp와 wt의 VIF가 10에 가까움<br>→ 크게 분제되지 않는다고 할 수 있음</li></ul></li><li><p>check point 3: residual plot</p></li><li><p>check point 4: outlier or other suspicious trend</p></li></ul></li></ul><h3 id="퀴즈-자료"><a href="#퀴즈-자료" class="headerlink" title="퀴즈 자료"></a>퀴즈 자료</h3><ul><li>변수 선택에 대한 R^2^ 확인<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># compare R-sqaured in regression </span><br><span class="line"># which one is the most important variable?</span><br><span class="line">summary(lm(mpg ~ disp))</span><br><span class="line">summary(lm(mpg ~ hp))</span><br><span class="line">summary(lm(mpg ~ wt))</span><br><span class="line">summary(lm(mpg ~ accler))</span><br></pre></td></tr></table></figure></li></ul><h3 id="다중회귀모형-데이터-탐색"><a href="#다중회귀모형-데이터-탐색" class="headerlink" title="다중회귀모형: 데이터 탐색"></a>다중회귀모형: 데이터 탐색</h3><ul><li>데이터 탐색(EDA: Explanatory Data Analysis)<ul><li>3rd model: a possible fitting method<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># more checking point</span><br><span class="line">plot(hp, mpg, col&#x3D;&quot;blue&quot;)</span><br></pre></td></tr></table></figure></li></ul></li><li>subset 생상 (hp &lt; 50)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">## 2nd model : data split</span><br><span class="line"># subset data hp&lt;50</span><br><span class="line">par(mfrow&#x3D;c(1,1))</span><br><span class="line">car_s1&lt;-subset(car, hp&lt;50)</span><br><span class="line">plot(car_s1$hp, car_s1$mpg,col&#x3D;10,  main&#x3D;&quot;hp&lt;50&quot;)</span><br><span class="line"># regression for hp&lt;50</span><br><span class="line">summary(lm(car_s1$mpg ~ car_s1$hp))</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>선형회귀식: mpg = 53.06 - 0.33 hp</li><li>선형회귀식의 결정계수: R^2^ = 0.45</li></ul></li></ul><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">step(r1, direction&#x3D;&quot;backward&quot;)</span><br></pre></td></tr></table></figure><h2 id="3-데이터-마이닝과-분류"><a href="#3-데이터-마이닝과-분류" class="headerlink" title="3. 데이터 마이닝과 분류"></a>3. 데이터 마이닝과 분류</h2><ul><li>분류규칙과 과적합</li></ul><h3 id="분류"><a href="#분류" class="headerlink" title="분류"></a>분류</h3><ul><li>분류분석(Classification Analysis)<ul><li>다수의 속성(attribute, variable)d을 갖는 객체(object)를 그룹 또는 범주(class, category)로 분류</li><li>학습 표본(training sample)으로부터 효율적인 분류규칙(classification rule)을 생성 → 오분류율 최소화(minimize cost function)</li></ul></li></ul><h3 id="분류규칙-예제"><a href="#분류규칙-예제" class="headerlink" title="분류규칙 예제"></a>분류규칙 예제</h3><ul><li><p>오분류율(misclassification rate)</p><ul><li>오분류 객체 수 / 전체 객체 수  </li></ul></li><li><p>과적합(overfitting)</p><ul><li><p>분류모형에서 훈련데이터에 과적합이 일어나면, 실제 데이터를 적용했을 때 더 높은 오분류율 발생</p></li><li><p>예측 모형에서의 과적합</p><ul><li>예측 모형에서 훈련 데이터에 대한 과적합 모델을 선택하는 경우, 실제 데이터 적용 시 더 높은 오차가 발생</li><li>이를 방지하기 위해, 학습데이터와 검증데이터를 5:5, 6:4, 7:3, 8:2로 분리하여 모형 성능을 비교 평가함</li></ul></li></ul></li></ul><h3 id="교차검증"><a href="#교차검증" class="headerlink" title="교차검증"></a>교차검증</h3><ul><li><p>교차검증(cross-validation)</p><ol><li>데이터 수집</li><li>training data / test data</li><li>training data에 Classifier(분류기) 적용</li><li>output  </li></ol></li><li><p>k-fold cross validation method 교차타당성 검증</p><ul><li>5-fold cross-validation: n=100이면, 5등분으로 나누어 4등분은 학습데이터로 예측 모형을 구성하고, 나머지 5등분째 데이터로 검증</li></ul></li></ul><h2 id="4-학습테이터와-검증데이터"><a href="#4-학습테이터와-검증데이터" class="headerlink" title="4. 학습테이터와 검증데이터"></a>4. 학습테이터와 검증데이터</h2><h3 id="iris-데이터-설명"><a href="#iris-데이터-설명" class="headerlink" title="iris 데이터 설명"></a>iris 데이터 설명</h3><ul><li>iris 데이터(붓꽃 데이터)<ol><li>목적: 꽃잎 폭과 길이에 관한 4개 변수로 꽃의 종류를 예측하는 것</li><li>타겟변수(y): setosa, versicolor, virginica</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># read csv file</span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week9_4&#x2F;iris.csv&quot;)</span><br><span class="line">head(iris)</span><br><span class="line">str(iris)</span><br><span class="line">attach(iris)</span><br></pre></td></tr></table></figure><ul><li>input 변수(독립변수): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width</li><li>output 변수(종속변수): Species</li></ul><h3 id="학습데이터와-검증데이터-생성"><a href="#학습데이터와-검증데이터-생성" class="headerlink" title="학습데이터와 검증데이터 생성"></a>학습데이터와 검증데이터 생성</h3><ul><li>iris 데이터: 150개 데이터<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># training&#x2F; test data : n&#x3D;150</span><br><span class="line">set.seed(100, sample.kind&#x3D;&quot;Rounding&quot;) # seed 1000 지정한 이유: 동일한 데이터를 사용하기 위해 임의 숫자로 고정</span><br><span class="line">N&#x3D;nrow(iris) # ramdom sampling을 위해 데이터에 number 부여</span><br><span class="line">tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # 100개는 학습데이터, 50개는 검증데이터로 사용하라</span><br><span class="line">tr.idx</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># attributes in training and test</span><br><span class="line">iris.train &lt;- iris[tr.idx, -5] # 5번째 열의 종속변수를 제외한 100개의 데이터</span><br><span class="line">iris.test &lt;- iris[-tr.idx, -5] # 5번째 열의 종속변수를 제외한 50개의 데이터</span><br></pre></td></tr></table></figure></li><li>iris 데이터의 타겟변수(학습데이터의 타겟변수, 검증데이터의 타겟변수)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># target value in training and test</span><br><span class="line">trainLabels &lt;- iris[tr.idx, 5]</span><br><span class="line">testLabels &lt;- iris[-tr.idx, -5]</span><br></pre></td></tr></table></figure><h3 id="퀴즈-1"><a href="#퀴즈-1" class="headerlink" title="퀴즈"></a>퀴즈</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># to get frequency of class in test set</span><br><span class="line">table(testLabels)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅸ-데이터-마이닝-기초&quot;&gt;&lt;a href</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="데이터마이닝" scheme="https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/"/>
    
    <category term="다중회귀모형" scheme="https://ne-choi.github.io/tags/%EB%8B%A4%EC%A4%91%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"/>
    
  </entry>
  
  <entry>
    <title>머신러닝 기법과 R 프로그래밍 1: XI. 서포트벡터머신</title>
    <link href="https://ne-choi.github.io/2020/12/12/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/XI_%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/"/>
    <id>https://ne-choi.github.io/2020/12/12/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/XI_%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/</id>
    <published>2020-12-12T00:00:00.000Z</published>
    <updated>2021-02-01T00:44:57.709Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  </li></ul><h1 id="XI-서포트벡터머신"><a href="#XI-서포트벡터머신" class="headerlink" title="XI. 서포트벡터머신"></a>XI. 서포트벡터머신</h1><h2 id="1-서포트벡터머신-Ⅰ"><a href="#1-서포트벡터머신-Ⅰ" class="headerlink" title="1. 서포트벡터머신 Ⅰ"></a>1. 서포트벡터머신 Ⅰ</h2><h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><ul><li><p>머신러닝 </p><ul><li>지도학습<ul><li>분류모델<ul><li>kNN모델</li><li><strong>SVM(서포트벡터머신)</strong></li></ul></li></ul></li><li>비지도학습<ul><li>군집모델</li></ul></li></ul></li><li><p>서포트벡터머신 장단점</p></li></ul><table><thead><tr><th>장점</th><th>단점</th></tr></thead><tbody><tr><td>상대적으로 정확도가 높음</td><td>해석상 어려움 발생</td></tr><tr><td>다양한 데이터(연속형, 범주형)를 다룰 수 있음</td><td>데이터가 많을 때 속도와 메모리가 많이 듦</td></tr></tbody></table><ul><li><p><strong>선형 SVM</strong></p><ul><li>각 클래스를 분류해주는 margin(각 클래스를 분류하는 하이퍼플레인)을 최대화하는 벡터를 찾는 분석 기법</li></ul></li><li><p><strong>비선형 SVM</strong></p><ul><li>대부분 패턴은 선형적 분리가 불가능<ul><li>비선형 패턴의 임력공간을 선형 패턴의 feature space로 변환</li><li>Kernel method로 비선형 경계면 도출</li></ul></li></ul></li></ul><h3 id="서포트백터머신-패키지와-함수"><a href="#서포트백터머신-패키지와-함수" class="headerlink" title="서포트백터머신 패키지와 함수"></a>서포트백터머신 패키지와 함수</h3><ul><li>서포트벡터머신 수행을 위한 패키지: e1071</li><li>서포트벡터머신 함수: svm(y변수~x변수, data= )</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;e1071&quot;)</span><br><span class="line">library (e1071)</span><br><span class="line"></span><br><span class="line">iris&lt;-read.csv(&quot;data&#x2F;week11_1&#x2F;iris.csv&quot;)</span><br><span class="line">attach(iris)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">## classification </span><br><span class="line"># 1. use all data </span><br><span class="line"></span><br><span class="line">iris$Species &lt;- as.factor(iris$Species)</span><br><span class="line"></span><br><span class="line">m1&lt;- svm(Species ~., data &#x3D; iris)</span><br><span class="line">summary(m1)</span><br></pre></td></tr></table></figure><ul><li>svm 옵션(default)<ul><li>kernel = radial basis function</li><li>gamma = 1/(# of dimension) (1/4=0.25)</li></ul></li></ul><h3 id="서포트벡터머신-결과"><a href="#서포트벡터머신-결과" class="headerlink" title="서포트벡터머신 결과"></a>서포트벡터머신 결과</h3><ul><li>svm 모델에 적용한 예측범주와 실제범주 비교(전체 데이터 사용 결과)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># classify all data using svm result (m1)</span><br><span class="line"># first 4 variables as attribute variables</span><br><span class="line">x&lt;-iris[, -5] # iris 데이터엣 타겟값인 5번째 열을 제외한 데이터, 즉 4개의 독립변수만 있는 데이터</span><br><span class="line">pred &lt;- predict(m1, x) # svm모델 m1을 적용해 예측된 범주값을 pred로 저장</span><br><span class="line"></span><br><span class="line"># Check accuracy (compare predicted class(pred) and true class(y))</span><br><span class="line"># y &lt;- Species or y&lt;-iris[,5]</span><br><span class="line">y&lt;-iris[,5]</span><br><span class="line">table(pred, y)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>오분류율: (4+1)/150 = 0.033%</li></ul></li></ul><h3 id="서포트벡터머신-시각화"><a href="#서포트벡터머신-시각화" class="headerlink" title="서포트벡터머신 시각화"></a>서포트벡터머신 시각화</h3><ul><li>iris 데이터의 서포트벡터머신 결과(전체 데이터 사용 결과)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># visualize classes by color</span><br><span class="line">plot(m1, iris,  Petal.Width~Petal.Length, slice&#x3D;list(Sepal.Width&#x3D;3, Sepal.Length&#x3D;4))</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>4개 변수 중, petal.width와 petal.length가 중요한 변수</li></ul></li></ul><h2 id="2-서포트벡터머신-Ⅱ"><a href="#2-서포트벡터머신-Ⅱ" class="headerlink" title="2. 서포트벡터머신 Ⅱ"></a>2. 서포트벡터머신 Ⅱ</h2><h3 id="서포트벡터머신-kernel-함수"><a href="#서포트벡터머신-kernel-함수" class="headerlink" title="서포트벡터머신(kernel 함수)"></a>서포트벡터머신(kernel 함수)</h3><ul><li><p>커널(kernel)이란?</p><ul><li>Input Space → Feature Space</li><li>f(x) = φ(x)^T^w + b</li></ul></li><li><p>커널함수(kernel function)</p><ul><li><p>x의 기저함수(basis function)</p></li><li><p>x에 대한 <strong>새로운 특징을 추출하는 변환함수</strong></p><ul><li>좋은 커널함수: x 데이터의 모든 정보를 보존하면서 class를 잘 분류할 수 있는 커널함수</li></ul></li><li><p>커널함수와 기저함수의 관계: <strong>K(x<del>i</del>,x<del>j</del>) = φ(x<del>i</del>)’φ(x<del>j</del>)</strong></p><ul><li>radial: K(x<del>i</del>,x<del>j</del>) = exp($\frac{-||x<del>i</del> - x<del>j</del>||^2^}{2σ^2^}$)</li><li>polynomial: K(x<del>i</del>,x<del>j</del>) = (x<del>i</del>‘x<del>j</del> + 1)^r^</li><li>sigmoid: K(x<del>i</del>,x<del>j</del>) = tanh(kx<del>i</del>‘x<del>j</del> - σ)</li></ul></li></ul></li><li><p>패키지 설치</p><ul><li>서포트벡터머신 수행 패키지: e1071</li><li>오분류율 교차표(confusion matrix) 생성 패키지: caret<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;caret&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">library(caret)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>iris 데이터 (학습데이터와 검증데이터 분할)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># training (100) &amp; test set (50)</span><br><span class="line">set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line">N&#x3D;nrow(iris)</span><br><span class="line">tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line"># target variable</span><br><span class="line">y&#x3D;iris[,5]</span><br><span class="line"># split train data and test data</span><br><span class="line">train&#x3D;iris[tr.idx,]</span><br><span class="line">test&#x3D;iris[-tr.idx,]</span><br></pre></td></tr></table></figure></li><li><p>커널함수 적용</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#svm using kernel</span><br><span class="line">m1&lt;-svm(Species~., data &#x3D; train) # default: radial</span><br><span class="line">summary(m1)</span><br><span class="line">m2&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;polynomial&quot;)</span><br><span class="line">summary(m2)</span><br><span class="line">m3&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;sigmoid&quot;)</span><br><span class="line">summary(m3)</span><br><span class="line">m4&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;linear&quot;)</span><br><span class="line">summary(m4)</span><br></pre></td></tr></table></figure><h3 id="서포트벡터머신-결과-1"><a href="#서포트벡터머신-결과-1" class="headerlink" title="서포트벡터머신 결과"></a>서포트벡터머신 결과</h3></li></ul><ol><li>정확도 측정: kernel-radial basis function<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#measure accuracy</span><br><span class="line">pred11&lt;-predict(m1,test) # radial basis</span><br><span class="line">confusionMatrix(pred11, test$Species)</span><br><span class="line">#table(pred11, y[-tr.idx])</span><br></pre></td></tr></table></figure></li></ol><ul><li>결과 해석<ul><li>예측범주: Prediction</li><li>실제범주: Reference</li><li>정확도: 0.96</li></ul></li></ul><ol start="2"><li>정확도 측정: kernel-polynomial<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred12&lt;-predict(m2,test) # polynomial</span><br><span class="line">confusionMatrix(pred12, test$Species)</span><br><span class="line">#table(pred12, y[-tr.idx])</span><br></pre></td></tr></table></figure></li></ol><ul><li>결과 해석<ul><li>정확도: 0.9</li></ul></li></ul><ol start="3"><li>정확도 측정: kernel-sigmoid<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred13&lt;-predict(m3,test) # simoid</span><br><span class="line">confusionMatrix(pred13, test$Species)</span><br><span class="line">#table(pred13, y[-tr.idx])</span><br></pre></td></tr></table></figure></li></ol><ul><li>결과 해석<ul><li>정확도: 0.9</li></ul></li></ul><ol start="4"><li>정확도 측정: kernel-linear<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pred14&lt;-predict(m4,test) # linear</span><br><span class="line">confusionMatrix(pred14, test$Species)</span><br><span class="line">#table(pred14, y[-tr.idx])</span><br></pre></td></tr></table></figure></li></ol><ul><li>결과 해석<ul><li>정확도: 0.9393</li></ul></li></ul><h3 id="서포트벡터머신-정확도-정리"><a href="#서포트벡터머신-정확도-정리" class="headerlink" title="서포트벡터머신 정확도 정리"></a>서포트벡터머신 정확도 정리</h3><ul><li>linear로만 처리해도 정확도 높은 경우가 많음</li></ul><h2 id="3-서포트벡터머신-Ⅲ"><a href="#3-서포트벡터머신-Ⅲ" class="headerlink" title="3. 서포트벡터머신 Ⅲ"></a>3. 서포트벡터머신 Ⅲ</h2><h3 id="Breast-Cancer-데이터"><a href="#Breast-Cancer-데이터" class="headerlink" title="Breast Cancer 데이터"></a>Breast Cancer 데이터</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;e1071&quot;)</span><br><span class="line"># load package for support vector machine</span><br><span class="line">library(e1071) #svm model</span><br><span class="line"></span><br><span class="line"># install.packages(&quot;caret&quot;)</span><br><span class="line"># load package for Confusion matrix</span><br><span class="line">library(caret)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># read data</span><br><span class="line">cancer&lt;-read.csv(&quot;data&#x2F;week11_3&#x2F;cancer.csv&quot;)</span><br><span class="line">head(cancer, n&#x3D;10)</span><br><span class="line"></span><br><span class="line"># remover X1 column(ID number) # ID number 필요 없는 feature로 제거</span><br><span class="line">cancer&lt;-cancer[, names(cancer) !&#x3D; &quot;X1&quot;]</span><br><span class="line">attach(cancer)</span><br><span class="line"></span><br><span class="line"># 종속변수형을 factor 함수로 변경해야 함(R 4.0버전 이후)</span><br><span class="line">cancer$Y &lt;- as.factor(cancer$Y)</span><br></pre></td></tr></table></figure><ul><li>Breast Cancer Wisconsin (Diagnostic) Data Set<ul><li>세침흡인 세포검사를 통해 얻은 683개 유방조직의 9개 특성을 나타냄</li><li>input변수(독립변수): 9개, output변수(종속, 타겟변수): 1개</li></ul></li></ul><ul><li>학습데이터 / 검증데이터 분할<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># training (455) &amp; test set (228)</span><br><span class="line"># set.seed(1000)</span><br><span class="line">N&#x3D;nrow(cancer)</span><br><span class="line">set.seed(998, sample.kind&#x3D;&quot;Rounding&quot;)</span><br><span class="line"></span><br><span class="line"># split train data and test data</span><br><span class="line">tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE)</span><br><span class="line">train &lt;- cancer[ tr.idx,]</span><br><span class="line">test  &lt;- cancer[-tr.idx,]</span><br></pre></td></tr></table></figure></li></ul><h3 id="서포트벡터머신-결과-2"><a href="#서포트벡터머신-결과-2" class="headerlink" title="서포트벡터머신 결과"></a>서포트벡터머신 결과</h3><ul><li>kernel 함수에 따른 서포트벡터머신<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#svm using kernel</span><br><span class="line">m1&lt;-svm(Y~., data &#x3D; train)</span><br><span class="line">summary(m1)</span><br><span class="line"></span><br><span class="line">#measure accuracy</span><br><span class="line">pred11&lt;-predict(m1,test) # radial basis</span><br><span class="line">confusionMatrix(pred11, test$Y)</span><br><span class="line">#table(pred11, y[-tr.idx])</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>악성으로 오분류 4, 정상으로 오분류 1</li><li>정확도: 0.9781</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m2&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;polynomial&quot;)</span><br><span class="line">summary(m2)</span><br><span class="line"></span><br><span class="line">pred12&lt;-predict(m2,test) # polynomial</span><br><span class="line">confusionMatrix(pred12, test$Y)</span><br><span class="line">#table(pred12, y[-tr.idx])</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>악성으로 오분류 0, 정상으로 오분류 12</li><li>정확도: 0.9474</li><li>정확도는 94% 이상이나, 암 진단 특성상 정상으로 오분류하는 경우가 많으면 안 됨</li><li>False Positive(예측: P, 실제: N)가 더 위험한 경우</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m3&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;sigmoid&quot;)</span><br><span class="line">summary(m3)</span><br><span class="line"></span><br><span class="line">pred13&lt;-predict(m3,test) # sigmoid</span><br><span class="line">confusionMatrix(pred13, test$Y)</span><br><span class="line">#table(pred13, y[-tr.idx])</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>악성으로 오분류 5, 정상으로 오분류 3</li><li>정확도: 0.9649</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">m4&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;linear&quot;)</span><br><span class="line">summary(m4)</span><br><span class="line"></span><br><span class="line">pred14&lt;-predict(m4,test) # linear</span><br><span class="line">confusionMatrix(pred14, test$Y)</span><br><span class="line">#table(pred14, y[-tr.idx])</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>결과 해석</p><ul><li>악성으로 오분류 1, 정상으로 오분류 3</li><li>정확도: 0.9825</li></ul></li><li><p>전체 결론</p><ul><li>정확도만 보았을 때 linear 모델이 적합하나, True Negative가 위험하기 때문에 radial 모델이 더 적합할 수 있음</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;XI-서포트벡터머신&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="ML" scheme="https://ne-choi.github.io/categories/Study/Postech/ML/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
    <category term="SVM" scheme="https://ne-choi.github.io/tags/SVM/"/>
    
    <category term="서포트벡터머신" scheme="https://ne-choi.github.io/tags/%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/"/>
    
  </entry>
  
  <entry>
    <title>빅데이터 분석과 R 프로그래밍 2: Ⅷ. 선형회귀모형과 텍스트마이닝</title>
    <link href="https://ne-choi.github.io/2020/12/10/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A7_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/"/>
    <id>https://ne-choi.github.io/2020/12/10/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A7_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/</id>
    <published>2020-12-09T15:00:00.000Z</published>
    <updated>2021-02-01T00:42:26.217Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="Ⅷ-선형회귀모형과-텍스트마이닝"><a href="#Ⅷ-선형회귀모형과-텍스트마이닝" class="headerlink" title="Ⅷ. 선형회귀모형과 텍스트마이닝"></a>Ⅷ. 선형회귀모형과 텍스트마이닝</h1><h2 id="1-상관분석"><a href="#1-상관분석" class="headerlink" title="1. 상관분석"></a>1. 상관분석</h2><h3 id="상관계수"><a href="#상관계수" class="headerlink" title="상관계수"></a>상관계수</h3><ul><li>상관계수: cor(변수1, 변수2)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">car &lt;- read.csv(&quot;week8_1&#x2F;autompg.csv&quot;)</span><br><span class="line"></span><br><span class="line">car1 &lt;- subset(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 | cyl&#x3D;&#x3D;8)</span><br><span class="line">attach(car1)</span><br></pre></td></tr></table></figure><ul><li>cor의 디폴트는 pearson의 상관계수   <ul><li>kendall 상관계수 or spearman의 상관계수를 구할 때: cor(변수1, 변수2, method=c(“spearman”))</li></ul></li></ul><h3 id="상관계수와-산점도"><a href="#상관계수와-산점도" class="headerlink" title="상관계수와 산점도"></a>상관계수와 산점도</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># new variable lists</span><br><span class="line">vars1&lt;-c(&quot;disp&quot;, &quot;wt&quot;, &quot;accler&quot;, &quot;mpg&quot;)</span><br><span class="line"># pariwise plot</span><br><span class="line">pairs(car1[vars1], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1))</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>차량 무게와 배기량은 정비례관계 = 양의 상관계수</li><li>mpg(연비)와 (wt, disp)는 상관성이 높음 = 반비례, 음의 상관계수</li><li>cylinder별로 색상 표시(파랑: 4, 핑크: 6, 회색: 8)</li></ul></li></ul><h3 id="상관분석"><a href="#상관분석" class="headerlink" title="상관분석"></a>상관분석</h3><ul><li>상관계수(r)은 절댓값이 0-1 사이<ul><li>절댓값이 0에 가까울수록 상관관계가 없음</li><li>절댓값이 1에 가까울수록 강한 상관성이 있음</li></ul></li></ul><h3 id="통계치와-그래프"><a href="#통계치와-그래프" class="headerlink" title="통계치와 그래프"></a>통계치와 그래프</h3><ul><li>Monkey 데이터 + King Kong 한 마리<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">## Monkey data</span><br><span class="line">monkey&lt;-read.csv(&quot;week8_1&#x2F;monkey.csv&quot;)</span><br><span class="line">attach(monkey)</span><br><span class="line"></span><br><span class="line"># correlation coefficients</span><br><span class="line">cor(height, weight)</span><br><span class="line"></span><br><span class="line"># scatterplot for weight and height</span><br><span class="line">par(mfrow&#x3D;c(1, 1))</span><br><span class="line">plot(height, weight, pch&#x3D;16, col&#x3D;3,main&#x3D;&quot;Monkey data&quot;)</span><br><span class="line"></span><br><span class="line"># add the best fit linear line (lec4_3.R)</span><br><span class="line">abline(lm(weight~height), col&#x3D;&quot;blue&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># linear model and summary of linear model</span><br><span class="line">m1&lt;-lm(weight~height)</span><br><span class="line">summary(m1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">## Monkey data + Kingkong</span><br><span class="line">monkey1 &lt;- read.csv(&quot;week8_1&#x2F;monkey_k.csv&quot;)</span><br><span class="line">head(monkey1)</span><br><span class="line">dim(monkey1)</span><br><span class="line">attach(monkey1)</span><br><span class="line"></span><br><span class="line"># correlation coefficients</span><br><span class="line">cor(height, weight)</span><br><span class="line"># scatterplot for weight and height</span><br><span class="line">par(mfrow&#x3D;c(1, 1))</span><br><span class="line">plot(height, weight, pch&#x3D;16, col&#x3D;3,main&#x3D;&quot;Monkey data&quot;)</span><br><span class="line"></span><br><span class="line"># add the best fit linear line (lec4_3.R)</span><br><span class="line">abline(lm(weight~height), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br></pre></td></tr></table></figure></li><li>Monkey 데이터에 King Kong 한 마리 데이터 추가<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># linear model and summary of linear model for monkey+king kong</span><br><span class="line">m2&lt;-lm(weight~height)</span><br><span class="line">summary(m2)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>한 마리의 킹콩 데이터가 몸무게와 신장의 상관관계 해석을 완전히 바꿀 수 있음</li></ul></li></ul><h2 id="2-선형회귀모형"><a href="#2-선형회귀모형" class="headerlink" title="2. 선형회귀모형"></a>2. 선형회귀모형</h2><h3 id="회귀분석-단순회귀모형"><a href="#회귀분석-단순회귀모형" class="headerlink" title="회귀분석: 단순회귀모형"></a>회귀분석: 단순회귀모형</h3><ul><li>단순회귀모형: lm(y변수~x변수, data= )<ul><li>종속변수: mpg(연비), 독립변수: wt(차량 무게)</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r1&lt;-lm(mpg~wt, data&#x3D;car1)</span><br><span class="line">summary(r1)</span><br><span class="line">anova(r1)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>선형회귀식: y(mpg)= 46.60 - 0.0077(wt)</li><li>선형회귀식의 결정계수: R^2^= 0.709  </li></ul></li></ul><ul><li>산점도에 회귀선 그리기<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># scatterplot with best fit lines</span><br><span class="line">par(mfrow&#x3D;c(1,1))</span><br><span class="line">plot(wt, mpg,  col&#x3D;as.integer(car1$cyl), pch&#x3D;19)</span><br><span class="line"># best fit linear line</span><br><span class="line">abline(lm(mpg~wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br></pre></td></tr></table></figure></li><li>코드 해석<ul><li>col=as.factor(cyl): 배기통별로 컬러를 칠해라</li><li>plot(x축변수, y축변수)</li><li>abline: add line(선 추가 함수)</li><li>lm(y변수~x변수) lm = linear model(선형모형)</li></ul></li></ul><h3 id="회귀분석의-목적"><a href="#회귀분석의-목적" class="headerlink" title="회귀분석의 목적"></a>회귀분석의 목적</h3><ul><li>예측(prediction)과 추정(estimation)<ul><li>선형모형: 독립변수와 종속변수 관계가 선형식으로 적합</li><li>최소자승법(least squares method): 예측값과 관측치간의 오차를 최소화하는 회귀계수를 추정</li></ul></li></ul><h3 id="휘귀분석-모형-적합도"><a href="#휘귀분석-모형-적합도" class="headerlink" title="휘귀분석: 모형 적합도"></a>휘귀분석: 모형 적합도</h3><ul><li><p>회귀식에 의해 설명되는 부분(SSR), 설명되지 않는 부분(SSE)</p><ul><li><p>R^2^ = $\frac{SSR}{SST}$ = %\frac{17029}{24017} = 0.709</p><ul><li>R^2^는 1에 가까울수록 회귀식에 의해 적합되는 부분이 높음</li><li>R^2^는 0에 가까우면 주어진 독립변수들에 의해 설명(예측 혹 적합)되는 부분이 없다고 할 수 있음</li></ul></li><li><p>참고</p><ul><li>SST = Total Sum of Squares</li><li>SSR = Regression Sum of Squares</li><li>SSE = error(residual) sum of squares</li></ul></li></ul></li><li><p>모형의 적합도와 결정계수</p><ul><li>R^2^: 0 ≤ R^2^ ≤ 1</li></ul></li></ul><h3 id="회귀모형-단순회귀모형"><a href="#회귀모형-단순회귀모형" class="headerlink" title="회귀모형: 단순회귀모형"></a>회귀모형: 단순회귀모형</h3><ul><li>종속변수: mpg(연비), 독립변수: disp(배기량)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r2&lt;-lm(mpg~disp, data&#x3D;car1)</span><br><span class="line">summary(r2)</span><br><span class="line">anova(r2)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>선형회귀식 y(mpg) = 35.49 - 0.0614(disp)</li><li>선형회귀식의 결정계수: R^2^ = 0.67</li></ul></li></ul><ul><li>pariwise scatterplot<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># pariwise plot</span><br><span class="line">pairs(car1[,1:6], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1))</span><br></pre></td></tr></table></figure></li></ul><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><p>r1&lt;-lm(mpg~wt, data=car1)</p><p>summary(r1) </p><p>r3 &lt;-lm(mpg~wt+accler, data=car1)</p><p>summary(r3)</p><h2 id="3-다중회귀모형"><a href="#3-다중회귀모형" class="headerlink" title="3. 다중회귀모형"></a>3. 다중회귀모형</h2><ul><li>다중회귀모형: lm(y~x1+x2+x3, data= )<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># multiple Regression</span><br><span class="line">r2&lt;-lm(mpg~wt+accler, data&#x3D;car1)</span><br><span class="line">summary(r2)</span><br><span class="line">anova(r2)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>선형회귀식 y(mpg) = 44.03 - 0.0057(wt) - 0.0176(disp)</li><li>p-value도 매우 적은 값<br>→ 두 변수 모두 유의함</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r1 &lt;- lm(mpg~wt+disp, data&#x3D;car1)</span><br><span class="line">summary(r1)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>선형회귀식의 결정계수</li><li>R^2^ = 0.7159 (비교: 단순회귀 wt, R^2^=0.709)</li></ul></li></ul><h3 id="회귀분석-잔차의-산점도"><a href="#회귀분석-잔차의-산점도" class="headerlink" title="회귀분석: 잔차의 산점도"></a>회귀분석: 잔차의 산점도</h3><ul><li><p>잔차산점도: 오차의 가정에 대한 적합성</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># residual diagnostic plot</span><br><span class="line">layout(matrix(c(1,2,3,4),2,2)) # optrional 4 graphs&#x2F;page</span><br><span class="line">plot(r2)</span><br></pre></td></tr></table></figure></li><li><p>오차에 대한 가정</p><ul><li>잔차가 정규분포한다</li><li>평균은 0, 분산은 모두 동일하다 (선형회귀의 가정)</li></ul></li><li><p>정규확률도(Normal Q-Q)</p><ul><li>점선에 거의 붙어있으면 정규분포한다고 함</li></ul></li><li><p>Residuals vs Leverage</p><ul><li>outlier를 찾아낼 때 사용</li></ul></li><li><p>Residuals vs Fitted</p><ul><li>분포 정도를 살펴봄</li></ul></li></ul><h3 id="그룹별-회귀모형"><a href="#그룹별-회귀모형" class="headerlink" title="그룹별 회귀모형"></a>그룹별 회귀모형</h3><ul><li>mpg=f(wt), cylinder별<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># filtered data : regression by group</span><br><span class="line">car2&lt;-filter(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 )</span><br><span class="line">car3&lt;-filter(car, cyl&#x3D;&#x3D;8)</span><br><span class="line"></span><br><span class="line"># car cyl&#x3D;4,6 vs cyl&#x3D;8 </span><br><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">plot(car2$wt, car2$mpg, col&#x3D;as.integer(car2$cyl), pch&#x3D;19, main&#x3D;&quot;cyl&#x3D;4 or 6&quot;)</span><br><span class="line"># best fit linear line</span><br><span class="line">abline(lm(car2$mpg~car2$wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br><span class="line"></span><br><span class="line">plot(car3$wt, car3$mpg, col&#x3D;&quot;green&quot;, pch&#x3D;19, main&#x3D;&quot;cyl&#x3D;8&quot;)</span><br><span class="line"># best fit linear line</span><br><span class="line">abline(lm(car3$mpg~car3$wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># compare with total </span><br><span class="line">m2 &lt;- lm(mpg~wt, data&#x3D;car2)</span><br><span class="line">summary(m2)</span><br><span class="line"></span><br><span class="line">m3&lt;-lm(mpg~wt, data&#x3D;car3)</span><br><span class="line">summary(m3)</span><br><span class="line"></span><br><span class="line">m0&lt;-lm(mpg~wt, data&#x3D;car1)</span><br><span class="line">summary(m0)</span><br></pre></td></tr></table></figure></li></ul><h2 id="4-회귀분석의-진단과-평가"><a href="#4-회귀분석의-진단과-평가" class="headerlink" title="4. 회귀분석의 진단과 평가"></a>4. 회귀분석의 진단과 평가</h2><h3 id="회귀분석-데이터"><a href="#회귀분석-데이터" class="headerlink" title="회귀분석 데이터"></a>회귀분석 데이터</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># subset of flight data in SFO (n&#x3D;2974)</span><br><span class="line"># dest&#x3D;&quot;SFO&quot;, origin&#x3D;&#x3D;&quot;JFK&quot;, arr_delay&lt;420, arr_delay&gt;0</span><br><span class="line">SF&lt;-read.csv(&quot;week8_4&#x2F;SF_2974.csv&quot;, stringsAsFactors &#x3D; TRUE)</span><br><span class="line">attach(SF)</span><br><span class="line"></span><br><span class="line">library(ggplot2)</span><br><span class="line">library(dplyr)</span><br></pre></td></tr></table></figure><h3 id="데이터-기술통계치"><a href="#데이터-기술통계치" class="headerlink" title="데이터 기술통계치"></a>데이터 기술통계치</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">head(SF)</span><br><span class="line">str(SF)</span><br><span class="line">dim(SF)</span><br></pre></td></tr></table></figure><h3 id="데이터-산점도"><a href="#데이터-산점도" class="headerlink" title="데이터 산점도"></a>데이터 산점도</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 1. graphic analytics</span><br><span class="line">SF %&gt;% </span><br><span class="line">  ggplot(aes(arr_delay)) + geom_histogram(binwidth &#x3D; 15)</span><br><span class="line"></span><br><span class="line"># 2. graphic analytics</span><br><span class="line">SF %&gt;%</span><br><span class="line">  ggplot(aes(x &#x3D; hour, y &#x3D; arr_delay)) +</span><br><span class="line">  geom_boxplot(alpha &#x3D; 0.1, aes(group &#x3D; hour)) + geom_smooth(method &#x3D; &quot;lm&quot;) + </span><br><span class="line">  xlab(&quot;Scheduled hour of departure&quot;) + ylab(&quot;Arrival delay (minutes)&quot;) + </span><br><span class="line">  coord_cartesian(ylim &#x3D; c(0, 120))</span><br></pre></td></tr></table></figure><h3 id="회귀분석-단순회귀모형-1"><a href="#회귀분석-단순회귀모형-1" class="headerlink" title="회귀분석: 단순회귀모형"></a>회귀분석: 단순회귀모형</h3><ul><li>종속변수: arr_delay, 독립변수: hour(출발시간)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># linear regression</span><br><span class="line">m1&lt;- lm(arr_delay ~ hour , data &#x3D; SF)</span><br><span class="line">summary(m1)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ul><li>선형회귀식 y(arr_delay) = 7.54 + 2.55(hour)</li><li>R^2^ = 0.03965 설명력이 높다고 하기 어려움</li></ul></li></ul><h3 id="산점도에-회귀선-그리기"><a href="#산점도에-회귀선-그리기" class="headerlink" title="산점도에 회귀선 그리기"></a>산점도에 회귀선 그리기</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># scatterplot with best fit lines</span><br><span class="line">library(dplyr)</span><br><span class="line"></span><br><span class="line">par(mfrow&#x3D;c(1,1))</span><br><span class="line">SF %&gt;%</span><br><span class="line">  plot(hour, arr_delay, col&#x3D;as.integer(SF$carrier), pch&#x3D;19)%&gt;% </span><br><span class="line">  # best fit linear line</span><br><span class="line">  abline(lm(arr_delay~hour), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>출발 시간이 도착 지연 시간을 정확히 나타내지는 않음</li></ul></li></ul><h3 id="회귀분석-잔차의-산점도-1"><a href="#회귀분석-잔차의-산점도-1" class="headerlink" title="회귀분석: 잔차의 산점도"></a>회귀분석: 잔차의 산점도</h3><ul><li>회귀분석의 가정과 진단<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># residual diagnostic plot </span><br><span class="line">layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page </span><br><span class="line">plot(m1)</span><br></pre></td></tr></table></figure></li><li>잔차에 대한 해석을 하고 선형회귀모형을 적욯해도 되는지 확인하기</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># pariwise plot</span><br><span class="line">pairs(car1[,1:6], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1))</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅷ-선형회귀모형과-텍스트마이닝&quot;&gt;&lt;a </summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Bigdata" scheme="https://ne-choi.github.io/categories/Study/Postech/Bigdata/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
  </entry>
  
  <entry>
    <title>빅데이터 분석과 R 프로그래밍 2: Ⅶ. R을 이용한 통계분석</title>
    <link href="https://ne-choi.github.io/2020/12/09/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A6_R%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D/"/>
    <id>https://ne-choi.github.io/2020/12/09/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A6_R%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D/</id>
    <published>2020-12-08T15:00:00.000Z</published>
    <updated>2021-02-01T00:42:19.485Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="Ⅶ-R을-이용한-통계분석"><a href="#Ⅶ-R을-이용한-통계분석" class="headerlink" title="Ⅶ. R을 이용한 통계분석"></a>Ⅶ. R을 이용한 통계분석</h1><h2 id="1-두-그룹간-평균비교-t-test"><a href="#1-두-그룹간-평균비교-t-test" class="headerlink" title="1. 두 그룹간 평균비교(t-test)"></a>1. 두 그룹간 평균비교(t-test)</h2><h3 id="단일표본의-평균검정"><a href="#단일표본의-평균검정" class="headerlink" title="단일표본의 평균검정"></a>단일표본의 평균검정</h3><ul><li><p>t.test(변수, mu=검정하고자 하는 평균값)</p><ol><li>가설 1: G3(최종성적)의 평균은 10인가?<ul><li>H<del>0</del>(null Hypothesis: 귀무가설): μ = 10<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stud&lt;-read.csv(&quot;week7_1&#x2F;stud_math.csv&quot;,stringsAsFactors &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">attach(stud)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># single t-test</span><br><span class="line">t.test(G3, mu&#x3D;10)</span><br></pre></td></tr></table></figure></li></ul></li></ol></li><li><p>결과 해석</p><ul><li>t: t검정통계량, df: 자유도, p-value<ul><li>p-value 유의 수준 0.05에서는 G3 평균이 10이라고 할 수 있는 근거가 있음</li></ul></li><li>alternative hypothesis 대립가설: 모형균은 10이 아니다</li><li>95% 신뢰구간: (9.96, 10.86), 표본평균값: 10.415</li></ul></li></ul><h3 id="두-집단의-평균검정"><a href="#두-집단의-평균검정" class="headerlink" title="두 집단의 평균검정"></a>두 집단의 평균검정</h3><ul><li><p>두 평균 차이 비교</p></li><li><p>두 집단 표본평균 비교검정: t.test(타겟변수~범주형변수, data= )</p><ol start="2"><li>가설 2: 거주지역(R, U)에 따른 G3(최종성적) 평균에 차이가 있는가?</li></ol></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># to test whether or not mean of G3 is same between Urban and Rural </span><br><span class="line">t.test(G3~address, data&#x3D;stud)</span><br><span class="line">boxplot(G3~address, boxwet&#x3D;0.5, col&#x3D;c(&quot;yellow&quot;, &quot;coral&quot;))</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>p-value=0.03으로 유의수준 0.05에서 거주지역에 따라 G3은 유의한 차이가 있다고 할 수 있음  </li></ul></li></ul><ul><li>단측검정: 기각역이 한 쪽에만 있는 경우, alternative=c(“less”) or alternative=c(“greater”)<ul><li>같으냐 다르냐가 아니라, 도심지역이 더 높으냐에 관해 test 하고 싶은 경우<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># alternative H : mu(Rural)&lt; mu(Urban)</span><br><span class="line">t.test(G3~address, data&#x3D;stud, alternative &#x3D; c(&quot;less&quot;))</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>p-value=0.018로 유의수준을 0.05로 했을 때, 성적(Rural) &lt; 성적(Urban)이라고 할 수 있음</li></ul></li></ul><ol start="3"><li>가설 3: 방과후 활동 여부(yes, no)에 따른 G3(최종성적) 평균에 차이가 있는가?<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># to test whether or not mean of G3 is equal for activities </span><br><span class="line">t.test(G3~activities, data&#x3D;stud)</span><br><span class="line">boxplot(G3~activities, boxwex &#x3D; 0.5, col &#x3D; c(&quot;blue&quot;, &quot;red&quot;))</span><br></pre></td></tr></table></figure><ul><li>결과 해석</li></ul></li></ol><ul><li>p-value=0.75로 유의수준 0.05보다 많이 큼<br>= 검정통계량 값이 기각역에 있지 않다<br>= 귀무가설(평균이 같다)을 기각할 수 없다<br>= 방과후 활동 여부는 G3에 유의한 영향이 없다는 결론</li><li>신뢰구간(-1.05, 0.79) 사이에 0값이 있으면 차이가 없음을 의미</li></ul><h3 id="두-집단의-비모수적-비교검정"><a href="#두-집단의-비모수적-비교검정" class="headerlink" title="두 집단의 비모수적 비교검정"></a>두 집단의 비모수적 비교검정</h3><ul><li>두 모집단의 비모수적 방법(Wilcoxon rank sum Test): wilcox.test(x,y)<ul><li>wilcos.test는 타겟변수가 등간척도(통증 정도, 만족도,,)일 때 사용 가능</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># wilcoxon signed-rank test</span><br><span class="line"># wilcox.test(G3, mu&#x3D;10)</span><br><span class="line">wilcox.test(G3~address)</span><br></pre></td></tr></table></figure><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(G3~internet, data&#x3D;stud) </span><br></pre></td></tr></table></figure><h2 id="2-짝을-이룬-그룹-간-비교"><a href="#2-짝을-이룬-그룹-간-비교" class="headerlink" title="2. 짝을 이룬 그룹 간 비교"></a>2. 짝을 이룬 그룹 간 비교</h2><h3 id="짝을-이룬-그룹-간-비교-paired-t-test"><a href="#짝을-이룬-그룹-간-비교-paired-t-test" class="headerlink" title="짝을 이룬 그룹 간 비교(paired t-test)"></a>짝을 이룬 그룹 간 비교(paired t-test)</h3><ul><li><p>paired t-test: t.test(before, after, mu=0, paired=T)</p><ul><li>특정 처리(treatment)의 효과를 비교분석할 때 사용</li><li>동일한 실험표본 before &amp; after 측정</li><li>예제: 혈압강하제 투약 효과, 방과후 프로그램 성과, 다이어트 프로그램 효과, 직무교육 후 생산성 향상 효과</li><li>방식: Before/After 차이를 구한 뒤, 평균과 편차를 계산해서 검정통계량을 구함</li></ul></li><li><p>paired: 양측 검정, 유의한 차이가 있는지 없는지를 검정</p></li></ul><ul><li>예제 1: 고혈압 환자 10명에게 혈압강하제를 12주 동안 투여한 후, 복용 전의 혈압과 복용 후의 혈압을 비교하였다. 새로운 혈압강하제가 효과 있다고 말할 수 있는가?</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bp&lt;-read.csv(&quot;week7_2&#x2F;bp.csv&quot;) </span><br><span class="line">attach(bp)</span><br><span class="line"></span><br><span class="line"># two-sided</span><br><span class="line">t.test(bp_pre, bp_post, mu&#x3D;0, paired&#x3D;T)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>p-value = 0.0015(매우 작음)로 유의수준 0.05보다 작기 때문에 H<del>0</del>를 기각<br>→ 투약 전과 투약 후 혈압에 유의한 차이가 있다고 볼 수 있음  </li></ul></li></ul><ul><li>단측검정 옵션 주기<ul><li>paired t-test: t.test(before, after, mu=0, <strong>alternative=”greater”</strong>, paired=T)</li><li>혈압(투약 전 - 투약 후) 차이가 0보다 큰가?<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># paired t-test (one-sided)</span><br><span class="line">t.test(bp_pre, bp_post, mu&#x3D;0, alternative&#x3D;&quot;greater&quot;, paired&#x3D;T)</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>p-value=0.0007로 유의수준 0.05보다 매우 작으므로 H<del>0</del>를 기간<br>→ 투약효과가 매우 유의하다고 볼 수 있음</li></ul></li></ul><hr><ul><li>예제 2: 비만 대상자(성인)들에게 12주 동안 극저 칼로리 식이요법(very low-calorie diet: VLCD)을 실시한 후 효과를 비교하였다. 이 프로그램에 체중감소에 효과가 있다고 할 수 있는가?</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">diet &lt;- read.csv(&quot;week7_2&#x2F;weight.csv&quot;)</span><br><span class="line">attach(diet)</span><br><span class="line"></span><br><span class="line"># paired t-test</span><br><span class="line"># 양측 검정: 극저칼로리 식이요법이 체중감량에 유의한 효과가 있는지 없는지에 관한 검정</span><br><span class="line">t.test(wt_pre, wt_post, mu&#x3D;0, paired&#x3D;T)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>p-value: 0.000001357 (0.001 = 1e-3)</li></ul></li></ul><h2 id="3-분산분석-ANOVA"><a href="#3-분산분석-ANOVA" class="headerlink" title="3. 분산분석(ANOVA)"></a>3. 분산분석(ANOVA)</h2><h3 id="분산분석의-개념"><a href="#분산분석의-개념" class="headerlink" title="분산분석의 개념"></a>분산분석의 개념</h3><ul><li>ANOVA(Analysis of Variance)<ul><li>전체 분산(variance)을 분할(분석, analysis)하여 어떤 요인(factor)의 영향이 유의(significant)한지 검정하는 방법</li></ul></li></ul><h3 id="factor가-한-개일-때"><a href="#factor가-한-개일-때" class="headerlink" title="factor가 한 개일 때"></a>factor가 한 개일 때</h3><ul><li>분산분석모형 적용<ol><li>거주 지역에 따른 학업성취도: 거주지역(factor: R/U), 학업성적(1-20)<ul><li>가설 1: 거주지역에 따라 G3에 유의한 차이가 있나?</li><li>aov(타겟변수~factor)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ANOVA</span><br><span class="line">a1 &lt;- aov(G3~address)</span><br><span class="line">summary(a1)</span><br></pre></td></tr></table></figure></li></ul></li></ol></li><li>결과 해석<ul><li>p-value=0.035로 유의수준을 0.05로 할 때, 0,05보다 작으므로 거주지역에 따른 학업성적에 유의한 차이가 있다고 할 수 있음</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tapply - give FUN value by address</span><br><span class="line">round(tapply(G3, address, mean),2)</span><br></pre></td></tr></table></figure><ol start="2"><li>통학 시간에 따른 학업성취도: 통학시간(factor: 1-4), 학업성적(1-20)<ul><li>가설 2: 통학 시간에 따라 G3에는 유의한 차이가 있나?<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">traveltime&lt;-as.factor(traveltime)</span><br><span class="line"></span><br><span class="line">a2 &lt;- aov(G3~traveltime)</span><br><span class="line">summary(a2)</span><br></pre></td></tr></table></figure></li><li>결과 해석</li></ul></li></ol><ul><li>p-value=0.139로 유의수준 0.05 하에서는 통학 시간에 따른 학업성적에는 유의한 차이가 없다고 할 수 있음</li><li>단, p-value가 0.139로 너무 크지 않기 때문에 어느정도 차이가 존재함을 알 수 있음</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tapply - give FUN value by address</span><br><span class="line">round(tapply(G3, traveltime, mean),2)</span><br></pre></td></tr></table></figure><h3 id="사후검정-post-hoc-analysis"><a href="#사후검정-post-hoc-analysis" class="headerlink" title="사후검정(post-hoc analysis)"></a>사후검정(post-hoc analysis)</h3><ul><li>사후검정: ANOVA에서 어떤 factor의 유의성이 검정되면, 그 이후에 하는 검정<ul><li>Tukey’s Honest Significant Difference Test<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># should be foctor for Tukey&#39;s Honest Significant Difference test</span><br><span class="line">TukeyHSD(a2, &quot;traveltime&quot;, ordered&#x3D;TRUE)</span><br><span class="line">plot(TukeyHSD(a2, &quot;traveltime&quot;))</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>모든 pairwise 신뢰구간에 0이 포함됨<br>→ 유의한 차이가 없음</li></ul></li></ul><ul><li>추가 예제: 분산분석<ul><li>연애경험 여부에 따른 학업성취도: 연애경험(yes, no), 학업성적(1-20)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 4. ANOVA by romantic </span><br><span class="line">a4 &lt;- aov(G3~romantic)</span><br><span class="line">summary(a4)</span><br><span class="line"></span><br><span class="line"># tapply - give FUN value by address</span><br><span class="line">round(tapply(G3,romantic, mean),2)</span><br><span class="line"></span><br><span class="line"># boxplot</span><br><span class="line">boxplot(G3~romantic, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;coral&quot;), main&#x3D;&quot;G3 by romantic&quot;)</span><br><span class="line"></span><br><span class="line"># posthoc analysis</span><br><span class="line">TukeyHSD(a4, &quot;romantic&quot;, ordered&#x3D;TRUE)</span><br><span class="line">plot(TukeyHSD(a4, &quot;romantic&quot;))</span><br></pre></td></tr></table></figure><h3 id="퀴즈-1"><a href="#퀴즈-1" class="headerlink" title="퀴즈"></a>퀴즈</h3>studytime &lt;- as.factor(studytime)</li></ul></li></ul><p>a0 &lt;- aov(G3~studytime)<br>summary(a0)</p><p>tapply(G3, studytime, mean)</p><h2 id="4-이원분산분석"><a href="#4-이원분산분석" class="headerlink" title="4. 이원분산분석"></a>4. 이원분산분석</h2><ul><li><p>이원분산분석(two-way ANOVA)</p></li><li><p>데이터: High-Density Lipoprotein (HDL) 콜레스테롤</p><ul><li>HDL(고밀도 리포 단백질): 높을수록 좋은 것으로 알려진 콜레스테롤, 40mg/dl 이상이 정상 범위  </li></ul></li><li><p>factor가 2개</p><ol><li>투약효과가 있는가? 5mg, 10mg, 위약</li><li>연령그룹(young/old)에 따른 영향이 있는가?<ul><li>가설 1. 신약 투약효과가 있나? HDL을 상승시키는 효과가 있나?</li><li>가설 2. 연령그룹에 따라 투약효과(HDL변화)에 차이가 있나?</li><li>가설 3. 신약의 투약과 연령그룹 간 상호작용 효과가 있나?  </li></ul></li></ol></li><li><p>이원분산분석: aov(타겟변수~factor1 + factor2)  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dat &lt;- read.csv(&quot;week7_4&#x2F;chol_ex.csv&quot;)</span><br><span class="line">attach(dat)</span><br><span class="line"></span><br><span class="line"># two-way ANOVA</span><br><span class="line">a6 &lt;- aov(value ~ drug + age)</span><br><span class="line">summary(a6)</span><br></pre></td></tr></table></figure></li><li><p>결과 해석</p><ul><li>drug effect: p-value~0이므로 HDL값에 통계적으로 유의한 차이가 있음</li><li>age: p-value=0.19로 유의수준 0.05에서 유의한 차이는 없음  </li></ul></li></ul><ul><li>aov(타겟변수~factor1 + factor2 + 상호작용)<ul><li>두 개의 factor 간 상호작용의 유의성을 검정하기 위한 분석<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># two-way ANOVA with interaction</span><br><span class="line">a7 &lt;- aov(value ~ drug + age+ drug*age)</span><br><span class="line">summary(a7)</span><br></pre></td></tr></table></figure></li></ul></li><li>결과 해석<ul><li>drug와 age 그룹 간 상호작용: p-value=0.286으로 유의수준 0.05에서 유의한 차이는 없음</li></ul></li></ul><h3 id="두-요인의-상자그림"><a href="#두-요인의-상자그림" class="headerlink" title="두 요인의 상자그림"></a>두 요인의 상자그림</h3><ul><li>투약용량과 연령그룹에 따른 상자그림<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># two-way ANOVA</span><br><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">boxplot(value ~ drug, boxwex &#x3D; 0.7, main&#x3D;&quot;HDL by drug dose&quot;, col &#x3D; c(&quot;yellow&quot;,&quot;orange&quot;, &quot;green&quot;))</span><br><span class="line">boxplot(value ~ age, boxwex &#x3D; 0.5, main&#x3D;&quot;HDL by Age&quot;, col &#x3D; c(&quot;blue&quot;, &quot;coral&quot;))</span><br></pre></td></tr></table></figure></li></ul><ol><li>drug effect: 10mg인 경우, HDL 상승효과가 가장 높음<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tapply - give FUN value by drug</span><br><span class="line">round(tapply(value, drug, mean),2)</span><br></pre></td></tr></table></figure></li><li>age: young 그룹(18-40)의 HDL 상승효과가 더 높음<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># tapply - give FUN value by age</span><br><span class="line">round(tapply(value, age, mean),2)</span><br></pre></td></tr></table></figure><h3 id="상호작용-그래프"><a href="#상호작용-그래프" class="headerlink" title="상호작용 그래프"></a>상호작용 그래프</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># interaction plot</span><br><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">interaction.plot(drug, age, value)</span><br><span class="line">interaction.plot(age, drug, value)</span><br></pre></td></tr></table></figure></li></ol><ul><li>결과 해석<ul><li>투약용량 10mg에서 young 그룹 상승효과가 old 그룹보다 훨씬 높음</li><li>5mg에서와 placebo에서는 연령그룹 차이가 거의 없음</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Create an interaction plot for the HDL data</span><br><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">interaction.plot(dat$drug, dat$age, dat$value)</span><br><span class="line">interaction.plot(dat$age, dat$drug, dat$value)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># two-way ANOVA model with interaction</span><br><span class="line">results &#x3D; lm(value ~ drug + age + drug*age, data&#x3D;dat)</span><br><span class="line">anova(results)</span><br></pre></td></tr></table></figure><h3 id="퀴즈-2"><a href="#퀴즈-2" class="headerlink" title="퀴즈"></a>퀴즈</h3><p>a00 &lt;- aov(G3<del>studytime+sex)<br>summary(a00)<br>boxplot(G3</del>sex, boxwex = 0.5, col = c(“yellow”, “coral”))</p>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅶ-R을-이용한-통계분석&quot;&gt;&lt;a hre</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Bigdata" scheme="https://ne-choi.github.io/categories/Study/Postech/Bigdata/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
  </entry>
  
  <entry>
    <title>빅데이터 분석과 R 프로그래밍 2: Ⅵ. 데이터 탐색</title>
    <link href="https://ne-choi.github.io/2020/12/08/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A5_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89/"/>
    <id>https://ne-choi.github.io/2020/12/08/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A5_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89/</id>
    <published>2020-12-07T15:00:00.000Z</published>
    <updated>2021-02-01T00:42:10.517Z</updated>
    
    <content type="html"><![CDATA[<ul><li>POSTECH에서 제공하는 <a href="https://pabi.smartlearn.io/">MOOC</a> 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  </li></ul><h1 id="Ⅵ-데이터-탐색"><a href="#Ⅵ-데이터-탐색" class="headerlink" title="Ⅵ. 데이터 탐색"></a>Ⅵ. 데이터 탐색</h1><h2 id="1-데이터-다루기"><a href="#1-데이터-다루기" class="headerlink" title="1. 데이터 다루기"></a>1. 데이터 다루기</h2><ul><li>데이터 결합, 분할, 정렬</li></ul><h3 id="데이터-결합-merge"><a href="#데이터-결합-merge" class="headerlink" title="데이터 결합: merge"></a>데이터 결합: merge</h3><ul><li>merge(data1, data2, by=”ID”)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># practice data</span><br><span class="line">dat1&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data1.csv&quot;)</span><br><span class="line">dat2&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data2.csv&quot;)</span><br><span class="line"></span><br><span class="line"># data merging</span><br><span class="line">dat12&lt;-merge(dat1, dat2, by&#x3D;&quot;ID&quot;)</span><br></pre></td></tr></table></figure><h3 id="데이터-결합-rbind"><a href="#데이터-결합-rbind" class="headerlink" title="데이터 결합: rbind"></a>데이터 결합: rbind</h3></li><li>두 개 데이터를 행(row)로 결합<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># add more data (combine in a row)</span><br><span class="line">dat3&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data3.csv&quot;)</span><br><span class="line">dat123&lt;-rbind(dat12, dat3)</span><br><span class="line"></span><br><span class="line">dat123</span><br></pre></td></tr></table></figure><h3 id="데이터-정렬-order"><a href="#데이터-정렬-order" class="headerlink" title="데이터 정렬: order"></a>데이터 정렬: order</h3></li><li>데이터명[order(변수1, 변수2)]</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># data sorting</span><br><span class="line">dats1&lt;-dat12[order(dat12$age),]</span><br><span class="line"></span><br><span class="line">dats1</span><br><span class="line"></span><br><span class="line">dats2&lt;-dat12[order(dat12$gender, dat12$age), ]</span><br><span class="line"></span><br><span class="line">dats2</span><br></pre></td></tr></table></figure><h3 id="데이터-추출-subset"><a href="#데이터-추출-subset" class="headerlink" title="데이터 추출: subset"></a>데이터 추출: subset</h3><ul><li>subset(데이터명, 조건1 &amp; 조건2)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># data subset (selecting data)</span><br><span class="line">#newdat&lt;-dat12[which(dat12$gender&#x3D;&#x3D;&quot;F&quot; &amp; dat12$age&gt;15)]</span><br><span class="line">newdat&lt;-subset(dat12, dat12$gender&#x3D;&#x3D;&quot;F&quot; &amp; dat12$age&gt;15)</span><br><span class="line"></span><br><span class="line">newdat</span><br></pre></td></tr></table></figure><h3 id="일부-변수-제거"><a href="#일부-변수-제거" class="headerlink" title="일부 변수 제거"></a>일부 변수 제거</h3><ul><li>데이터이름[!names(데이터) %in% c(“변수1”, “변수2)]</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># excluding variables</span><br><span class="line">exdat&lt;-dat12[!names(dat12) %in% c(&quot;age&quot;,&quot;gender&quot;)]</span><br><span class="line"></span><br><span class="line">exdat</span><br></pre></td></tr></table></figure><h3 id="퀴즈"><a href="#퀴즈" class="headerlink" title="퀴즈"></a>퀴즈</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new &lt;- subset(dat123, dat123$hourwk &gt;&#x3D; 10 &amp; dat123$alcohol&#x3D;&#x3D; &quot;yes&quot;)</span><br><span class="line"></span><br><span class="line">new</span><br></pre></td></tr></table></figure><h2 id="2-데이터-기술통계치-요약"><a href="#2-데이터-기술통계치-요약" class="headerlink" title="2. 데이터 기술통계치 요약"></a>2. 데이터 기술통계치 요약</h2><ul><li>데이터: 학업성취도(포르투칼 고등학생 수학점수)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stud&lt;-read.csv(&quot;week6_2&#x2F;stud_math.csv&quot;)</span><br><span class="line"></span><br><span class="line">head(stud)</span><br><span class="line">dim(stud)</span><br><span class="line">str(stud)</span><br><span class="line"></span><br><span class="line">attach(stud)</span><br></pre></td></tr></table></figure><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><ul><li>summary(데이터명): 변수별로 요약통계량 제공<ul><li>문자변수에는 빈도를 주고, 숫자변수에는 최솟값, 25%, 중위값, 75%, 최댓값 제공</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 1-1 Numeriacl analytics</span><br><span class="line">summary(stud)</span><br><span class="line">mean(G3) # 평균</span><br><span class="line">sd(G3) # 표준편차(분산의 제곱근)</span><br><span class="line">var(G3) # 분산</span><br></pre></td></tr></table></figure><h3 id="vars"><a href="#vars" class="headerlink" title="vars"></a>vars</h3><ul><li>특정 변수에 관한 요약통계량: var &lt;- c(“변수1”, “변수2”, “변수3”)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># creating interested variable list</span><br><span class="line">vars&lt;-c(&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;)</span><br><span class="line">head(stud[vars])</span><br><span class="line">summary(stud[vars])</span><br></pre></td></tr></table></figure><h3 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h3><ul><li>summary보다 더 구체적인 요약통계량 얻을 수 있음</li><li>psych 패키지 필요<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># install.packages(&quot;psych&quot;)</span><br><span class="line">library(psych)</span><br><span class="line"></span><br><span class="line"># require &quot;psych&quot; for &quot;describe&quot; function</span><br><span class="line">describe(stud[vars])</span><br></pre></td></tr></table></figure><h3 id="table"><a href="#table" class="headerlink" title="table"></a>table</h3></li><li>범주형 변수 요약: table(변수이름)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># categorical data</span><br><span class="line">table(health)</span><br><span class="line"></span><br><span class="line">health_freq&lt;-table(health)</span><br><span class="line">names(health_freq) &lt;- c (&quot;very bad&quot;, &quot;bad&quot;, &quot;average&quot;, &quot;good&quot;,</span><br><span class="line">                      &quot;very good&quot;)</span><br><span class="line">barplot(health_freq, col&#x3D;3)</span><br></pre></td></tr></table></figure><ul><li>2*2 분할표<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 2*2 contingency table</span><br><span class="line">table(health,studytime)</span><br></pre></td></tr></table></figure></li></ul><h2 id="3-그래프를-이용한-데이터-탐색"><a href="#3-그래프를-이용한-데이터-탐색" class="headerlink" title="3. 그래프를 이용한 데이터 탐색"></a>3. 그래프를 이용한 데이터 탐색</h2><h3 id="히스토그램"><a href="#히스토그램" class="headerlink" title="히스토그램"></a>히스토그램</h3><ul><li>1학년, 2학년, 3학년 성적의 분포<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 1. histogram with color and title, legend</span><br><span class="line">par(mfrow&#x3D;c(2,2))</span><br><span class="line">hist(G1, breaks &#x3D; 10, col &#x3D; &quot;lightblue&quot;, main&#x3D;&quot;Histogram of Grade 1&quot; )</span><br><span class="line">hist(G2, breaks &#x3D; 10, col &#x3D; &quot;green&quot;, main&#x3D;&quot;Histogram of Grade 2&quot; )</span><br><span class="line">hist(G3, breaks &#x3D; 10, col &#x3D; &quot;coral&quot;, main&#x3D;&quot;Histogram of Grade 3&quot; )</span><br></pre></td></tr></table></figure><h3 id="상자그림"><a href="#상자그림" class="headerlink" title="상자그림"></a>상자그림</h3></li><li>거주 지역에 따른 G3, 통학 시간에 따른 G3<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">boxplot(G3~address, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;coral&quot;), main&#x3D;&quot;G3 by (Urban, Rural)&quot;)</span><br><span class="line">boxplot(G3~traveltime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;red&quot;,&quot;orange&quot;,&quot;yellow&quot;,&quot;green&quot;), main&#x3D;&quot;G3 by traveltime&quot;)</span><br></pre></td></tr></table></figure></li><li>결과 해석<ol><li>도심지역 학생 성적이 외곽지역 학생보다 높음</li><li>통학시간이 짧은(15분 이내) 학생 성적이 높음  </li></ol></li></ul><ul><li>자유시간에 따른 G3, 공부시간에 따른 G3<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">par(mfrow&#x3D;c(1,2))</span><br><span class="line">boxplot(G3~freetime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;grey&quot;, &quot;red&quot;))</span><br><span class="line">boxplot(G3~studytime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;blue&quot;, &quot;grey&quot;, &quot;red&quot;))</span><br></pre></td></tr></table></figure></li><li>결과 해석<ol><li>방과후 자유시간에 따른 G3 차이: 적은편이라고 응답한 학생 성적이 다소 높음</li><li>주중 공부 시간이 5시간 이상인 학생 성적이 높은 편</li></ol></li></ul><h3 id="lattice-패키지"><a href="#lattice-패키지" class="headerlink" title="lattice 패키지"></a>lattice 패키지</h3><ul><li>통학시간과 최종성적(G3)의 멀티패널 그림, 성별</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(lattice)</span><br><span class="line">xyplot(G3~traveltime | sex, data&#x3D;stud, pch&#x3D;16, main&#x3D;&quot;G3 ~ traveltime | sex&quot;)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>대부분 학생은 30분 이내 통학거리</li><li>통학거리가 짧은 학생 성적 평균이 다소 높게 나타남</li><li>통학거리 1시간 이상은 표본이 상대적으로 적음</li><li>추가: 0점인 데이터가 확인되어 점검이 필요함</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># find a data G3 &#x3D; 0</span><br><span class="line">s1 &lt;- subset(stud, G3&#x3D;&#x3D;0)</span><br><span class="line">s1</span><br></pre></td></tr></table></figure><h3 id="산점도"><a href="#산점도" class="headerlink" title="산점도"></a>산점도</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">library(ggplot2)</span><br><span class="line">ggplot(stud, aes(x&#x3D;G1, y&#x3D;G3, color&#x3D;sex, shape&#x3D;sex)) +</span><br><span class="line">  geom_point(size&#x3D;2)</span><br></pre></td></tr></table></figure><h3 id="막대그림"><a href="#막대그림" class="headerlink" title="막대그림"></a>막대그림</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bar chart for romantic by sex</span><br><span class="line">ggplot(data&#x3D;stud, aes(factor(romantic)))+geom_bar(aes(fill&#x3D;factor(sex)), width&#x3D;.4, colour&#x3D;&quot;black&quot;)+ ggtitle(&quot;Romantic by sex&quot;)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>연애 경험 있는 경우, 여학생 비율이 높음</li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># bar chart for internet use by (Urban, Rural)</span><br><span class="line">ggplot(data&#x3D;stud, aes(factor(internet)))+geom_bar(aes(fill&#x3D;factor(address)), width&#x3D;.4, colour&#x3D;&quot;black&quot;)+ggtitle(&quot;Internet use by (Urban, Rural)&quot;)</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>인터넷 사용자 중에는 도심지역에 사는 경우가 훨씬 많음</li></ul></li></ul><h3 id="pariwise-plot"><a href="#pariwise-plot" class="headerlink" title="pariwise plot"></a>pariwise plot</h3><ul><li>pairwise scatterplot: pairs(변수리스트)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># new variable lists</span><br><span class="line">vars1&lt;-c(&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;)</span><br><span class="line"># pariwise plot</span><br><span class="line">pairs (stud[vars1], main &#x3D; &quot;Student Math Data&quot;,</span><br><span class="line">       pch &#x3D; 21,bg &#x3D; c (&quot;red&quot;,&quot;green3&quot;)[unclass(stud$sex)])</span><br></pre></td></tr></table></figure><ul><li>결과 해석<ul><li>G1, G2, G3 상관성은 매우 높음</li><li>성별 차이는 없음</li></ul></li></ul><h2 id="4-데이터의-정규성검정과-신뢰구간"><a href="#4-데이터의-정규성검정과-신뢰구간" class="headerlink" title="4. 데이터의 정규성검정과 신뢰구간"></a>4. 데이터의 정규성검정과 신뢰구간</h2><h3 id="데이터의-정규성검정"><a href="#데이터의-정규성검정" class="headerlink" title="데이터의 정규성검정"></a>데이터의 정규성검정</h3><ul><li>정규확률도(Normal Q-Q plot): 데이터가 정규분포하는가?</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Testing for normality</span><br><span class="line"># multiple plot (2 by 2)</span><br><span class="line">par(mfrow&#x3D;c(2,2))</span><br><span class="line">#Quantile plot</span><br><span class="line">qqnorm(G1)</span><br><span class="line">qqline(G1, col &#x3D; 2, cex&#x3D;7)</span><br><span class="line"></span><br><span class="line">qqnorm(G2)</span><br><span class="line">qqline(G2, col &#x3D; 2, cex&#x3D;7)</span><br><span class="line"></span><br><span class="line">qqnorm(G3)</span><br><span class="line">qqline(G3, col &#x3D; 2, cex&#x3D;7)</span><br></pre></td></tr></table></figure><h3 id="정규분포"><a href="#정규분포" class="headerlink" title="정규분포"></a>정규분포</h3><ul><li><p>정규분포(Normal distribution)</p></li><li><p>정규분포 적합성검정: 데이터가 정규분포하는지에 대한 검정</p><ol><li>Shapiro-Wilks 검정<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#Shapiro-Wilks test</span><br><span class="line">shapiro.test(G3)</span><br></pre></td></tr></table></figure></li></ol></li><li><p>결과 해석</p><ul><li>p-value가 0에 가까워서 정규분포한다고 볼 수 없음</li></ul><ol start="2"><li>Anderson-Darling 검정(패키지 설치 필요)<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#Anderson-Darling test require installing package &quot;nortest&quot;</span><br><span class="line"># install.packages(&#39;nortest&#39;)</span><br><span class="line">library(nortest)</span><br><span class="line">ad.test(G3)</span><br></pre></td></tr></table></figure></li></ol></li><li><p>결과 해석</p><ul><li>p-value가 0에 가까워서 정규분포한다고 볼 수 없음  </li></ul></li></ul><h3 id="확률분포함수"><a href="#확률분포함수" class="headerlink" title="확률분포함수"></a>확률분포함수</h3><ul><li>사용 함수</li></ul><table><thead><tr><th>분포함수</th><th>설명</th></tr></thead><tbody><tr><td>binom(x)</td><td>이항분포 rbinom(5, size=100, prob-.2)</td></tr><tr><td>exp(x)</td><td>지수분포</td></tr><tr><td>gamma(X)</td><td>감마분포 rgamma(5, shape=3, rate=2)</td></tr><tr><td>norm(x)</td><td>정규분포 rnorm(50, mean=10, sd=5)</td></tr><tr><td>pois(x)</td><td>포아송분포 rpois(n, lambda)</td></tr><tr><td>unif(x)</td><td>균일분포 runif(30)</td></tr></tbody></table><ul><li>p: 누적함수 / d: 확률밀도함수 / q: quantile 함수 / r: 랜덤넘버 생성</li></ul><ul><li>확률분포함수로부터 데이터 생성<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Simulation examples</span><br><span class="line">runif(5,min&#x3D;1,max&#x3D;5)</span><br><span class="line">rnorm(5,mean&#x3D;5,sd&#x3D;1)</span><br><span class="line">rgamma(5,shape&#x3D;3,rate&#x3D;2)</span><br><span class="line">rbinom(5,size&#x3D;100,prob&#x3D;.2)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># from normal distribution</span><br><span class="line">x&lt;-rnorm(1000)</span><br><span class="line">plot(density(x),xlim&#x3D;c(-5,10))</span><br></pre></td></tr></table></figure></li><li>데이터 생성(정규분포(평균=500, 편차=100)에서 100개 데이터 생성)</li></ul><p>set.seed(5)<br>mean(rnorm(1000))</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># confidence interval of normal distribution</span><br><span class="line">nreps &lt;- 100</span><br><span class="line">ll &lt;- numeric(nreps)</span><br><span class="line">ul &lt;- numeric(nreps)</span><br><span class="line">n &lt;- 100</span><br><span class="line">mu &lt;- 500</span><br><span class="line">sigma &lt;- 100</span><br><span class="line">for(i in 1:nreps) &#123;</span><br><span class="line">  set.seed(i)</span><br><span class="line">  x &lt;- rnorm(n, mu, sigma)</span><br><span class="line">  ll[i] &lt;- mean(x) - qnorm(0.975)*sqrt(sigma^2&#x2F;n)</span><br><span class="line">  ul[i] &lt;- mean(x) + qnorm(0.975)*sqrt(sigma^2&#x2F;n)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Draw 95% confidence interval(신뢰 구간)</span><br><span class="line"># 95%: 100개 중 5개는 모평균을 포함하고 있지 않다</span><br><span class="line"># 95%: 100개 중, 5개는 오류가 있을 것이다</span><br><span class="line"></span><br><span class="line">par(mfrow&#x3D;c(1,1))</span><br><span class="line">plot(1:100, ylim&#x3D;c(min(ll), max(ul)),</span><br><span class="line">     ylab&#x3D;&quot;95% Confidence Interval&quot;, xlab&#x3D;&quot;iterations&quot;)</span><br><span class="line"></span><br><span class="line">for(i in 1:100) lines(c(i, i), c(ll[i], ul[i]))</span><br><span class="line">abline(h&#x3D;mu, col&#x3D;&quot;red&quot;, lty&#x3D;2, lwd&#x3D;3)</span><br></pre></td></tr></table></figure><h3 id="신뢰구간의-의미"><a href="#신뢰구간의-의미" class="headerlink" title="신뢰구간의 의미"></a>신뢰구간의 의미</h3><ul><li>신뢰수준, 표본오차<ul><li>전국 유권자 1,500명 조사 결과, A 후보 지지율은 45%이며, 95% 신뢰수준에서 오차한계는 3.5%이다.<br>→ 지지도에 대한 95% 신뢰구간<br>: 표본 지지율 +- 오차한계 (41.5%, 48.5%)</li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;ul&gt;
&lt;li&gt;POSTECH에서 제공하는 &lt;a href=&quot;https://pabi.smartlearn.io/&quot;&gt;MOOC&lt;/a&gt; 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다.  &lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;Ⅵ-데이터-탐색&quot;&gt;&lt;a href=&quot;#Ⅵ</summary>
      
    
    
    
    <category term="Study" scheme="https://ne-choi.github.io/categories/Study/"/>
    
    <category term="Postech" scheme="https://ne-choi.github.io/categories/Study/Postech/"/>
    
    <category term="Bigdata" scheme="https://ne-choi.github.io/categories/Study/Postech/Bigdata/"/>
    
    
    <category term="Postech" scheme="https://ne-choi.github.io/tags/Postech/"/>
    
  </entry>
  
</feed>
