{"meta":{"title":"Better Thinking","subtitle":"this is naeun blog","description":"The load to be a data scientist","author":"Naeun Choi","url":"https://ne-choi.github.io","root":"/"},"pages":[{"title":"Naeun is !","date":"2020-10-29T06:57:43.000Z","updated":"2021-01-20T04:35:22.097Z","comments":true,"path":"about/index.html","permalink":"https://ne-choi.github.io/about/index.html","excerpt":"","text":"BBA from Sookmyung Women’s University Double majored in Business Administration and Korean Language &amp; Literature Learning R, SQL, and Python Interested in NLP 디지털 광고 업계에서 일하면서, 고객 데이터 수치를 깊이있게 파악하고 싶어 데이터 공부를 시작했습니다. 결국 퇴사를 선택했고 20년 8월부터 데이터 분석 공부를 하고 있습니다. R과 Python이 주언어이며 데이터를 통해 인사이트를 얻는 일에 흥미가 있습니다. 현재는 자연어 처리, 그 중에서 텍스트 분류 감정분석 영역을 중점적으로 연구하고 있습니다. Ⅰ. 기본 정보 이름: 최나은 이메일: &#110;&#97;&#101;&#117;&#x6e;&#x31;&#50;&#49;&#x38;&#64;&#103;&#109;&#97;&#105;&#108;&#x2e;&#x63;&#111;&#109; 블로그: ne-choi.github.io Ⅱ. 학력 학교 2021.02 강림직업전문학교 빅데이터반 수료 예정 2018.08 숙명여자대학교 경영/한국어문(복수전공) 졸업 기타 교육 POSTEC 청년 AI · Big Data 아카데미(MOOC) 기간: 2020.11.02 - 2021.01.15 교육 시간: 103시간 포항공과대학교 총장 수료증 발급 수료 내역 → 자세히 보기 강의명|학습 시간|수료 일자빅데이터분석과 R프로그래밍 Ⅰ|12시간|2020.11.27빅데이터분석과 R프로그래밍 Ⅱ|12시간|2020.12.07데이터사이언스를 위한 통계학입문 Ⅰ|12시간|2020.11.27데이터사이언스를 위한 통계학입문 Ⅱ|12시간|2020.12.22머신러닝기법과 R프로그래밍 Ⅰ|10시간|2020.12.09머신러닝기법과 R프로그래밍 Ⅱ|10시간|2020.12.21Python 프로그래밍 Ⅰ|12시간|2020.12.14Python 프로그래밍 Ⅱ|12시간|2020.12.23Computational Thinking Ⅰ|4시간|2021.01.09Computational Thinking Ⅱ|4시간|2021.01.13특강_AI 입문|2시간|2020.12.10특강_Computer Vision 입문|1시간|2021.01.10 Ⅲ. 증명 어학 능력 TOEIC: 905점 / 2019.03.16 / ETS OPIc: IH / 2020.03.28 / ACTFL 자격증 ADsP 데이터분석 준전문가 / 2020.12.22 / 한국데이터산업진흥원 ERP 생산 정보관리사 2급 / 2020.10.13 / 한국생산성본부 ERP 물류 정보관리사 2급 / 2020.10.13 / 한국생산성본부 GTQ 그래픽기술자격 1급 / 2019.04.12 / 한국생산성본부 컴퓨터활용능력 2급 / 2017.12.08 / 대한상공회의소 수상 내역 소비트렌드 코리아 2020: KDX 유통소비데이터분석&amp;시각화경진대회 우수상 / 2020.11.18 / KDX 한국데이터거래소, MBN Ⅳ. 경력 ㈜메조미디어 (CJ그룹 계열사) 기간: 2019.07 - 2020.07 (13개월) 부서: 디지털광고 1국2팀 직위: 정규직 사원 업무: 미디어 플래너 세부 내용: 광고 운영 및 성과 분석, 고객 타겟팅 데이터 관리, 미디어 믹스 작성, 광고주/대행사 관리, 비딩 참여, 지역 광고주 청약 담당 사용 툴: Google Analytics, Facebook 광고관리자, 네이버 NOSP 광고 시스템 등 교육: Google Analytics Academy, Facebook blueprint 수료, 다양한 광고 매체 설명회 참석 성과: 독립적 업무 수행, 월 10개 이상 캠페인 운영, 100개 이상 지역 광고주 청약 관리, 월 2회 이상 광고주/대행사 제안, 미팅 조율 및 참여 보임안경원 기간: 2018.02 - 2018.10 (9개월) 직위: 아르바이트 업무: 온라인 쇼핑몰 관리 세부 내용: 안경 모델, 상품 발주 및 업로드, 네이버 광고 관리, 고객 타겟팅, 네이버 애널리틱스 분석 사용 툴: 네이버 광고관리시스템 성과: 오픈 6개월 만에 온라인 쇼핑몰 매출액 800만원 달성 고려 컴퓨터학원 기간: 2016.12 - 2017.10 (11개월) 직위: 아르바이트 업무: 수능반, 내신반 국어 강사 세부 내용: 고3 수능 국어 강의, 고1-2 내신 국어 강의, 글쓰기 교재 제작, 방학시즌 글쓰기 특강 진행 Ⅴ. 경험 경영학부 학생회 기간: 2014.03 - 2015.12 (21개월) 업무: MT 진행, 사물함 철거 작업, 과방 지킴이, 전공 설명회 개최, 축제 주점 운영, 학부 간식 배부, 학생지도의 날 행사 진행 기여: 스탭 업무 지원, 참석률 100% 학생지원팀 소속 리더십그룹 블룸 기간: 2015.09 - 2017.06 (21개월) 2016.09 - 2016.06 그룹원 2016.06 - 2017.06 기획부장 업무: 입학식 도우미, 평생지도 교수제 홍보&amp;관리, 리더십그룹 자체 행사 진행 기여: 각종 행사 기획안 작성, 결과 보고서 작성, 입학식 명찰과 팻말 제작 국제협력팀소속 봉사동아리 GPM 기간: 2015.09 - 2016.06 (9개월) 업무: 외국인 유학생 대상 한국어와 전공 공부 교육, 학교 생활 멘토링 기여: 1:1 멘토, 행사 기획 아이디어 제시(유학생과 함께 하는 뷰티 클래스, 체육대회 등) 경상대학(경영/경제) 회장단 기간: 2016.01 - 2016.12 (12개월) 업무: 입학식, OT, 시험 기간 간식 사업, 축제 기획, 학생지도의 날 체육대회 개최, 졸업식 등 행사 자율적으로 준비 기여: 단과대학 모든 행사 진행 숙명여자대학교 비상대책위원회 기간: 2016.01 - 2016.12 (12개월) 업무: 총학생회장단 부재로 직무 대리 학교 축제 준비- 기획부 선거 준비- 중앙선거관리위원장 기여: 총학 부재 1년 동안 교내외 행사 진행, 선거 기관과 보궐선거 준비 및 진행 숙명여자대학교 어학당 한국어 도우미 기간: 2016.06 - 2016.08 (3개월) 업무: 어학당 멘토 활동, 한국어 공부&amp;중간고사 준비 보조 기여: 멘티 2명 모두 서울 소재 대학교 진학 뷰티 브랜드 서포터즈 스킨푸드 푸드 마니아 16기 (17.03 - 17.06, 3개월) 클리오 클러버 6기 (17.07 - 17.12, 5개월) 더블유랩 더블유 프렌즈 10기 (18.04 - 18.08, 4개월) 오즈비 서포터즈 1기 (18.06 - 18.09, 3개월) 닥터슈라클 클레버리더 2기 (18.09 - 19.03, 6개월)→ 내용: 온/오프라인 활동을 통한 브랜드 상품 제안→ 성과: 블로그 일 평균 방문자 2,000명 달성 Ⅵ. 포트폴리오1. 프로젝트 KDX 소비트렌드 코리아 2020 → 자세히 보기 작업툴: RStudio 인원: 3명 기간: 2020.10.12 - 2020.10.24 내용 주어진 데이터와 외부 데이터를 사용하여 한국 소비 트렌드를 분석하고 인사이트 도출하기 (주제 선정 자유) 선정 주제 “코로나19 발생 이후 색조 및 기초 화장품 수요 비교분석” 코로나19로 인한 마스크 일상화/의무화 시대, 화장품 업계 매출 추이를 살펴보고 발전 방향 제언하기 기여: EDA 및 매출액 시각화코드 작성, 결과 보고 ppt 전체 제작 결과: 3등(우수상) → 매경 기사 보기 소설 작가 분류 AI 경진대회 → 자세히 보기 작업툴: python 인원: 2명 기간: 2020.12.03 - 2020.12.04 내용 문체 분석 알고리즘 개발 소설 속 문장뭉치 분석을 통한 저자 예측 작가의 글을 분석하여 특징 도출 취향 추천 시스템 활용 / 대필, 유사작 탐지 기여: 텍스트 분석 전처리, 모델링 코드 작성 결과: 158/287 (전체 참가 팀: 658팀) 2020 Kaggle Machine Learning &amp; Data Science Survey → 자세히 보기 → 캐글에서 보기 언어 및 작업툴: python, jupyter notebook 인원: 1명 기간: 2020.12.30 - 2021.01.06 내용 캐글에서 진행하는 연례 데이터 과학 설문 조사 데이터에서 인사이트 도출하기 주제 선정 자유 선정 주제 “The future of ML (Americans vs Indians)” 기여: 데이터 전처리, 시각화 및 분석 Ⅵ. NLP1. NLP 강의안 제작2. NLP 논문 리뷰"}],"posts":[{"title":"Python 프로그래밍 2: Ⅶ. 데이터 구조","slug":"Study/Postech/Python/Ⅷ. 파이썬과 인공지능","date":"2021-01-22T00:00:00.000Z","updated":"2021-02-05T13:35:56.634Z","comments":true,"path":"/2021/01/22/Study/Postech/Python/Ⅷ. 파이썬과 인공지능/","link":"","permalink":"https://ne-choi.github.io/2021/01/22/Study/Postech/Python/%E2%85%A7.%20%ED%8C%8C%EC%9D%B4%EC%8D%AC%EA%B3%BC%20%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, Python 프로그래밍 Ⅱ 과정입니다. Ⅷ. 파이썬과 인공지능1. 파이썬과 인공지능Outline 문제 해결(Problem Solving) 컴퓨팅 사고력(Computational Thinking) 4차산업혁명(Industry 4.0) 인공지능, 머신러닝, 딥러닝(AI, ML, DL) 인공지능 프로젝트 사례(AI Project) 문제 해결문제 입력 → 문제 해결 방법과 절차 (알고리즘; Algorithm) → 해결 출력 Computational Thinking 컴퓨터 과학자처럼 생각하기 컴퓨터 과학의 이론, 기술, 도구를 활용하여 현실의 복잡한 문제를 해결하는 사고 방식 컴퓨터공학 사고력 주어진 일상 생활 문제를 분석, 이해하고 문제 해결 방법을 컴퓨터 공학 원리로 알고리즘을 도출하여 컴퓨터가 이해하는 프로그래밍 언어로 제시하는 사고 능력 4차 산업혁명(Industry 4.0) 인공지능 로봇기술 생명과학 기술이 주도하는 차세대 산업기술 지능정보기술: 지능(AI SW) + 정보(빅데이터, IoT, 클라우드) 산업혁명 역사 1차: 증기기관 → 기계적 형명 2차: 전기 → 대량 생산의 시작 (인간 육체 노동을 대체) 3차: 컴퓨터를 통한 제품 생산과 유통 자동화 (인간 지능 노동을 보조) 4차: IoT &amp; AI → 제품 생산 지능화, 고객 맞춤형 생산 (인간 지능 노동을 대체) 4차산업혁명 산업 + 사회 혁명 3차 vs 4차산업혁명 다양한 분야 융합 이미 존재하는 기술의 상호연결: 에어비앤비, 우버 제조업과 인공지능 융합하여 새로운 가치 창출 중앙집권화에서 분산화로: 블록체인 기술 인공지능 인공지능(Artificial Intelligence) 컴퓨터가 사람처럼 생각하고 판단하게 만드는 기술 머신러닝(Machine Learning) 인공지능의 한 분야 인간의 학습능력과 같은 기능을 컴퓨터에 부여하기 위한 기술 딥러닝(Deep Learning) 인공 신경망을 기반으로 한 머신러닝 방법론 중 하나 빅데이터 기반으로 스스로 학습하여 판단하는 기술 2. 인공지능 프로젝트 사례Application of AI 사물인식 추천 프로그램 얼굴인식 추천 프로그램 기사 요약 프로그램 인공지능 챗봇 프로그램 자율주행 RC Car 사물 인식 식자재 사물 인식을 통한 맞춤 요리 추천 모듈 구현 냉장고에 있는 음식을 확인 후, 레시피를 추천하는 알고리즘 얼굴 인식 얼굴을 인식하여 구글 로그인 가능 딥러닝과 컴퓨터 비전 Object Detection 이미지/동영상에서 사람, 동물, 차량 등 의미 있는 객체 종류와 그 위치(bounding box)를 정확하게 찾기 위한 computer vision 기술 영상에서 관심 대상을 인식하기 위해 일반적으로 검출 대상에 관한 후보 영역을 찾고, 후보 영역에 대한 객체 종류와 위치를 학습된 모델을 통해 예측 영상 및 영상 내 객체 종류(class)와 객체 위치 정보 필요 얼굴, 도로 상 보행자 및 차량 인식 등에 딥러닝 기반 객체 탐지 기술이 주로 이용됨 자연어 처리 신문기사 내용을 요약해주는 서비스 브라우저에서 파이썬을 이용하여 기사를 크롤링 데이터 베이스에 넣고 txt 저장 알고리즘을 통해 기사를 요약하고 저장 챗봇 사용자의 발화를 듣고 요청 내용을 이해하여 답변하는 서비스 자율주행 자동차 RC Car 부품에 라즈베리파이라는 작은 부품이 있음 영상 정보를 전달하면, 영상 정보를 딥러닝으로 분석하여 길인지 아닌지 분류 분류된 내용 기반으로 전송하면 차가 움직임","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"}],"author":"ne-choi"},{"title":"Python 프로그래밍 2: Ⅶ. 데이터 구조","slug":"Study/Postech/Python/Ⅶ. 데이터 구조","date":"2021-01-21T00:00:00.000Z","updated":"2021-02-05T13:35:29.742Z","comments":true,"path":"/2021/01/21/Study/Postech/Python/Ⅶ. 데이터 구조/","link":"","permalink":"https://ne-choi.github.io/2021/01/21/Study/Postech/Python/%E2%85%A6.%20%EB%8D%B0%EC%9D%B4%ED%84%B0%20%EA%B5%AC%EC%A1%B0/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, Python 프로그래밍 Ⅱ 과정입니다. Ⅶ. 데이터 구조1. 리스트리스트란? 리스트(list)는 여러 개의 데이터가 저장된 장소 선언: [] 안에 쉼표로 구분하여 정보를 기입하면 됨 리스트 이름 &#x3D; [값1, 값2, 값3] scores &#x3D; [32, 56, 64, 72, 12, 37, 98, 77, 59, 69] 리스트는 여러 개 데이터를 하나의 이름으로 관리할 수 있는 데이터 구조 서로 다른 타입의 데이터를 하나의 리스트 이름으로 관리할 수 있음 리스트 종류 문자열 원소 숫자 원소 여러 타입 원소 Empty list empty_list = [] 리스트 원소에 접근하기 인덱스 원소가 배열된 순서 (0부터 시작_ season &#x3D; [&#39;spring&#39;, &#39;summer&#39;, &#39;fall&#39;, &#39;winter&#39;] 인덱스를 이용해 원소 접근 가능 season[1]: ‘summer’ 리스트와 연산자 ‘in’ &amp; ‘not in’ in: list의 element인가를 결정하는 연산자 not it: list의 element가 아닌 element를 결정하는 연산자 리스트 순회하기scores = [32, 56, 64, 72, 12, 37, 98, 77, 59, 69] for element in scores: print(element, end=\" \") #end=\" \": 가로로 배열 32 56 64 72 12 37 98 77 59 69 예제: 성적 처리 프로그램 학생 성적 처리 프로그램 작성 사용자로부터 성적을 입력받아 리스트에 저장 성적 평균을 구하고 80점 이상 성적 받은 학생 수를 계산하여 출력 STUDENTS = 5 scores = [] scoreSum = 0 for i in range(STUDENTS): value = int(input(\"성적을 입력하세요: \")) scores.append(value) #리스트에 값 추가 scoreSum += value scoreAvg = scoreSum / len(scores) highScoreStudents = 0 for i in range(len(scores)): if scores[i] >= 80: highScoreStudents += 1 print(\"성적 평균은\", scoreAvg, \"입니다.\") print(\"80점 이상 성적을 받은 학생은\", highScoreStudents,\"명입니다.\") 성적을 입력하세요: 40 성적을 입력하세요: 80 성적을 입력하세요: 50 성적을 입력하세요: 70 성적을 입력하세요: 90 성적 평균은 66.0 입니다. 80점 이상 성적을 받은 학생은 2 명입니다. 예제: 문자열 처리 프로그램 학생 이름을 저장했다가 출력하는 프로그램 작성 Names = [] while True: name = input(\"학생 이름을 입력하세요: \") if name == \"\": break Names.append(name) print(\"학생 이름:\") for name in Names: print(name, end=\", \") 학생 이름을 입력하세요: 나은 학생 이름을 입력하세요: 선호 학생 이름을 입력하세요: 더는 안 돼 학생 이름을 입력하세요: 더 없어 학생 이름을 입력하세요: 둘이 끝이야 학생 이름을 입력하세요: 안 돼 학생 이름을 입력하세요: 학생 이름: 나은, 선호, 더는 안 돼, 더 없어, 둘이 끝이야, 안 돼, 2. 데이터 구조데이터 구조 자료 구조(data structure) 프로그램에서 자료를 저장하는 여러 구조 파이썬: 리스트, 튜플, 딕셔너리, 문자열 등 제공 리스트 예제: 연락처 관리 프로그램 리스트 자료구조를 이용해 연락처를 관리하는 프로그램을 작성 menu = 0 friends = [] #리스트 생성 while menu != 9: print(\"---------------\") print(\"1. 친구 리스트 출력\") print(\"2. 친구 추가\") print(\"3. 친구 삭제\") print(\"4. 이름 변경\") print(\"9. 종료\") menu = int(input(\"메뉴를 선택하시오: \")) if menu == 1: print(friends) elif menu == 2: name = input(\"이름을 입력하시오: \") friends.append(name) elif menu == 3: del_name = input(\"삭제할 이름을 입력하시오: \") if del_name in friends: friends.remove(del_name) else: pinrt(\"해당되는 이름이 없습니다.\") elif menu == 4: old_name = input(\"변경하고 싶은 이름을 입력하시오: \") if old_name in friends: index = friends.index(old_name) new_name = input(\"새로운 이름을 입력하시오: \") friends[index] = new_name else: print(\"해당되는 이름이 없습니다.\") --------------- 1. 친구 리스트 출력 2. 친구 추가 3. 친구 삭제 4. 이름 변경 9. 종료 --------------------------------------------------------------------------- KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-10-b5244f1bc6b3&gt; in &lt;module&gt; 9 print(&quot;4. 이름 변경&quot;) 10 print(&quot;9. 종료&quot;) ---&gt; 11 menu = int(input(&quot;메뉴를 선택하시오: &quot;)) 12 if menu == 1: 13 print(friends) ~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in raw_input(self, prompt) 858 &quot;raw_input was called, but this frontend does not support input requests.&quot; 859 ) --&gt; 860 return self._input_request(str(prompt), 861 self._parent_ident, 862 self._parent_header, ~\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in _input_request(self, prompt, ident, parent, password) 902 except KeyboardInterrupt: 903 # re-raise KeyboardInterrupt, to truncate traceback --&gt; 904 raise KeyboardInterrupt(&quot;Interrupted by user&quot;) from None 905 except Exception as e: 906 self.log.warning(&quot;Invalid Message:&quot;, exc_info=True) KeyboardInterrupt: Interrupted by user 튜플(tuple) 값을 변경할 수 없는 리스트 튜플 &#x3D; (항목1, 항목2, ..., 항목n) numbers = (1,2,3,4,5) colors = (\"red\", \"pink\", \"blue\", \"purple\") t = numbers + colors print(t) (1, 2, 3, 4, 5, &#39;red&#39;, &#39;pink&#39;, &#39;blue&#39;, &#39;purple&#39;) 딕셔너리 키(key)와 값(value) 쌍을 저장할 수 있는 객체딕셔너리 &#x3D; &#123;키1:값1, 키2:값2, 키3:값3&#125; contacts = &#123;'Kim':'0102345678', 'Choi':'01067891234', 'Hwang':'01013467946'&#125; print(contacts) &#123;&#39;Kim&#39;: &#39;0102345678&#39;, &#39;Choi&#39;: &#39;01067891234&#39;, &#39;Hwang&#39;: &#39;01013467946&#39;&#125; contacts['Kim'] &#39;0102345678&#39; if 'Kim' in contacts: print(\"키가 딕셔너리에 있음\") 키가 딕셔너리에 있음 contacts = &#123;'Kim':'0102345678', 'Choi':'01067891234', 'Hwang':'01013467946'&#125; for item in contacts.items(): print(item) (&#39;Kim&#39;, &#39;0102345678&#39;) (&#39;Choi&#39;, &#39;01067891234&#39;) (&#39;Hwang&#39;, &#39;01013467946&#39;) 문자열 문자열은 문자들의 시퀀스로 정의 글자들이 실(string)로 묶인 것 s1 = str(\"Naeun\") s2 = \"Seonho\" s3 = s1 + s2 print(s3) NaeunSeonho word = 'abcdef' print(word[0]) print(word[5]) ## 슬라이싱 word1 = 'Python' print(word[0:2]) #인덱스 2보다 작을 때까지 → 0, 1 a f ab in 연산자 not in 연산자s = 'Love will find a way.' 'Love' in s True 'wi' in s True 'e w' in s True","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"},{"name":"리스트","slug":"리스트","permalink":"https://ne-choi.github.io/tags/%EB%A6%AC%EC%8A%A4%ED%8A%B8/"},{"name":"튜플","slug":"튜플","permalink":"https://ne-choi.github.io/tags/%ED%8A%9C%ED%94%8C/"}],"author":"ne-choi"},{"title":"Python 프로그래밍 2: Ⅵ. 함수와 모듈","slug":"Study/Postech/Python/Ⅵ. 함수와 모듈","date":"2021-01-20T00:00:00.000Z","updated":"2021-02-05T13:35:40.483Z","comments":true,"path":"/2021/01/20/Study/Postech/Python/Ⅵ. 함수와 모듈/","link":"","permalink":"https://ne-choi.github.io/2021/01/20/Study/Postech/Python/%E2%85%A5.%20%ED%95%A8%EC%88%98%EC%99%80%20%EB%AA%A8%EB%93%88/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, Python 프로그래밍 Ⅱ 과정입니다. Ⅵ. 함수와 모듈1. 함수 개념함수의 개념 함수(function) 독립적으로 수행하는 프로그램 단위로 특정 작업을 수행하는 명령어 모음에 이름 붙인 것 프로그램에서 반복적으로 수행되는 기능을 함수로 만들어 호출할 수 있음 함수는 작업에 필요한 데이터(매개변수)를 받을 수 있고, 작업 완료 후에는 작업 결과를 호출자에게 반환할 수 있음 함수 정의_Python 함수 정의는 def로 시작하고 콜론(:)으로 끝 함수 시작과 끝은 코드의 들여쓰기로 구분 시작과 끝 명시할 필요 없음 def 함수이름(입력 인수): 수행할 문장 ... return 반환값 def add(a,b): return a+b print(\"add 결과: \",add(10,10)) add 결과: 20 함수 정의 def add(a,b): result &#x3D; a + b return result 함수 호출 sum &#x3D; add(100,200) print(sum) 함수 매개변수와 반환값(함수 내 정의된 문장은 바로 실행되지 않음) 입력값, 반환값이 없는 함수 def say(): print(\"hello\") 입력값은 없고 반환값이 있는 함수 def say(): return(\"hello\") 함수 작성 예시 정수 거듭제곱값을 계산하여 반환하는 함수 작성(** 연산자 사용 가능) def power(x,y): result = 1 for i in range(y): result = result * x return result print(\"power 함수 실행: \",power(10,2)) power 함수 실행: 100 ## main() 함수 호출 활용 def main(): print(power(10,2)) def power(x,y): result = 1 for i in range(y): result = result * x return result main() 100 2. 함수 실습일상 생활 문제: 성적 처리 문제 데이터: 점수 처리 기능: 총점, 정렬(두 수의 교환) 처리 총점 계산(add()) 총점으로 평균 계산 총점 반영하여 성적순 정렬 정렬 시, 두 변수의 값을 바꾸는 함수 필요(swap()) a, b = b, a ## 함수 def add(a,b): return a + b def swap(a,b): a,b = b,a return a,b ## 시작코드 a = int(input(\"정수 1 입력: \")) b = int(input(\"정수 2 입력: \")) sum = add(a,b) average=sum/2 print(\"두 수의 합: \",sum) print(\"두 수의 평균: \",average) a,b=swap(a,b) print(\"두 수 교환: \",a,b) 정수 1 입력: 4 정수 2 입력: 5 두 수의 합: 9 두 수의 평균: 4.5 두 수 교환: 5 4","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"},{"name":"함수","slug":"함수","permalink":"https://ne-choi.github.io/tags/%ED%95%A8%EC%88%98/"}],"author":"ne-choi"},{"title":"Python 프로그래밍 2: Ⅴ. 제어 문장 2","slug":"Study/Postech/Python/Ⅴ. 제어 문장 2","date":"2021-01-19T00:00:00.000Z","updated":"2021-02-05T13:34:14.569Z","comments":true,"path":"/2021/01/19/Study/Postech/Python/Ⅴ. 제어 문장 2/","link":"","permalink":"https://ne-choi.github.io/2021/01/19/Study/Postech/Python/%E2%85%A4.%20%EC%A0%9C%EC%96%B4%20%EB%AC%B8%EC%9E%A5%202/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, Python 프로그래밍 Ⅱ 과정입니다. Ⅴ. 제어 문장 21. 반복문(for, while)제어문(Control statements) 종류 조건문: 조건에 따라 프로그램 흐름을 제어하는 명령문 if, if elif 반복문: 조건에 따라 정해진 문장을 반복 수행하는 명령문 for, while 분기문(Jump Statements) return, break, continue Computational Thinking 컴퓨팅 사고력: 컴퓨터가 문제를 해결하는 방식을 현실 문제 해결에 적용하는 것 추상화: 실제 세계 문제를 해결 가능한 형태로 표현하기 위한 사고 과정 자동화: 추상화 과정에서 만들어진 해결 모델을 컴퓨터가 이해할 수 있는 프로그래밍 언어로 표현하는 것 for 반복적이고 지루한 작업을 컴퓨터를 이용해 자동화 for: 리스트나 튜플, 문자열의 첫 번째 요소부터 마지막 요소까지 차례로 변수에 대입되어 “수행할 문장 1”, “수행할 문장 2” 등 수행 for 변수 in 리스트(or 튜플, 문자열): 수행할 문장 1 수행할 문장 2 ... 참고: range() 함수range([start,] stop [,step]) # [] 생략 가능 range(start, stop)과 같이 호출하면 start부터 시작해서 (stop-1)까지의 정수 생성 stop은 포함되지 않음 start부터 stop-1까지 step 간격으로 정수 생성 range(10): 0부터 9까지의 정수가 생성 range(start, stop, step) 지정된 범위 값을 반환 특정 구간의 정수 생성 가능 for x in range(5): print(\"Hello Python~!!\") Hello Python~!! Hello Python~!! Hello Python~!! Hello Python~!! Hello Python~!! ## 리스트형 for name in [\"나은\", \"선호\"]: print(\"안녕! \" + name) 안녕! 나은 안녕! 선호 while 조건의 결과(참 or 거짓)에 따라 특정 부분 처리를 반복 실행하는 제어문장 조건문이 참인 동안 while문 아래에 속하는 문장들이 반복하여 수행 while 조건문: 수행할 문장 1 수행할 문장 2 수행할 문장 3 i = 0; while i &lt;5: print(\"Hello 김선호 배우님\") i = i + 1 print(\"반복이 종료되었습니다.\") Hello 김선호 배우님 Hello 김선호 배우님 Hello 김선호 배우님 Hello 김선호 배우님 Hello 김선호 배우님 반복이 종료되었습니다. 분기문(Jump Statement) 반복문 탈출하는 break문 for i in range(1,100): print(\"for문을 %d번 실행\" % i) break for문을 1번 실행 반복문으로 다시 돌아가는 continue문 블록 남은 부분을 건너 뛰고 무조건 처음으로 돌아감 hap, i = 0,0 for i in range(1,101): if i % 3 == 0: continue hap += i print(\"1-100의 합계(3의 배수 제외): %d\" % hap) 1-100의 합계(3의 배수 제외): 3367 2. 반복문 실습정수들의 합 for문 사용 1부터 사용자가 입력한 수 n까지 더하여 (1+2+3+…+n)을 계산하는 프로그램 작성 # 반복을 이용한 정수합 프로그램 sum = 0 #sum을 저장할 수 있는 변수 생성 limit = int(input(\"어디까지 계산할까요?: \")) for i in range(1, limit+1): #stop이 -1 되어 있기 때문에 +1 해줌 sum += i print(\"1부터 \",limit,\"까지의 정수 합 =\", sum) 어디까지 계산할까요?: 50 1부터 50 까지의 정수 합 = 1275 팩토리얼 계산 for문 사용 팩토리얼 n!은 1부터 n까지 정수를 모두 곱한 것 fact = 1.0 #초기값으로 실수를 넣음 n = int(input(\"정수를 입력하시오: \")) for i in range(1, n+1): #range의 stop이 n-1이기 때문에 +1을 해줌 fact = fact * i; print(n,\"!은\", fact,\"이다.\") 정수를 입력하시오: 5 5 !은 120.0 이다. 자리수의 합 정수 안 각 자리수의 합을 계산하는 프로그램 작성 e.g. 1234 → 1+2+3+4 number = 1234 sum = 0; while number > 0: digit = number % 10 sum = sum + digit number = number // 10 print(\"자리수의 합은 %d입니다.\" % sum) 자리수의 합은 10입니다. number = int(input(\"정수를 입력하시오: \")) print(\"입력한 수는: \",number) sum = 0; while number > 0: digit = number % 10 sum = sum + digit number = number // 10 print(\"자리수의 합은 %d입니다.\" % sum) 정수를 입력하시오: 555 입력한 수는: 555 자리수의 합은 15입니다. 숫자 맞추기 게임 컴퓨터가 선택한 숫자를 사용자가 맞추는 게임 입력한 숫자가 선택한 숫자보다 높은 수인지 낮은 수인지 정보 제공 시도 횟수 제공 import random tries = 0 number = random.randint(1,100) #1부터 100사이의 숫자를 랜덤으로 컴퓨터가 선택 print(\"1부터 100 사이의 숫자를 맞추시오.\") while tries &lt; 10: #시도횟수 제한: 10회 guess = int(input(\"숫자를 입력하시오: \")) tries = tries + 199 if guess &lt; number: print(\"Up.\") elif guess > number: print(\"Down.\") else: break if guess == number: print(\"축하합니다. 시도횟수: \", tries,\"회\") else: print(\"정답은 \",number,\"입니다.\") 1부터 100 사이의 숫자를 맞추시오. 숫자를 입력하시오: 55 Down. 정답은 51 입니다. 커피 자판기 수정 필요 coffee = int(input(\"커피 재고: \")) money = int(input(\"돈: \")) while money: print(\"\\n돈을 받았으니 4000원 짜리 커피를 줍니다.\") money = money - 4000 if money &lt; 0: print(\"커피를 구매할 수 없습니다.\") if money >= 0: print(\"커피를 1잔 구매했습니다. 남은 돈은 %d원입니다.\" % money) coffee = coffee - 1 > 0 print(\"남은 커피의 양은 %d개입니다.\" % coffee) coffee &lt; 0 print(\"커피가 다 떨어졌습니다. 판매를 중지합니다.\") if not money: print(\"예산이 다 떨어졌습니다. 커피를 구매할 수 없습니다.\") break","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"},{"name":"for문","slug":"for문","permalink":"https://ne-choi.github.io/tags/for%EB%AC%B8/"},{"name":"while문","slug":"while문","permalink":"https://ne-choi.github.io/tags/while%EB%AC%B8/"}],"author":"ne-choi"},{"title":"Python 프로그래밍 1: Ⅳ. 제어 문장 1","slug":"Study/Postech/Python/Ⅳ. 제어 문장 1","date":"2021-01-18T00:00:00.000Z","updated":"2021-02-05T13:33:38.552Z","comments":true,"path":"/2021/01/18/Study/Postech/Python/Ⅳ. 제어 문장 1/","link":"","permalink":"https://ne-choi.github.io/2021/01/18/Study/Postech/Python/%E2%85%A3.%20%EC%A0%9C%EC%96%B4%20%EB%AC%B8%EC%9E%A5%201/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, Python 프로그래밍 Ⅰ 과정입니다. Ⅳ. 제어 문장 11. 조건문(if else)제어 문장 Sequence (순차) Selecton (선택) - 조건문 if else, if elif Repetition (반복) - 조건문 loop: for, while 조건문 공부가 필요한 이유 예제: 일상 생활 문제_성적처리 성적자료를 입력 받아 변수에 저장 변수 선언 (python101) 입력 함수: input 함수 (int형으로 입력 받기) 점수에 따라 학점을 계산하기 위해 필요한 연산자 관계 연산자: (python101 &gt;= 90) 조건에 따라 프로그램의 흐름을 제어하는 명령어 조건 명령어: if else 여러 조건 비교 명령어: if elif 조건문_if 조건의 결과(참 or 거짓)에 따라 프로그램 흐름을 제어하는 문장 어떤 조건을 만족하면 그에 해당하는 일이 처리되는 문장 if (expression): statement1 next_statement 조건문의 결과가 참(True)이면 A를 수행if 조건문: ---→수행할 문장 1 ---→수행할 문장 2 ... ... ---→수행할 문장 n 블록(Block) 여러 코드가 이루는 일정한 구역 python의 경우, 들여쓰기로 구역을 나눔 들여쓰기는 스페이스나 탭을 사용 스페이스 4칸 사용 권장 조건문_if else 조건의 결과가 참(True)이면 if문 바로 다음 문장(if 블록)들을 수행 조건의 결과가 거짓(False)이면 else문 다음의 문장(else 블록)들을 수행if 조건문: 수행할 문장 1 수행할 문장 2 else: 수행할 문장 A 수행할 문장 B a = 200 if a &lt; 100: print(\"참\") print(\"100보다 작다\") else: print(\"거짓\") print(\"100보다 크거나 같다\") print(\"프로그램 끝\") 거짓 100보다 크거나 같다 프로그램 끝 조건문_중첩 ifif 조건 A: if 조건 1: 수행할 문장 A1 else: 수행할 문장 A2 else: 수행할 문장 B 예제마트에 사과가 신선하면 사과를 사기로 한다. 만약 사과가 개당 1,000원 미만이면 10개를 산다. 하지만, 사과가 개당 1,000원 이상이면 5개만 산다. appleQuality = input(\"사과의 상태를 입력하시오: \") applePrice = int(input(\"사과 1개의 가격을 입력하시오: \")) if appleQuality == \"신선\": if applePrice &lt; 1000: print(\"10개를 산다.\") else: print(\"5개를 산다.\") else: print(\"사과를 사지 않는다.\") 사과의 상태를 입력하시오: 안신선 사과 1개의 가격을 입력하시오: 400 사과를 사지 않는다. 조건문_if elif 여러 개의 조건을 비교하여 조건에 맞는 문장 수행if 조건 A: 수행할 문장 A elif 조건 B: 수행할 문장 B elif 조건 C: 수행할 문장 C else: 예제1개의 정수를 입력 받아 양수, 0 , 음수를 판별 number = int(input(\"정수를 입력하시오: \")) if number &lt; 0: print(\"입력된 정수는 음수입니다.\") elif number == 0: print(\"입력된 정수는 0입니다.\") else: print(\"입력된 정수는 양수입니다.\") 정수를 입력하시오: -4 입력된 정수는 음수입니다. 2. 조건문 실습 성적 처리 예제 실습 python101 = int(input(\"python 점수를 입력하세요: \")) if python101 >= 90: print(\"파이썬 프로그래밍 학점은 A입니다.\") elif python101 >= 80: print(\"파이썬 프로그래밍 학점은 B입니다.\") elif python101 >= 70: print(\"파이썬 프로그래밍 학점은 C입니다.\") elif python101 >= 60: print(\"파이썬 프로그래밍 학점은 D입니다.\") else: print(\"파이썬 프로그래밍 학점은 F입니다.\") print(\"고생하셨습니다!\") python 점수를 입력하세요: 80 파이썬 프로그래밍 학점은 B입니다. 고생하셨습니다! 가위바위보 게임 예제 실습 난수 생성 (컴퓨터의 가위바위보 선택을 위해) 라이브러리 함수 활용(import random) 사용자 선택 변수를 입력 받아 변수에 저장 변수 선언(player) 입력 함수: input 함수 조건에 따라 난수 생성 난수 함수 활용 (number = random.randint(0,2)) 생성된 난수를 활용해 가위바위보 게임 알고리즘 만들기 게임 규칙에 따라 여러 조건에 따라 수행되는 문장 만들기 (if elif) import random # 난수 생성 player = input(\"(가위, 바위, 보) 중 하나를 선택하세요: \") number = random.randint(0,2) # 정수 난수 0,1,2 중 1개 생성 if number == 0: computer = \"가위\" elif number == 1: computer = \"바위\" elif number == 2: computer = \"보\" print(\"사용자: \", player, \"컴퓨터: \", computer) if player == computer: print(\"비겼음\") elif player == \"가위\": if computer == \"바위\": print(\"컴퓨터 승\") else: print(\"사용자 승\") elif player == \"바위\": if computer == \"보\": print(\"컴퓨터 승\") else: print(\"사용자 승\") elif player == \"보\": if computer == \"가위\": print(\"컴퓨터 승\") else: print(\"사용자 승\") (가위, 바위, 보) 중 하나를 선택하세요: 가위 사용자: 가위 컴퓨터: 바위 컴퓨터 승","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 2: Ⅷ. 회귀분석을 이용한 예측모형","slug":"Study/Postech/통계학입문/Ⅷ_회귀분석을_이용한 예측모형","date":"2021-01-14T00:00:00.000Z","updated":"2021-02-01T01:02:57.419Z","comments":true,"path":"/2021/01/14/Study/Postech/통계학입문/Ⅷ_회귀분석을_이용한 예측모형/","link":"","permalink":"https://ne-choi.github.io/2021/01/14/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A7_%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다. Ⅷ. 회귀분석을 이용한 예측모형1. 선형모형의 특성과 상관분석데이터의 상관관계 데이터 탐색: 변수 간 관계 변수 간 상관관계 여부 상관관계 형태 상관계수(Pearson’s) 상관계수(correlation coefficient) 두 변수 간 선형관계의 강도를 나타내주는 척도 상관계수(r) 절댓값은 0-1 사이 0에 가까울수록 상관관계가 없음 1에 가까울수록 강한 상관성이 있음 정리 상관계수 및 변수 간 산점도: 데이터 탐색 X, Y 간 선형관계를 산점도에 도식화 X, Y 간 선형함수식 추정: 회귀모형 Y 분산에 관한 독립변수를 추가함으로써 선형모형 설명력 향상 2. 실제데이터 예측모형구현회귀모형의 적합 조선 회귀분석- 예측모형 회귀분석의 목적: 예측(prediction)과 추정(estimation) 선형모형: 독립변수와 종속변수 관계가 선형식으로 적합 최소자승법(least squares method): 예측값과 관측치 오차를 최소화시키는 회귀계수 추정 예측 모형 회귀 계수(regression coefficient)와 독립변수의 선형 결합으로 표현되는 예측모형 Y = β0 + β1x1 + β2x2 + … + βkxk Y: 종속변수(dependent variable), 반응변수, 타겟변수 x: 독립변수(independent variable), 설명변수, 원인 역할을 하는 변수 회귀모형의 적합도 및 회귀성 검정 모형의 적합도와 결정계수 (R^2^) 회귀식이 데이터를 얼마나 잘 설명하는지에 관한 척도 회귀모형으로 설명할 수 있는 부분의 비율 0 ≤ R^2^ ≤ 1 R^2^ = $\\frac{SSR}{SST}$ = 1 - $\\frac{SSE}{SST}$ 회귀모형의 가정과 진단 가정 X와 Y는 선형적 관계 오차항(εi)은 정규분포를 따름 오차항의 평균은 0 오차항의 등분산성 오차항은 서로 독립 진단 잔차 ei와 Y 산점도 스튜던트화 잔차 정규확률분포도 회귀계수에 대한 검정 β1 관한 t-검정 해당 독립변수가 유의한 영향을 갖고 있는지 판단 회귀계수의 p-value 특정변수의 회귀계수에 관한 p-value가 유의수준(α)보다 작으면 그 변수는 유의(significant)하다고 판단 정리 회귀모형의 적합도 검정 이론적인 모형이 실제 관측 데이터에 의해 어느 정도 지지를 받는지 검정하는 것 R^2^ 결정계수: 회귀모형의 설명력 개별 βk값 검정: 어떤 변수가 중요한지 잔차에 관한 가정 확인: 잔차산점도 3. 회귀모형의 적합조건실제 데이터로 예측모형 구현 다중회귀모형(multiple regression) 종속변수 Y를 설명하는 데 k개의 독립변수가 있을 때의 회귀모형 Yi = β0 + β1x1 + β2x2 + … + βkxk + εi(잔차) 회귀계수 βk의 해석: 다른 독립변수가 일정할 때, Xk 한 단위 변화에 따른 평균변화량 다중회귀모형 예제car &lt;- read.csv(&#39;.&#x2F;data&#x2F;autompg.csv&#39;) r1 &lt;- lm(mpg ~ disp+hp+wt+accler, data&#x3D;car) summary(r1) 결과 해석 선형회귀식: mpg = 40.88 - 0.001 disp + 0.0048 hp - 0.0061 wt + 0.17 accler 선형회귀식의 결정계수: R^2^ = 0.7006 분산을 70% 정도 설명한다고 해석할 수 있음 문제 마력(hp)이 높을수록 연비가 좋은가? 일반적으로는 아니기 때문에 데이터 탐색 필요 var1 &lt;- c(&quot;mpg&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;accler&quot;) pairs(car[var1], main&#x3D;&quot;Autompg&quot;, cex&#x3D;1, col&#x3D;as.integer(car$cyl)) 다중공선성(Multicollinearity) 독립변수들 사이 상관관계가 높은 경우 다중공선성이 존재하면, 회귀계수 분산이 크고 모형이 불안정 해당 예제에서는 disp(배기량) 제외하는 것이 더 좋음 hp 기준으로 두 개의 모델로 분류해 예측하기car2 &lt;- subset(car, hp&lt;50) attach(car2) r2 &lt;- lm(mpg~hp+wt+accler, data&#x3D;car2) summary(r2) car3 &lt;- subset(car, hp&gt;&#x3D;50) attach(car3) r3 &lt;- lm(mpg~hp+wt+accler, data&#x3D;car2) summary(r3) 4. 예측모형의 핵심과 고려사항분류분석- 로지스틱회귀모형 타겟변수(Y)가 범주형일 때 binary: 2개의 범주 (보험사기, 질병) ordinal(서열): 서열이 있는 범주 (순위) nominal(명목): 서열이 없는 범주 (직업군) 로지스틱회귀모형 Y가 범주이기 때문에 선형 조합이 나옴 로짓 트랜스포메이션을 해서 확률값으로 변형 re &lt;- read.csv(&#39;.&#x2F;data&#x2F;remiss.csv&#39;) t3 &lt;- glm(remiss~cell+li+temp, data&#x3D;re,family&#x3D;binomial(logit)) summary(t3) 결과 해석 logistic equation logit(p) = 67.63 + 9.65 Cell + 3.87 Li - 82.07 Temp Li 1단위 증가 시 remission 확률: exp(3.867) = 47.79 예측확률값 출력: 원래 데이터 + 예측확률값dat1_pred &lt;- cbind(re,t3$fitted.values) write.table(dat1_pred,file&#x3D;&#39;dat1_pred.csv&#39;, row.names&#x3D;FALSE, sep&#x3D;&quot;,&quot;, na&#x3D;&quot; &quot;) 예측모형 고려사항 예측모형 선택 예측모형(어느 모형, 어느 기법) 결정계수(기여율)가 높은 회귀식: 설명변수가 많을수록 결정계수가 높아짐 추정값의 표준오차(잔차의 표준편차)가 낮은 회귀식 변수 선택방법 모든 가능한 회귀: 모든 설명변수를 포함한 모형을 시도 단계적 선택법(stepwise regression): 설명변수를 단계적으로 선택하는 방법 이상치(outlier) 탐지 이상치(outlier): 보편적인 데이터 값 번위를 벗어나는 데이터 왜곡된 예측모형 위험이 있으므로 이상치 제외 후 분석 이상치 탐색 거리 계산: 각 데이터와 독립변수 평균과의 거리가 크거나, 회귀모형에서의 잔차 이상 수치 확인 산점도: 변수들 간 산점도에서 탐색 정리 데이터 수집(DB, 설문조사) 종속변수(타겟변수)와 관계 있는 독립변수(설명변수) 확인 산포도 작성 → 데이터 특성 파악 및 이상치 탐색 회귀모형 결정(변수 선택/ 특징 변환) 회귀모형 적합도 평가","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"회귀모형","slug":"회귀모형","permalink":"https://ne-choi.github.io/tags/%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 2: Ⅶ. 빅데이터 차원축소와 예측모형","slug":"Study/Postech/통계학입문/Ⅶ_빅데이터_차원축소와_예측모형","date":"2021-01-13T00:00:00.000Z","updated":"2021-02-03T07:19:49.307Z","comments":true,"path":"/2021/01/13/Study/Postech/통계학입문/Ⅶ_빅데이터_차원축소와_예측모형/","link":"","permalink":"https://ne-choi.github.io/2021/01/13/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A6_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C%EC%99%80_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다. Ⅶ. 빅데이터 차원축소와 예측모형1. 데이터 탐색과 정제데이터 탐색 Basic Information: 분포, 평균, 최솟값, 최댓값, 분산 등 Variation: 변동성, 시계열 Outlier: 모든 데이터가 상식적인 범위 안에 존재하는가? Correlation: 변수 간 상관성, 변수 간 상호작용이 있는가? 2. 빅데이터의 차원축소차원축소기법은 왜 필요한가 현업 데이터의 특성 타겟변수 특성에 영향을 미치는 요인이 매우 많음 변수 간 다중공선성(높은 상관관계, 상호작용) 과적합 위험 증가 타겟값 정보가 들어있는 구조 파악이 어려움 차원축소기법- 주성분분석 주성분분석(Principal Component Analysis; PCA) 가장 대표적인 차원 숙소 방법 첫 번째 변수가 전체 분산을 가장 많이 설명하고, 다음 변수가 나머지 분산을 가장 많이 설명하는 방식으로 변수 생성 새로 구성된 변수는 서로 독립 주성분분석 시행 방법 데이터 공간에서 분산이 최대인 축을 찾는다. (첫 번째 주성분: PC1) 첫 번째 축과 직교하며 분산이 최대인 두 번째 축을 찾는다. (두 번째 주성분: PC2) 최적 주성분 찾기 전체 변동에 대한 기여도: 전체 변동의 약 90%를 설명하는 차원까지 Scree plot 활용: 기울기가 갑자기 줄어드는 차원까지 3. 데이터 변환에 의한 저차원 시각화고차원데이터의 차원축소 데이터 유형 파악 Classification(Y: 범주형) Prediction(Y: 연속형) 변수 선택(feature selection) 기법 특정 변수를 선택하여 모델링 다중공선성 존재하는 변수는 그 중 하나의 변수만 선택 특징 추출(feature extraction) 기법 새로운 축을 생성했을 때 생기는 변화 고차원 데이터 정보를 보존하며 노이즈를 제거하는 방식으로 특징 추출 고차원데이터의 저차원 시각화 주성분분석: 선형 추출 기법 인접보존기법: 비선형 추출 기법 차원축소와 예측모형 Feature Ectraction 인접보존기법 제한적 볼츠만머신 오토인코더 Classifier 4. 데이터와 예측모형데이터와 예측모형 데이터 정제 → 데이터 탐색 → 통계적 모델링(통계모형, 기계학습, 인공지능) 데이터 분석목적 예측(prediction) 분류(classification)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"주성분분석","slug":"주성분분석","permalink":"https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"},{"name":"차원축소","slug":"차원축소","permalink":"https://ne-choi.github.io/tags/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 2: Ⅵ. 현업 데이터 특성과 예측모형","slug":"Study/Postech/통계학입문/Ⅵ_현업_데이터_특성과_예측모형","date":"2021-01-12T00:00:00.000Z","updated":"2021-02-01T01:00:03.851Z","comments":true,"path":"/2021/01/12/Study/Postech/통계학입문/Ⅵ_현업_데이터_특성과_예측모형/","link":"","permalink":"https://ne-choi.github.io/2021/01/12/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A5_%ED%98%84%EC%97%85_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%8A%B9%EC%84%B1%EA%B3%BC_%EC%98%88%EC%B8%A1%EB%AA%A8%ED%98%95/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다. Ⅵ. 현업 데이터 특성과 예측모형1. 데이터 수집- random의 의미데이터 수집 데이터 수집에서는 양질의 데이터를 확보하는 것이 중요 나쁜 데이터로는 나쁜 모델밖에 만들 수 없음 ‘대표성’, ‘랜덤’ 데이터 데이터 수집_표본 추출 좋은 표본: 모집단의 특성을 가능한 정확하게 반영한 표본 표본추출에서 가장 중요한 문제는 대표성 있는 표본을 확보하는 것 전수조사 vs 표본조사 전수조사: 연구대상집단의 모든 데이터 수집 표본조사: 연구대상집단 일부 데이터 추출 랜덤의 의미 무작위 추출(Random sampling) 모집단에서 표본을 뽑을 때 각 개체가 선택될 확률은 동일 무작위로 추출된 표본은 편의가 최대한 배제됨 군집표본추출 vs 층화표본추출 군집표본추출(cluster sampling) 각 군집(cluster)이 동일한 특성을 갖고 있다면, 그 중 무작위로 cluster 선택 e.g. 학급, ##구, ##동 층화표본추출(stratified sampling) 모집단 내 하위집단 특성이 다를 때, 하위집단을 기반으로 표본 선택 e.g. 정치적 의견(연령별, 지역별) 복원추출 vs 비복원추출 복원추출: 표본추출 시 뽑은 표본의 데이터를 다시 넣고 추출 비복원추출: 표본추출 시 추출된 표본을 제외하고 다음 추출 2. 예측모형에서 training과 test set예측모형을 구현할 때 e.g. 영화 추천, 음악 추천 어떻게 원하는 콘텐츠를 예측하여 추천하는가? 과거 구매패턴을 분석하여 미래 구매를 예측(추천) 과거 데이터 → 분석 → 예측 모형 → 예측 주어진 데이터 → 예측모형 모형이 적절한지 어떻게 판단하는가? 예측모형: 좋은 예측모형 좋은 예측모형이란 새로운 데이터가 들어왔을 때 정확하게 예측하는 모형 훈련데이터(Training set): 모형을 만들기 위해 사용 검증데이터(Test set): 만들어진 모형 성능 평가에 사용 k-fold cross-validation (k=3, 5, 10) 주어진 데이터를 몇 개로 나눌 것인지 k-1/k는 훈련데이터로, 1/k는 검증데이터로 사용 데이터 나누는 것은 random split으로 3. 예측모형의 과적합(overfitting)예측모형의 과적합 과적합(overfitting): 주어진 데이터에 과하게 적합하여, 새로운 데이터가 들어왔을 때 정확도를 보장할 수 없는 경우 과소적합(Under-fitted) 적정적합(Generalized-fitted) 과잉적합(Over-fitted) 4. over &amp; under sampling 문제데이터 기반 예측 모형 데이터를 기반으로 모델을 생성하여 새로운 데이터 예측 가능 데이터 불균형 문제 집단 간 데이터 비율 차이가 크면 다수 집단의 정확도를 기준으로 예측모형이 결정될 수 있음 e.g. 보험 사기 건 수 전체 데이터 25개 중, 범주 1인 22개만 정확히 분류하고 범주 2인 3개는 모두 오분류된다고 해도 전체 정확도는 88%로 높음 → 전체 정확도 기준 이외, 소수집단 오분류율에 위험 비용을 주고 최적화된 모형을 찾는 것이 바람직함 데이터 균형 맞추기 Over-sampling: 소수범주에서 다수범주 수만큼 복원 추출 장) 정보 손실 없음 단) 소수 데이터가 단순 복사되어 과적합 가능성 있음 Under-sampling: 다수범주에서 소수범주 수만큼 랜덤하게 추출 장) 데이터 저장 용량 감소, 데이터가 적어 실행 속도 향상 단) 중요 정보 누락될 가능성 있음 → 혼합형 방식의 sampling 사용","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"표본추출","slug":"표본추출","permalink":"https://ne-choi.github.io/tags/%ED%91%9C%EB%B3%B8%EC%B6%94%EC%B6%9C/"},{"name":"과적합","slug":"과적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%A0%81%ED%95%A9/"},{"name":"과소적합","slug":"과소적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 2: Ⅴ. 통계검정방법","slug":"Study/Postech/통계학입문/Ⅴ_통계검정방법","date":"2021-01-11T00:00:00.000Z","updated":"2021-02-01T00:58:27.877Z","comments":true,"path":"/2021/01/11/Study/Postech/통계학입문/Ⅴ_통계검정방법/","link":"","permalink":"https://ne-choi.github.io/2021/01/11/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A4_%ED%86%B5%EA%B3%84%EA%B2%80%EC%A0%95%EB%B0%A9%EB%B2%95/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅱ 과정입니다. Ⅴ. 통계검정방법1. 신뢰구간의 의미신뢰구간 신뢰구간: 구간추정 실제 모수가 존재할 가능성이 높은 구간으로 추정 모평균, 모비율 등 모수를 포함할 확률 신뢰수준(Confidence level): 구간에 모수 μ가 포함될 확률 일반적으로 100(1-α)%로 나타냄 95% 신뢰구간의 의미 100번의 반복샘플링으로 얻은 평균과 편차로 계산한 100개의 신뢰구간 중, 5개는 실제 모평균(μ)을 포함하고 있지 않는다. 표본을 통해 얻은 95% 신뢰구간에 실제 모평균이 포함되지 않을 확률은 5%이다 전체 집단(즉, 모집단 전체)을 조사할 수 없기 때문에 이용한다 신뢰구간은 고정된 단일 값이 아닌 움직이는 여러 값이다. 적당한 신뢰구간은? 90% 신뢰구간: zα/2 = z0.5 = 1.65 95% 신뢰구간: zα/2 = z0.25 = 1.96 99% 신뢰구간: zα/2 = z0.05 = 2.57 표본사이즈와 허용오차 허용오차를 일정수준으로 정하면 그에 따른 표본크기가 정해짐 신뢰구간에서 허용오차에 영향을 미치는 요소 표본의 크기, 유의수준, 표준편차 (표본평균은 X) 요약 실험 및 조사설계 시에는 허용오차 및 신뢰수준을 고려해야 정확한 분석이 가능 2. 통계적 검정은 왜 필요한가통계적 검정이란? 가설의 진위 여부를 판단, 증명, 검정하는 통계적 추론 방식 귀무가설(Null Hypothesis, H0) 검정 대상이 되는 가설 기각(reject)을 목표로 함 대립가설(Alternative Hypothesis, H1) 귀무가설이 기각될 때 받아들여지는 가설 채택(accept)을 목표로 함 가설 검정 절차 가설 설정 → 유의수준 설정 → 검정통계량 산출 → 가설 기각/채택 검정 예시: T-test 단측검정(one-sided test) 양측검정(two-sided test): 기각역이 양측 단측검정 소비자 주장) 카페에서 파는 커피 용량이 200ml보다 작다. 귀무가설(H0): 커피 용량은 200ml다 대립가설(H1): 커피 용량은 200ml보다 적다 t-분포를 이용한 검정 한 집단 혹은 두 집단 간 평균 차이에 대한 통계적 검정 방법 T-distribution 사회 현상은 평균 데이터가 많으므로 정규 분포(normal distribution) 형태 정규 분포는 표본의 데이터 수가 많아야 신뢰도가 향상됨(일반적으로 30개 이상) 데이터가 적은 경우, 예측 범위가 더 넓은 T-distribution 사용 T-test의 가정 독립성: 두 집단 변수는 서로 독립 정규성: 두 집단 데이터는 정규분포를 만족 등분산성: 두 집단 분산은 동일 통계적 검정의 계산 예시 t = 표본평균 - μ / (표본표준편차(s)/표본의 수 루트) 3. 두 집단 t-검정두 집단 간 평균 비교 학습 목표 두 모집단 평균 비교를 위한 t-검정 계산 과정 익히기 t-검정 검정통계량과 기각역 가설 → 조사 → 데이터 → 검정통계량 → 가설에 대한 결정 (유의수준 α) 검정통계량이 기각역 안에 있으면 귀무가설 기각 → 대립가설 인정 퀴즈4. p-value의 실제 의미p-value 데이터 결과 자료에서 보게 되는 값 변수의 통계적 유의도를 나타내는 값 (유의확률) p-value 찾기 예제 25개 표본의 과자 한 팩 평균 무게가 87g이다. (모표준편차 = 15g) 유의수준 α = 0.05에서 과자 한 팩이 80g 이상이라고 할 수 있는가? 귀무가설: μ = 80, 대립가설: μ &gt; 80 검정통계량 Z = 2.33 검정통계량이 기각역 하에 있으면 귀무가설을 기각 결과: 과자 한 팩 무게는 80g이라는 주장을 인정 통계검정에서 p-value는? p-value는 유의확률: 대립가설에 대한 증거의 정도 p-value가 작다는 것은 그 검정이 매우 유의하다는 증거→ p-value는 변수의 유의성 정도 혹은 검정의 유의도를 나타냄 검정의 오류 통계적 추정을 해석할 때 발생하는 오차 Type 1 error: H0이 참인데 기각하는 오류 Type 2 error: H0이 거짓인데 채택하는 오류 |||H0 true|H0 false||—|—|—||H0 accept|1-α|Type 2 error(β)||H0 reject|Type 1 error(α)|Power(1-β)|","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"신뢰구간","slug":"신뢰구간","permalink":"https://ne-choi.github.io/tags/%EC%8B%A0%EB%A2%B0%EA%B5%AC%EA%B0%84/"},{"name":"t-test","slug":"t-test","permalink":"https://ne-choi.github.io/tags/t-test/"},{"name":"p-value","slug":"p-value","permalink":"https://ne-choi.github.io/tags/p-value/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 1: Ⅳ. 빅데이터 분석에서 확률과 분포","slug":"Study/Postech/통계학입문/Ⅳ_빅데이터_분석에서_확률과_분포","date":"2021-01-07T00:00:00.000Z","updated":"2021-02-01T00:58:36.934Z","comments":true,"path":"/2021/01/07/Study/Postech/통계학입문/Ⅳ_빅데이터_분석에서_확률과_분포/","link":"","permalink":"https://ne-choi.github.io/2021/01/07/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A3_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D%EC%97%90%EC%84%9C_%ED%99%95%EB%A5%A0%EA%B3%BC_%EB%B6%84%ED%8F%AC/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다. Ⅳ. 빅데이터 분석에서 확률과 분포1. 확률의 기초개념통계에 확률 개념이 필요한 이유 통계(Statistics) 데이터를 수집, 처리, 분석, 활용하는 지식 실제 얻어진 데이터를 바탕으로 정보 도출 확률(Probabilities) 특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것 관측 전에 가능성을 논하는 것 현실 세계는 매우 랜덤하여 미리 결과를 알 수 없음 단기적으로 어떤 사건이 일어날 비율은 매우 랜덤함 단, 장기적으로 어떤 사건이 일어날 가능성은 확률적으로 예측 가능 확률, 사건, 표본공간 확률: 특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것 사건: 표본공간에서 관심 대상인 부분집합 표본공간: 확률실험의 모든 가능한 결과의 집합 P(A) = $\\frac{사건 A가 일어나는 경우의 수}{모든 가능한 결과의 수}$ 합집합사건: 사건 A 또는 사건 B가 일어날 때 교집합사건: 사건 A와 사건 B가 동시에 일어날 때 여집합사건: 표본공간 S에서 사건 A가 일어나지 않을 때 배반사건: 교집합사건이 공사건일 때, 사건 A와 B가 서로 배반(mutually exclusive) 확률변수와 기대값 확률변수: 확률 실험에서 나타난 결과에 실수를 할당한 점수 표본공간 확률변수 HH 0 HT 1 TH 1 TT 2 기대값: 확률변수의 중심척도 랜덤한 상황에서 수치로 나타난 결과가 A1, A2, …, Ak이고 각 결과 확률이 P1, P2, …, Pk면 기대값은 각 결과에 확률을 곱하여 전부 합한 것 기대값 = A1P1 + A2P2 + … + AkPk 요약 확률: 특정 사건이 일어날 가능성을 0과 1 사이 값으로 나타낸 것 사건 하나하나를 미리 아는 것은 불가능하지만, 확률적 모형으로 많은 시행 결과 예측이 가능 표본공간과 사건을 밴다이어그램으로 나타내 특정 사건에 관한 확률을 구할 수 있음 확률변수: 확률 실험으로부터 나타난 결과에 실수를 할당한 함수 2. 조건부 확률과 베이즈확률조건부 확률과 통계적 독립 예제 두 개의 주사위를 던져 두 눈 합이 10일 확률: 1/12 첫 번째 주사위 눈이 4라는 것을 아는 상황 두 눈의 합이 10일 확률은? → 확률이 1/6으로 바뀜 조건부 확률(conditional probability) 어떤 사건(B)이 발생한다는 조건 하에서 다른 사건(A)이 발생할 확률 P(A|B) = $\\frac{P(A∩B)}{P(B)}$ P(A|B) = P(A)일 때, 즉 사건 B가 일어난다는 정보가 사건 A 발생에 전혀 영향을 주지 않을 때, ‘두 사건이 통계적 독립(independent)’이라고 한다. 베이즈 정리 베이즈 정리(Bayes’ Theorem) P(A1|B) = $\\frac{P(B∩A1)}{P(B)}$= $\\frac{P(B|A1)P(A1)}{P(B)}$= $\\frac{[P(B|A1)P(A1)]}{P(B|A1)P(A1) + P(B|A2)P(A2)}$ 베이즈 정리도 아래와 같은 조건부 확률 계산식으로 볼 수 있음 P(A1|B) = $\\frac{P(B∩A1)}{P(B)}$ 사건 B가 발생했을 때, 사건 A1이 발생할 확률을 조건부 확률 공식으로 표현 $\\frac{P(B|A1)P(A1)}{P(B)}$ P(B|A1)에 대한 조건부 확률 공식 이용 $\\frac{[P(B|A1)P(A1)]}{P(B|A1)P(A1) + P(B|A2)P(A2)}$ P(B)를 P(B∩A1) + P(B∩A2)로 계산할 수 있음 주어진(사전정보) 가설에 새로운 정보(B)가 주어졌을 때, 사후확률을 계산함 $\\frac{[P(B|A1)P(A1)]}{P(B|A1)P(A1) + P(B|A2)P(A2)}$ P(B|A1): 가농도, P(A1): 사전확률 요약 조건부 확률: 어떤 사건이 발생한다는 조건 하 다른 사건이 발생할 확률 베이즈 정리: 사후확률을 사전확률과 가능도를 이용해 계산할 수 있게 하는 확률 변환식 머신러닝기법 중, ‘나이브베이즈 분류’ 기법 계산에서 베이즈정리가 활용됨 3. 정규분포(연속형)와 포아송분포(이산형)확률분포란? 확률분포에는 이산형(discrete) 분포와 연속형(continuous) 분포가 있음 이산형 분포: 점이 띄엄띄엄 분포되어 있음 연속형 분포: 점이 연속적으로 분포되어 있음 이산형 분포 확률변수가 이산형(discrete)일 때의 확률분포 기대값 E(X) = Σx*p(x) ← 가중치 평균의 개념 분산 Var(X) = E(X^2^) - E(X)^2^ 이항분포, 다항분포, 초기하분포, 포아송분포 등 이항분포 어떤 시행의 결과가 단순히 ‘성공’ 또는 ‘실패’로 나타날 때(베르누이 시행), 성공이 나오는 횟수에 대한 확률분포 성공화귤이 p인 베르누이시행을 n회 반복할 때 성공 횟수 X E(X) = np Var(X) = np(1-p) 포아송분포 단위 시간 안에 어떤 사건이 몇 번 발생하는가에 대한 확률분포 확률변수 X가 포아송확률변수이고, 모수(평균발생횟수)가 λ E(X) = λ Var(X) = λ 연속형 분포 확률변수가 연속형(continuous)일 때의 확률분포 연속형 분포에서는 정규분포(Normal distribution)가 가장 중요함 모집단의 분포가 정규분포를 가진다고 가정하면 통계 분석이 쉬워짐 사회적 자연적 현상 통계치의 분포가 정규분포와 비슷한 형태를 띔 표준정규분포 평균이 0이고 분산이 1인 정규분포 정규분포를 표준정규분포로 만드는 법 XN(μ,σ^2^) → Z = $\\frac{X-μ}{σ}$N(0,1) 표준화를 하는 이유? 표준정규분포에서의 구간 면적을 미리 구해두면 이를 이용해서 모든 정규분포 면적을 구할 수 있음 카이제곱(x^2^)분포 확률변수 Z가 표준정규분포 N(0,1)을 따를 때, z^2^은 자유도가 1인 카이제곱분포를 따름 F-분포 두 확률변수 X1^2^과 x2^2^이 서로 독립이며, 각각의 자유도가 v1, v2인 카이제곱분포를 따를 때, 확률변수 F는 자유도가 (v1, v2)인 F-분포를 따름 요약 이산형 분포: 확률변수가 이산형일 때의 확률분포 이항분포: 베르누이시행에서 ‘성공’이 나오는 횟수에 대한 확률분포 포아송 분포: 단위시간 안에 어떤 사건이 몇 번 발생하는가에 대한 확률분포 연속형 분포: 확률변수가 연속형일 때의 확률분포 정규분포: 정규분포는 평균을 중심으로 대칭을 이루는 종모양의 연속확률분포 4. 데이터에서 출발하는 확률과 분포(중심극한)현실의 분포 현실의 다양한 분포 → 설명할 수 없는 분포 존재 중심극한정리(central limit theorem) 중심극한정리 이항분포에서 표본 수가 증가함에 따라 표본들의 전체 합이 점점 정규분포에 근접해짐 지수분포에서도 표본 수의 증가에 따른 표본평균의 분포가 점점 정규분포와 비슷해짐 원래의 분포가 정규분포가 아니더라도, 표본 수가 증가함에 따라 표본평균이 점점 정규분포모형과 비슷해짐 중심극한정리 정리 모집단이 정규분포가 아닌 경우에도 표본 수가 증가하면 표본평균의 분포가 정규분포에 근접 평균이 μ이고 분산이 σ^2^인 모집단으로부터 크기 n인 확률표본을 추출할 때, n이 크면 표뵨평균 X는 N(μ, σ^2^/n)에 근접 보통 n이 30 이상이면 모집단의 분포에 관계 없이 X는 정규분포에 근사 중심극한정리가 유용한 이유 대부분의 통계적 검정과 추정은 모집단이 정규분포를 따른다는 가정 하에 이루어짐 → 모집단의 분포를 몰라도 중심극한정리를 이용하면 표본평균의 통계적 검정과 추정이 가능해짐 요약 중심극한정리란 모집단의 분포에 관계 없이 표본의 수가 증가하면 표본평균의 분포가 정규분포에 근접한다는 이론 평균이 μ이고 분산이 σ^2^인 모집단으로부터 크기 n(≥ 30)인 확률표본을 추출할 때 표본평균 X는 N(μ, σ^2^/n)에 근접 모집단의 분포를 몰라도 중심극한정리를 이용하면 표본평균의 통계적 검정과 추정이 가능해짐","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"통계","slug":"통계","permalink":"https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"},{"name":"데이터시각화","slug":"데이터시각화","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/"}],"author":"ne-choi"},{"title":"Kaggle: The future of ML(Americans vs Indians)","slug":"Projects/Kaggle Competition-The_future_of_ML_americans_vs_indians","date":"2021-01-07T00:00:00.000Z","updated":"2021-01-20T04:42:31.814Z","comments":true,"path":"/2021/01/07/Projects/Kaggle Competition-The_future_of_ML_americans_vs_indians/","link":"","permalink":"https://ne-choi.github.io/2021/01/07/Projects/Kaggle%20Competition-The_future_of_ML_americans_vs_indians/","excerpt":"","text":"View to Kaggle The future of ML (Americans vs Indians)Since I started Kaggle competitions, I have a question. I called it beginner’s curiosity. “Have you ever thought about which countries have the biggest impact on Kaggle competition?” As everybody knows, USA is an obvious IT powerhouse along with Silicon Valley. It has the largest number of Kaggle Grandmaster Champions. There is no doubt that the USA has the most influence on Kaggle competitions. You can also see a lot of Indians on the list of participants. India, known as the rising IT powerhouse, is interested in the Kaggle competitions. It can be confirmed easily by checking a percentage of respondents to the 《2020 Kaggle Machine Learning &amp; data Science Survey》. The response rate for Indians is the highest about 29.2%(5,850 people). It is higher than the second-ranked country, which I would explain. In addition, a few years ago, every country emphasized the importance of machine learning because It can be used in lots of fields in the world. I also wondered about the future of machine learning, so I decided to look into the future of Machine Learning through responses from two leader countries. Actually, if we look at the current trend of machine learning, we can easily discover it. “Do you know the future of Machine Learning?” To sum up, I will compare the responses of American and Indian especially about Machine Learning in the order below. Table of Contents1. Responses Rate by Country2. Percentage by age &amp; gender3. Education &amp; Job4. Development Environment5. Basic of Machine Learning6. CV &amp; NLP7. Machine Learning in the business8. Want to learn machine learning product Library &amp; Data Importimport numpy as np import pandas as pd # Visualization import matplotlib.pyplot as plt %matplotlib inline from matplotlib import rcParams plt.rcParams['figure.dpi'] = 200 plt.rcParams['lines.linewidth'] = 2 import matplotlib as mpl from matplotlib import style style.use('fivethirtyeight') import plotly.express as px import seaborn as sns sns.set_style(\"whitegrid\") import warnings warnings.filterwarnings(\"ignore\", category=FutureWarning) warnings.filterwarnings(\"ignore\", category=DeprecationWarning) pd.set_option('mode.chained_assignment', None) survey = pd.read_csv('../input/kaggle-survey-2020/kaggle_survey_2020_responses.csv') question = survey.iloc[0,:].T full_df = survey.iloc[1:,:] full_df[full_df.columns[3]].replace(&#123;'United States of America':'USA', 'United Kingdom of Great Britain and Northern Ireland':'UK'&#125;, inplace=True) df = full_df[(full_df['Q3']=='India')|(full_df['Q3']=='USA')] df = df.iloc[1:,:] df.reset_index(drop=True, inplace=True) df = df[(df['Q2']=='Man')|(df['Q2']=='Woman')] df = df.iloc[1:,:] df.reset_index(drop=True, inplace=True) /opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) # Extract data by country India = df[df['Q3']=='India'] India = India.iloc[1:,:] India.reset_index(drop=True, inplace=True) USA = df[df['Q3']=='USA'] USA = USA.iloc[1:,:] USA.reset_index(drop=True, inplace=True) question.head() Time from Start to Finish (seconds) Duration (in seconds) Q1 What is your age (# years)? Q2 What is your gender? - Selected Choice Q3 In which country do you currently reside? Q4 What is the highest level of formal education ... Name: 0, dtype: object full_df.head() 1. Responses Rate by Country full_pie = px.pie(full_df, full_df.columns[3], title='Survey Responses Rate by Country', template='seaborn', hole=0.5) full_pie.update_traces(textposition='inside', textinfo='percent+label') full_pie.update_layout(uniformtext_minsize=10, uniformtext_mode='hide') full_pie.show() The response rate for Indians is the highest which is about 29.2% or 5,850 people. It is about 2.6 times higher than USA, a second-ranked country. Based on the response of the Americans and Indians for this report, I extracted the data by 2 using the two countries. PreprocessingTo organize the data neatly, I pre-processed a gender column in which only females and males would remain. (146 responses will be deleted) df = df[(df[df.columns[2]]=='Man')|(df[df.columns[2]]=='Woman')] df = df.iloc[1:,:] df.reset_index(drop=True, inplace=True) df.shape (7940, 355) print(\"- Valid American Response: \",len(df[df['Q3']=='USA'])) print(\"- Valid Indian Response: \",len(df[df['Q3']=='India'])) - Valid American Response: 2161 - Valid Indian Response: 5779 I also made each data set(‘USA’, ‘India’) to make pie graphs. 2. Percentage by age &amp; gender To see their interest in coding, let’s look at the age and gender data. legend_list = ['USA', 'India'] def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 2200]) plt.xticks(rotation=0, fontsize=8) plt.xlabel('age group', fontsize=8) plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Age Distribution', fontsize=15) plt.show() q1_order = ['18-21','22-24','25-29','30-34','35-39','40-44','45-49','50-54','55-59','60-69','70+'] col_name = \"Q1\" countplot_(df, col_name, q1_order) You can easily notice in USA the distribution of age groups is more regular than India. It represents the cording fever especially among young Indians. It can be interpreted that India has a craze for coding. q3_order = df['Q3'].value_counts()[:11].index df_q2q3 = df[['Q2','Q3']].groupby('Q3')['Q2'].value_counts().unstack().loc[q3_order] df_q2q3['sum'] = df_q2q3.sum(axis=1) df_q2q3_ratio = (df_q2q3.T / df_q2q3['sum']).T[['Man','Woman']][::-1] fig, ax = plt.subplots(1,1,figsize=(10, 1.5)) ax.barh(df_q2q3_ratio.index, df_q2q3_ratio['Man'], alpha=0.5, label='Man') ax.barh(df_q2q3_ratio.index, df_q2q3_ratio['Woman'], left=df_q2q3_ratio['Man'], alpha=0.5, label='Woman') ax.set_xlim(0, 1) ax.set_xticks([]) ax.set_yticklabels(df_q2q3_ratio.index, fontsize=15) # male percentage for i in df_q2q3_ratio.index: ax.annotate(f\"&#123;df_q2q3_ratio['Man'][i]*100:.3&#125;%\", xy=(df_q2q3_ratio['Man'][i]/2, i), fontsize=12, va='center') for i in df_q2q3_ratio.index: ax.annotate(f\"&#123;df_q2q3_ratio['Woman'][i]*100:.3&#125;%\", xy=(df_q2q3_ratio['Man'][i]+df_q2q3_ratio['Woman'][i]/2, i), va='center', ha='center', fontsize=12) plt.title('Gender Distribution', fontsize=20) for s in ['top', 'left', 'right', 'bottom']: ax.spines[s].set_visible(False) ax.legend(loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.40)) plt.show() The gender ratio of two countries is alike. In both countries, male respondents are about 3.4 times more than female respondents. 3. Education &amp; Job Then I checked their education level and their current job. 3-1. Level of EducationPlease notice that I can just confirm their degree level, not major. However, many respondents study above bachelor’s degree, so I think it is okay to assume that their major is related to coding or programming. df['Q4'] = df['Q4'].str.replace(\"[^A-Za-z0-9-\\s]+\", \"\") df['Q4'].replace(&#123;'No formal education past high school':'~ High school', 'I prefer not to answer':'Not answer', 'Some collegeuniversity study without earning a bachelors degree':'Study without a BD', 'Masters degree':\"Master's degree\", 'Bachelors degree':\"Bachelor's degree\", ' High school':'~ High school'&#125;, inplace=True) def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['~ High school', 'Professional degree', 'Study without a BD', \"Bachelor's degree\",\"Master's degree\",'Doctoral degree', 'Not answer']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 3200]) plt.xticks(rotation=15, fontsize=8) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Level of Education ', fontsize=15) plt.show() q4_order = ['~ High school', 'Professional degree', 'Study without a BD', \"Bachelor's degree\",\"Master's degree\",'Doctoral degree', 'Not answer'] col_name = \"Q4\" countplot_(df, col_name, q4_order) df_normalize4 = df.groupby(['Q3'])['Q4'].value_counts(dropna=False, normalize=True, ascending=False) df_normalize4 = pd.DataFrame(df_normalize4) df_normalize4 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Q4 Q3 Q4 India Bachelor's degree 0.518602 Master's degree 0.316145 Doctoral degree 0.046548 Professional degree 0.036512 Study without a BD 0.035127 NaN 0.021284 Not answer 0.020938 ~ High school 0.004845 USA Master's degree 0.469227 Bachelor's degree 0.256826 Doctoral degree 0.184174 Study without a BD 0.056918 Professional degree 0.017122 NaN 0.006941 ~ High school 0.004627 Not answer 0.004165 In the case of master and doctor, the percentage of Americans(46.9%, 18.4%) is higher than Indians (31.6%, 4.7%). Many Indians obtained their bachelor’s degree(51.9%) and I also noticed that 3.7% of the Indians had a professional degree. It shows that USA already has many IT technicians that are highly educated, and India’s level of coding education is growing. 3-2. Current Jobdf['Q5'].replace(&#123;'Currently not employed':'Not employed'&#125;,inplace=True) def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['Business Analyst','Data Analyst','Data Engineer','Data Scientist', 'DBA/Database Engineer','Machine Learning Engineer','Product/Project Manager', 'Research Scientist','Software Engineer','Statistician','Student','Not employed','Other']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 2400]) plt.xticks(rotation=20, fontsize=6) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper left') plt.title('Current Job', fontsize=15) plt.show() q5_order = ['Business Analyst','Data Analyst','Data Engineer','Data Scientist', 'DBA/Database Engineer','Machine Learning Engineer','Product/Project Manager', 'Research Scientist','Software Engineer','Statistician','Student','Not employed','Other'] col_name = \"Q5\" countplot_(df, col_name, q5_order) df_normalize5 = df.groupby(['Q3'])['Q5'].value_counts(dropna=False, normalize=True, ascending=False) df_normalize5 = pd.DataFrame(df_normalize5) df_normalize5 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Q5 Q3 Q5 India Student 0.383803 Software Engineer 0.102094 Data Scientist 0.099844 Not employed 0.097076 Other 0.069389 Data Analyst 0.058661 Machine Learning Engineer 0.050874 NaN 0.035473 Business Analyst 0.033916 Research Scientist 0.023707 Product/Project Manager 0.021457 Data Engineer 0.013497 Statistician 0.005883 DBA/Database Engineer 0.004326 USA Data Scientist 0.173994 Student 0.156872 Other 0.132346 Software Engineer 0.097640 Data Analyst 0.086997 Not employed 0.074965 Research Scientist 0.061083 Product/Project Manager 0.052753 Business Analyst 0.047200 Machine Learning Engineer 0.043961 Data Engineer 0.031004 Statistician 0.017584 NaN 0.016659 DBA/Database Engineer 0.006941 Most noticeably, more than 38% of Indians are students. That’s because 61.2% of the respondents in India are under 25. In the USA, there are lots of Data Scientists(17.4%), and Software Engineers(9.8%). Likewise, India has a lot of Sotfware Engineers(10.2%), and Data Scientists(10.0%). The results of the fewest jobs are the same, too. Both of the countries have few DBA/Database Engineers and Statisticians. 4. Development Environment In this part, I examined a more detailed part of the development environment. Compared to the period of coding, I discovered each country’s interest in writing code or programming. Furthermore, I will check their basic programming languages and the languages recommended by people who already used it. 4-1. Period of codingdf['Q6'].replace(&#123;'I have never written code':'Never', '&lt; 1 years':'&lt; 1 year'&#125;, inplace=True) def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['&lt; 1 year','1-2 years','3-5 years','5-10 years','10-20 years','20+ years','Never']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 1850]) plt.xticks(rotation=0, fontsize=8) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Period of Coding', fontsize=15) plt.show() q6_order = ['&lt; 1 year','1-2 years','3-5 years','5-10 years','10-20 years','20+ years','Never'] col_name = \"Q6\" countplot_(df, col_name, q6_order) df_normalize6 = df.groupby(['Q3'])['Q6'].value_counts(dropna=False, normalize=True, ascending=False) df_normalize6 = pd.DataFrame(df_normalize6) df_normalize6 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Q6 Q3 Q6 India 1-2 years 0.305416 3-5 years 0.243295 &lt; 1 year 0.198131 5-10 years 0.091711 Never 0.061429 10-20 years 0.045856 NaN 0.043087 20+ years 0.011075 USA 3-5 years 0.229986 5-10 years 0.174456 20+ years 0.164276 1-2 years 0.143915 10-20 years 0.136974 &lt; 1 year 0.089311 Never 0.037483 NaN 0.023600 In the case of the USA respondents, the number of people who have never coded nor programmed is less than that of other groups. It show us that Americans have always been interested in it. More than 30.1% of the respondents are experts who have coded for more than 10 years. Additionally, it is noticeable that the number of beginners is decreasing. India, on the other hand, has about 6.1% of respondents who have never done coding nor programming. Plus, there are numerous beginners. Almost 74.6% of Indians studied coding for less than 5 years, and about half of the respondents in India coded for less than 3 years. This graph also shows that there are already lots of coding masters in the USA, and that there are many beginners in India. 4-2. Basic programming language (multiple)# Preprocess Basic programming language data USA7 = (USA['Q7_Part_1'], USA['Q7_Part_2'], USA['Q7_Part_3'], USA['Q7_Part_4'], USA['Q7_Part_5'], USA['Q7_Part_6'], USA['Q7_Part_7'], USA['Q7_Part_8'], USA['Q7_OTHER']) USA7 = pd.concat(USA7) India7 = (India['Q7_Part_1'], India['Q7_Part_2'], India['Q7_Part_3'], India['Q7_Part_4'], India['Q7_Part_5'], India['Q7_Part_6'], India['Q7_Part_7'], India['Q7_Part_8'], India['Q7_OTHER']) India7 = pd.concat(India7) mpl.rcParams['font.size'] = 20 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA7.replace(&#123;'General purpose image/video tools (PIL, cv2, skimage, etc)':'General image/video', 'Image segmentation methods (U-Net, Mask R-CNN, etc)':'Image segmentation methods', 'Object detection methods (YOLOv3, RetinaNet, etc)':'Object detection methods', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)':'Image classification &amp; networks', 'Generative Networks (GAN, VAE, etc)':'Generative Networks'&#125;, inplace=True) India7.replace(&#123;'General purpose image/video tools (PIL, cv2, skimage, etc)':'General image/video', 'Image segmentation methods (U-Net, Mask R-CNN, etc)':'Image segmentation methods', 'Object detection methods (YOLOv3, RetinaNet, etc)':'Object detection methods', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)':'Image classification &amp; networks', 'Generative Networks (GAN, VAE, etc)':'Generative Networks'&#125;, inplace=True) USA7.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('Basic Program Language in USA', fontsize=20) ax[0].set_ylabel('') India7.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('Basic Program Language in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() Both countries use Python the most. The next is SQL. A noticeable difference is shown in the 3rd most used language. In the USA, 14.6% of respondents use R, but in India the 3rd most used programming language is C. In India, R is the third- lowest programming language used whereas in the USA, C is the second-lowest. Nevertheless, it is clear that the most commonly used languages of both countries are Python and SQL. 4-3. Recommend programming language for starterdef countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\") for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 4500]) plt.xticks(rotation=0, fontsize=8) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Recommend Language for Starter', fontsize=15) plt.show() q8_order = [] col_name = \"Q8\" countplot_(df, col_name, q8_order) The graph shows that Python has an overwhelming proportion. What is unusual is that Indians who use relatively less R recommended R. 4-4. Integrated development environments (multiple)# Preprocess integrated development environments data USA9 = (USA['Q9_Part_1'], USA['Q9_Part_2'], USA['Q9_Part_3'], USA['Q9_Part_4'], USA['Q9_Part_5'], USA['Q9_Part_6'], USA['Q9_Part_7'], USA['Q9_Part_8'], USA['Q9_Part_9'], USA['Q9_Part_10'], USA['Q9_Part_11'], USA['Q9_OTHER']) USA9 = pd.concat(USA9) India9 = (India['Q9_Part_1'], India['Q9_Part_2'], India['Q9_Part_3'], India['Q9_Part_4'], India['Q9_Part_5'], India['Q9_Part_6'], India['Q9_Part_7'], India['Q9_Part_8'], India['Q9_Part_9'], India['Q9_Part_10'], India['Q9_Part_11'], India['Q9_OTHER']) India9 = pd.concat(India9) mpl.rcParams['font.size'] = 13 USA9.replace(&#123;'Jupyter (JupyterLab, Jupyter Notebooks, etc) ':'Jupyter', 'Visual Studio Code (VSCode)':'VSCode'&#125;, inplace=True) India9.replace(&#123;'Jupyter (JupyterLab, Jupyter Notebooks, etc) ':'Jupyter', 'Visual Studio Code (VSCode)':'VSCode'&#125;, inplace=True) f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA9.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('Using IDE in USA', fontsize=20) ax[0].set_ylabel('') India9.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('Using IDE in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() The development environment used frequently is similar except for RStudio. As we saw in 4-2, the reason why the ratio of RStudio in India is not high is Indian less use R. 5. Basic of Machine Learning It’s time to check the machine learning section. First, I check to period of using machine learning in each country. And then, let’s look at the machine learning framework and algorithms that they use. 5-1. Period of Using Machine Learningdf['Q15'].replace(&#123;'Under 1 year':'&lt; 1 year', '20 or more years':'20+ years', 'I do not use machine learning methods':'Never'&#125;, inplace=True) def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['&lt; 1 year','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20+ years','Never']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 2400]) plt.xticks(rotation=0, fontsize=8) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Period of Using Machine Learning', fontsize=15) plt.show() q15_order = ['&lt; 1 year','1-2 years','2-3 years','3-4 years','4-5 years','5-10 years','10-20 years','20+ years','Never'] col_name = \"Q15\" countplot_(df, col_name, q15_order) df_normalize15 = df.groupby(['Q3'])['Q15'].value_counts(dropna=False, normalize=True, ascending=False) df_normalize15 = pd.DataFrame(df_normalize15) df_normalize15 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Q15 Q3 Q15 India &lt; 1 year 0.395224 NaN 0.201938 1-2 years 0.179270 Never 0.100536 2-3 years 0.060391 3-4 years 0.025091 4-5 years 0.017996 5-10 years 0.015747 10-20 years 0.003461 20+ years 0.000346 USA &lt; 1 year 0.213790 1-2 years 0.159186 NaN 0.127256 Never 0.116613 2-3 years 0.109671 5-10 years 0.084683 3-4 years 0.071263 4-5 years 0.068024 10-20 years 0.027765 20+ years 0.021749 The results of this graph shows the two countries similar. In the USA, many people have been coding for a long time, but there are many beginners in machine learning. Expecially in India, there are about 57.4% of respondents using machine learning. It can be said that India’s coding craze began with the development of machine learning. 5-2. Using machine learning framework (multiple)# Preprocess machine learning framework data USA16 = (USA['Q16_Part_1'], USA['Q16_Part_2'], USA['Q16_Part_3'], USA['Q16_Part_4'], USA['Q16_Part_5'], USA['Q16_Part_6'], USA['Q16_Part_7'], USA['Q16_Part_8'], USA['Q16_Part_9'], USA['Q16_Part_10'], USA['Q16_Part_11'], USA['Q16_Part_12'], USA['Q16_Part_13'], USA['Q16_Part_14'], USA['Q16_Part_15'], USA['Q16_OTHER']) USA16 = pd.concat(USA16) India16 = (India['Q16_Part_1'], India['Q16_Part_2'], India['Q16_Part_3'], India['Q16_Part_4'], India['Q16_Part_5'], India['Q16_Part_6'], India['Q16_Part_7'], India['Q16_Part_8'], India['Q16_Part_9'], India['Q16_Part_10'], India['Q16_Part_11'], India['Q16_Part_12'], India['Q16_Part_13'], India['Q16_Part_14'], India['Q16_Part_15'], India['Q16_OTHER']) India16 = pd.concat(India16) mpl.rcParams['font.size'] = 13 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA16.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('Machine Learning Framework in USA', fontsize=20) ax[0].set_ylabel('') India16.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('Machine Learning Framework in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() The ranking of machine learning framework of both countries is almost similar. There is a difference between the rank 4th and 5th graphs which is PyTorch and Xgboost. The most popular framework used is Scikit-learn, with about 25.9% in the USA and about 27.6% in India. TensorFlow and Keras showed about 14.5%, and about 13.4% in the USA and in India about 19.0% and about 17.3% are observed. It shows that more than three quarters of people uses 5 types of framework and one quarter of people uses various framework which occupy less than about 10% each. 5-3. Using machine learning algorithm (multiple)# Preprocess machine learning algorithm data USA17 = (USA['Q17_Part_1'], USA['Q17_Part_2'], USA['Q17_Part_3'], USA['Q17_Part_4'], USA['Q17_Part_5'], USA['Q17_Part_6'], USA['Q17_Part_7'], USA['Q17_Part_8'], USA['Q17_Part_9'], USA['Q17_Part_10'], USA['Q17_Part_11'], USA['Q17_OTHER']) USA17 = pd.concat(USA17) India17 = (India['Q17_Part_1'], India['Q17_Part_2'], India['Q17_Part_3'], India['Q17_Part_4'], India['Q17_Part_5'], India['Q17_Part_6'], India['Q17_Part_7'], India['Q17_Part_8'], India['Q17_Part_9'], India['Q17_Part_10'], India['Q17_Part_11'], India['Q17_OTHER']) India17 = pd.concat(India17) mpl.rcParams['font.size'] = 10 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA17.replace(&#123;'Gradient Boosting Machines (xgboost, lightgbm, etc)':'Gradient Boosting Machines', 'Dense Neural Networks (MLPs, etc)':'Dense Neural Networks', 'Transformer Networks (BERT, gpt-3, etc)':'Transformer Networks'&#125;, inplace=True) India17.replace(&#123;'Gradient Boosting Machines (xgboost, lightgbm, etc)':'Gradient Boosting Machines', 'Dense Neural Networks (MLPs, etc)':'Dense Neural Networks', 'Transformer Networks (BERT, gpt-3, etc)':'Transformer Networks'&#125;, inplace=True) USA17.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('Machine Learning Algorithm in USA', fontsize=20) ax[0].set_ylabel('') India17.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('Machine Learning Algorithm in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() The ranking of used machine learning algorithm of both countries is almost similar, too. Linear or logistic regression and decision trees or random forests are most popular in two countries. And there are no significant thing because the result of both countries is alike. 6. CV &amp; NLP Then, I check about Computer Vision and Natural Language Processing because it is importance part of machine learning. Therefore I assumed lots of beginners and exports are interested in this field. In this part, the difference between the USA and India is not visible clearly, so I just show you the ratio graphs. 6-1. Computer Vision (multiple)# Preprocess CV data USA18 = (USA['Q18_Part_1'], USA['Q18_Part_2'], USA['Q18_Part_3'], USA['Q18_Part_4'], USA['Q18_Part_5'], USA['Q18_Part_6'], USA['Q18_OTHER']) USA18 = pd.concat(USA18) India18 = (India['Q18_Part_1'], India['Q18_Part_2'], India['Q18_Part_3'], India['Q18_Part_4'], India['Q18_Part_5'], India['Q18_Part_6'], India['Q18_OTHER']) India18 = pd.concat(India18) # Preprocess NLP data USA19 = (USA['Q19_Part_1'], USA['Q19_Part_2'], USA['Q19_Part_3'], USA['Q19_Part_4'], USA['Q19_Part_5'], USA['Q19_OTHER']) USA19 = pd.concat(USA19) India19 = (India['Q19_Part_1'], India['Q19_Part_2'], India['Q19_Part_3'], India['Q19_Part_4'], India['Q19_Part_5'], India['Q19_OTHER']) India19 = pd.concat(India19) mpl.rcParams['font.size'] = 10 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA18.replace(&#123;'General purpose image/video tools (PIL, cv2, skimage, etc)':'General image/video', 'Image segmentation methods (U-Net, Mask R-CNN, etc)':'Image segmentation methods', 'Object detection methods (YOLOv3, RetinaNet, etc)':'Object detection methods', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)':'Image classification &amp; networks', 'Generative Networks (GAN, VAE, etc)':'Generative Networks'&#125;, inplace=True) India18.replace(&#123;'General purpose image/video tools (PIL, cv2, skimage, etc)':'General image/video', 'Image segmentation methods (U-Net, Mask R-CNN, etc)':'Image segmentation methods', 'Object detection methods (YOLOv3, RetinaNet, etc)':'Object detection methods', 'Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)':'Image classification &amp; networks', 'Generative Networks (GAN, VAE, etc)':'Generative Networks'&#125;, inplace=True) USA18.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('CV Method in USA', fontsize=20) ax[0].set_ylabel('') India18.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('CV Method in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() 6-2. Natural Language Processing (multiple)mpl.rcParams['font.size'] = 10 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA19.replace(&#123;'Word embeddings/vectors (GLoVe, fastText, word2vec)':'Word embeddings/vectors', 'Encoder-decorder models (seq2seq, vanilla transformers)':'Encoder-decoder models', 'Contextualized embeddings (ELMo, CoVe)':'Contextualized embeddings', 'Transformer language models (GPT-3, BERT, XLnet, etc)':'Transformer language models'&#125;, inplace=True) India19.replace(&#123;'Word embeddings/vectors (GLoVe, fastText, word2vec)':'Word embeddings/vectors', 'Encoder-decorder models (seq2seq, vanilla transformers)':'Encoder-decoder models', 'Contextualized embeddings (ELMo, CoVe)':'Contextualized embeddings', 'Transformer language models (GPT-3, BERT, XLnet, etc)':'Transformer language models'&#125;, inplace=True) USA19.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('NLP Method in USA', fontsize=20) ax[0].set_ylabel('') India19.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('NLP Method in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() 7. Machine Learning in the business I wondered if the company actually uses machine learning in their business, so I choose this question. Unfortunately, there are few answers that could be used because there are many missing values. df = df.dropna(subset=['Q22']) # drop NaN df['Q22'].replace(&#123;'We are exploring ML methods (and may one day put a model into production)':'Exploring step', 'We use ML methods for generating insights (but do not put working models into production)':'Use for insights', 'We recently started using ML methods (i.e., models in production for less than 2 years)':'Started using ML methods', 'We have well established ML methods (i.e., models in production for more than 2 years)':'Have well established ML methods', 'No (we do not use ML methods)':'Do not use', 'I do not know':'Do not know'&#125;, inplace=True) def countplot_(data, col_name, q_order): values = data[col_name].value_counts()[q_order].values ax = sns.countplot(x = col_name, hue=data.columns[3], data=data, hue_order = legend_list, palette = \"husl\", order = ['Exploring step','Use for insights','Started using ML methods','Have well established ML methods','Do not use','Do not know']) for p in ax.patches: height = p.get_height() ax.text(p.get_x() + p.get_width()/2., height+3, height, ha='center', size=6) ax.set_ylim([0, 550]) plt.xticks(rotation=20, fontsize=8) plt.xlabel('') plt.yticks(fontsize=8) plt.ylabel('') plt.legend(fontsize=8, loc='upper right') plt.title('Using ML in the business', fontsize=15) plt.show() q22_order = ['Exploring step','Use for insights','Started using ML methods','Have well established ML methods','Do not use','Do not know'] col_name = \"Q22\" countplot_(df, col_name, q22_order) df_normalize22 = df.groupby(['Q3'])['Q22'].value_counts(dropna=False, normalize=True, ascending=False) df_normalize22 = pd.DataFrame(df_normalize22) df_normalize22 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Q22 Q3 Q22 India Exploring step 0.217248 Have well established ML methods 0.179466 Do not know 0.177823 Started using ML methods 0.165092 Do not use 0.156468 Use for insights 0.103901 USA Have well established ML methods 0.262517 Exploring step 0.165088 Started using ML methods 0.163058 Do not use 0.146143 Do not know 0.132612 Use for insights 0.130582 There are so many NaN. I think they didn’t know about it, so they just skipped that question. I dropped all NaN in this question. In the USA, there are about 72.3% of respondents’s firm use machine learning for their business. In India there are about 66.4% of respondents’s firm use it for their business. It is noticeable that the ratio of using machine learning in the business in both counrty is not big different. However, it may be an uncertain value because all missing values are deleted. Therefore if you wondered about it, I strongly recommend find some useful reports. 8. Want to learn machine learning product (multiple) Finally, I comfirm that the machine learning products what Americans and Indians want to learn. Because the importance and potential of machine learning is continuosly emphasized, I feel like it help us to understand the trend of indusrty. # Preprocess data USA28b = (USA['Q28_B_Part_1'], USA['Q28_B_Part_2'], USA['Q28_B_Part_3'], USA['Q28_B_Part_4'], USA['Q28_B_Part_5'], USA['Q28_B_Part_6'], USA['Q28_B_Part_7'], USA['Q28_B_Part_8'], USA['Q28_B_Part_9'], USA['Q28_B_OTHER']) USA28b = pd.concat(USA28b) India28b = (India['Q28_B_Part_1'], India['Q28_B_Part_2'], India['Q28_B_Part_3'], India['Q28_B_Part_4'], India['Q28_B_Part_5'], India['Q28_B_Part_6'], India['Q28_B_Part_7'], India['Q28_B_Part_8'], India['Q28_B_Part_9'], India['Q28_B_OTHER']) India28b = pd.concat(India28b) mpl.rcParams['font.size'] = 10 f, ax = plt.subplots(1, 2, figsize = (12, 8)) USA28b.replace(&#123;' Azure Machine Learning Studio ':'Azure ML Studio', ' Google Cloud AI Platform / Google Cloud ML Engine':'Google Cloud AI Platform/ML Engine'&#125;, inplace=True) India28b.replace(&#123;' Azure Machine Learning Studio ':'Azure ML Studio', ' Google Cloud AI Platform / Google Cloud ML Engine':'Google Cloud AI Platform/ML Engine'&#125;, inplace=True) USA28b.value_counts().plot.pie(autopct='%.1f%%', ax = ax[0]) ax[0].set_title('Want to learn in USA', fontsize=20) ax[0].set_ylabel('') India28b.value_counts().plot.pie(autopct='%.1f%%', ax = ax[1]) ax[1].set_title('Want to learn in India', fontsize=20) ax[1].set_ylabel('') plt.tight_layout() plt.show() Everything was chosen by similarly ratio. Above all, Google Cloud AI Platform / Google Cloud ML Engine is the most popular among beginners in two countries. ConclusionThrough exploratory data analysis of 《2020 Kaggle Machine Learning &amp; df Science Survey》, we were able to learn about trends of coding and machine learning. Comparing with an obvious IT powerhouse country, the USA and the rising IT powerhouse India is very meaningful to explore trends of it. Most noticeable is that the USA has been interested in coding and programs from long ago, and India is a country with coding craze especially among young ages. It is amazing that the two countries’ response are very similar in the part of machine learning surveys. I think it is proven the importance of machine learning these days. It was an interesting work.","categories":[{"name":"Project","slug":"Project","permalink":"https://ne-choi.github.io/categories/Project/"},{"name":"Kaggle","slug":"Project/Kaggle","permalink":"https://ne-choi.github.io/categories/Project/Kaggle/"}],"tags":[{"name":"EDA","slug":"EDA","permalink":"https://ne-choi.github.io/tags/EDA/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://ne-choi.github.io/tags/Kaggle/"},{"name":"Kaggle beginner","slug":"Kaggle-beginner","permalink":"https://ne-choi.github.io/tags/Kaggle-beginner/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 1: Ⅲ. 데이터 시각화와 통계적 해석","slug":"Study/Postech/통계학입문/Ⅲ_데이터_시각화와_통계적_해석","date":"2021-01-06T00:00:00.000Z","updated":"2021-02-01T00:58:40.495Z","comments":true,"path":"/2021/01/06/Study/Postech/통계학입문/Ⅲ_데이터_시각화와_통계적_해석/","link":"","permalink":"https://ne-choi.github.io/2021/01/06/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A2_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%8B%9C%EA%B0%81%ED%99%94%EC%99%80_%ED%86%B5%EA%B3%84%EC%A0%81_%ED%95%B4%EC%84%9D/","excerpt":"","text":"setup, includeknitr::opts_chunk$set(echo &#x3D; TRUE) POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다. Ⅲ. 데이터 시각화와 통계적 해석1. 데이터 시각화데이터 분석단계 수집 → 정제 → 시각화 → 예측모형/분석 2. 그래프의 유용성과 오류그래프의 유용성 그래프는 데이터 시각화의 일종 그래프의 올바른 해석은 데이터사이언티스트의 필수 능력이자 커뮤니케이션 도구 히스토그램 같은 분산이라도 데이터 분포를 더 잘 파악할 수 있음 계급 구간 설정에 따라 히스토그램 그래프가 완전히 달라짐 데이터 시각화 주의할 점 그래프 목적은 데이터를 분명하게 표현하는 것 그래프 작성 시, 축의 범위와 간격 등을 잘 정해야 함 그래프를 보는 사람의 수준을 고려해야 함 그래프 종류별 장단점을 정확히 파악하고 사용해야 함 퀴즈 오답 히스토그램을 통해 알 수 있는 것 데이터의 분포 범위 이상치(Outlier)의 존재 유무 3. 상자그림이 주는 정보와 해석상자그림이 필요한 이유? 평균과 분산(기술통계치)만으로는 부족함 평균, 분산을 안다고 데이터가 어느 쪽에 더 많이 분포하는지 알 수 없음 이상치 존재 여부를 알 수 없음 데이터 분포 범위(최대값, 최소값)를 한눈에 알기 어려움 상자그림이 주는 정보 한눈에 5가지 정보를 제공 중앙값, 일사분위수, 삼사분위수, 최대값, 최소값 데이터 분포의 대칭성, 치우침, 이상치를 쉽게 파악할 수 있음 상자그림 그리는 방법 데이터의 중앙값(median)을 찾는다. 중앙값이란? n개의 관측치를 오름차순으로 배열했을 때, 중앙 위치에 놓이는 값 데이터 수가 작고 이상치가 있을 때, 평균보다 더 정확한 모집단의 중심값이 됨 일사분위수(Q1)와 삼사분위수(Q3)을 찾는다. 일사분위수(Q1) 데이터를 크기 순서로 배열했을 때, 25% 지점 값 삼사분위수(Q3) 데이터를 크기 순서로 배열했을 때, 75% 지점 값 일사분위수 ~ 삼사분위수를 상자로 그린다. (사분위범위) 최소값 ~ 일사분위수, 삼사분위수 ~ 최대값을 그린다. 이상치를 표시한다. 일사분위로부터 -(1.5)*사분위범위를 넘는 관측치는 이상치로 표시 삼사분위로부터 +(1.5)*사분위범위를 넘는 관측치는 이상치로 표시 요약 상자그림은 다섯 가지 숫자로 데이터를 요약한 그래프 가운데 상자는 Q1에서 Q1까지 그림 상자 안의 선은 중앙값을 나타냄 상자 밖 선은 최대값과 최소값까지 이어짐 상자와 수염 밖 데이터는 이상치 4. 산점도와 상관관계: 트렌드 분석산점도 필요 이유 지금까지 히스토그램, 상자그림으로 변수 1개의 데이터 분포를 살펴봄 두 변수 사이 관계를 아는 방법은? 산점도(Scatter plot) 변수 간 관계 방향, 트렌드, 강도를 알 수 있음 산점도의 x축과 y축은 독립변수와 종속변수로 이루어짐 독립변수: 원인 역할을 하는 변수, X 종속변수: 결과 관측 변수, Y ex. 학점 - 공부 시간 산점도로부터 알 수 있는 3가지 트렌드: linear, curved, clusters, no pattern 방향: positie, negative, no direction 강도: how closely the points fit the trend 산점도 해석: 방향 양의 상관관계(Positively associated) 두 변수 X와 Y가 X값이 클 때 Y값도 큰 경향이 있고, X값이 작을 때 Y값도 작은 경향 음의 상관관계(Negatively associated) 두 변수 X와 Y가 X값이 클 때 Y값은 작은 경향이 있고, X값이 작을 때 Y값은 큰 경향 산점도 해석: 강도 상관계수(Correlation, r) r은 -1부터 +1까지 존재 +1에 가까울수록 강한 양의 상관관계 -1에 가까울수록 강한 음의 상관관계 0은 가장 약한 상관관계(상관관계가 없음) 산점도에서 의미하는 상관관계: 선형적인 상관관계만을 말함 정리 산점도는 두 변수간 관계 방향, 형태, 강도를 살펴볼 수 있는 그래프 상관계수(r)는 두 변수간 선형적인 상관관계의 강도를 나타냄 산점도에서 선형모형(선형함수식)을 구현할 수 있음","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"통계","slug":"통계","permalink":"https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"},{"name":"데이터시각화","slug":"데이터시각화","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 1: Ⅱ. 빅데이터 탐색의 첫걸음","slug":"Study/Postech/통계학입문/Ⅱ_빅데이터_탐색의_첫걸음","date":"2021-01-05T00:00:00.000Z","updated":"2021-02-01T00:58:44.309Z","comments":true,"path":"/2021/01/05/Study/Postech/통계학입문/Ⅱ_빅데이터_탐색의_첫걸음/","link":"","permalink":"https://ne-choi.github.io/2021/01/05/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A1_%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89%EC%9D%98_%EC%B2%AB%EA%B1%B8%EC%9D%8C/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다. Ⅱ. 빅데이터 탐색의 첫걸음1. 데이터의 평균(중심위치)평균 평균은 데이터를 하나의 값으로 표현한 요약된 정보: 추정치 평균 = 데이터 값의 총합 / 데이터 개수 평균 다룰 때 주의할 점 평균은 혼자 존재하는 개념이 아님 평균과 표본선정 표본선정에 따라 평균값이 달라짐 ex. 대기업 평균 연봉 조사 표본 A: 연령대별로 각 50명씩 선정 표본 B: 50대에서 200명을 선정 조사된 평균값이 모집단을 대표하는 통계치라고 할 수 있는가? 표본이 적합하게 추출되었는지 평가하는 법 편의(bias)가 적은가: 표본으로부터 얻어지는 통계치(표본평균)의 기대값이 모수의 참값과 유사한가? 정확도(precision)가 높은가: 반복해서 표본 추출 시, 얼마나 유사한 값이 나오는가? 평균과 분산 같은 평균이라도 분산이 다르면 데이터 특성이 달라짐 평균값은 그 집단에서 가장 많이 존재하는 것이 아님 데이터: 1, 2, 2, 7 평균: 3 데이터의 중심척도 평균(mean) 표본이 적은 경우, 아주 큰 값이나 작은 값(outlier)에 민감한 추정치 중앙값이 평균보다 더 적합한 중심척도인 경우도 있음 중앙값(median) n개의 관측치를 크기순으로 배열했을 때, 중앙 위치에 놓이는 값 데이터 수가 작고 이상치(outlier)가 있을 때, 평균보다 더 정확한 모집단의 중심값이 됨 최빈값(mode) 전체 데이터 중, 가장 빈도(frequency)가 높은 값 데이터 수가 많아질수록 평균과 가까워짐 2. 데이터의 분산(산포정도)어느 집단 분산이 클까? 평균만 아는 사람 vs 평균과 표준편차를 아는 사람 평균 연봉은 같지만 편차가 큰 경우, 편차가 적은 기업에 비해 초봉이 낮고 승진 시 월급이 높아짐 분산 공식 데이터 평균과 데이터간 거리 합으로 분산 계산 데이터: x1, x2, …, xn 평균: Xbar 편차: (x1 - xbar),(x2 - xbar),…,(xn - xbar) 편차들의 합: (x1 - xbar) + (x2 - xbar) + … + (xn - xbar) = ? - 데이터가 평균으로부터 대칭적으로 존재할 경우, 편차들의 합이 0 → **편차를 제곱하여 더함** - 분산 = 편차들의 제곱합을 (n-1)*로 나눔 - (n-1)로 나누는 이유: 자유도와 관련, 평균값으로 표본평균을 사용하므로 1개의 자유도를 잃게 되어 (n-1)로 나눔 표준편차 (개별데이터값 - 평균값) 차이를 제곱하여 더하였으므로 값이 커지고 단위가 달라짐→ 분산에 제곱근을 취하여 원래 단위로 복원 → 표준편차라고 부름 분산의 의미 분산: 데이터가 분포되어있는 정도 데이터에 대한 요약 정보 보완 평균값만으로는 데이터 상상이 어려움 3. 데이터와 빅데이터데이터란? 모든 숫자를 데이터라고 할 수 있을까? No 데이터: 구조화된 데이터 다차원 배열(매트릭스) 각 열의 형식이 다른 표 or 스프레드시트 탭이나 텍스트파일 형식으로 저장 데이터화(Datafication) 기계가 읽어들일 수 있는 모든 것(숫자, 이미지, 텍스트)을 데이터로 변환하는 것 개인의 활동을 실시간으로 추적해 이를 예측분석이 가능한 수량화된 온라인 데이터로 변환하는 것을 의미 빅데이터란 Volume(양) Velocity(속도) Variety(다양성) 4. 데이터 탐색의 첫걸음통계치로 인사이트 얻기 ㅇㅇ회사 공채에 합격하기 위해 합격자 평균 분석 사람들이 선호하는 기업 문화 알아보기 최적의 의사결정: 데이터탐색 공정에 대한 평균, 산포, 불량률 추정 품질 변동상황을 관리도(control chart)로 표현 공정에 발생하는 이상요인을 빨리 탐지하여 수정조치 → 불량 사전 예방 공정에서 정상범위 관리도 차트 중심선, 관리상한선, 관리하한선을 어떻게 정할 것인가? 숨겨진 패턴 분석: 분류 암과 정상 뇌 영상을 숫자화(데이터화)함 분류(암/정상)를 가장 잘 구분하는 변수를 찾고, 범주간 차이를 가장 잘 표현하는 새로운 함수를 구함 새로운 환자 영상을 보고 어느 범주에 더 가까운지를 판별하여 암 여부를 진단 두 범주가 잘 분류된다는 것: 두 범주가 겹치지 않으면서 두 범주 중심위치가 가능한 먼 것 트렌드 분석: 웹 마이닝 1년간 검색어 트렌드 분석 Moving Average를 통한 트렌드 파악","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"평균","slug":"평균","permalink":"https://ne-choi.github.io/tags/%ED%8F%89%EA%B7%A0/"},{"name":"분산","slug":"분산","permalink":"https://ne-choi.github.io/tags/%EB%B6%84%EC%82%B0/"}],"author":"ne-choi"},{"title":"데이터사이언스를 위한 통계학입문 1: Ⅰ. 데이터과학과 통계","slug":"Study/Postech/통계학입문/Ⅰ_데이터과학과_통계","date":"2021-01-04T00:00:00.000Z","updated":"2021-02-01T00:58:48.715Z","comments":true,"path":"/2021/01/04/Study/Postech/통계학입문/Ⅰ_데이터과학과_통계/","link":"","permalink":"https://ne-choi.github.io/2021/01/04/Study/Postech/%ED%86%B5%EA%B3%84%ED%95%99%EC%9E%85%EB%AC%B8/%E2%85%A0_%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B3%BC%ED%95%99%EA%B3%BC_%ED%86%B5%EA%B3%84/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 데이터사이언스를 위한 통계학입문 Ⅰ 과정입니다. Ⅰ 데이터과학과 통계1. 데이터과학이란 무엇인가데이터과학이란? 빅데이터: 통계학, 데이터마이닝, 인공지능, 딥러닝 Data → Data Analytics → Insight 필요한 기술 통계적 개념과 지식: 샘플링, 확률분포, 가설검정, p-value 데이터 다루는 기술(데이터 큐레이션): 빅데이터 다루기 데이터 요약된 정보 전달 기술: 데이터 시각화(공간지도분석, 다차원그래픽) 데이터윤리, 데이터보안 데이터 도메인 지식, 분석 능력: 현실 문제 해결 능력 데이터 과학 예시 핀란드 의료데이터 프로젝트: FinnGen 핀란드인(Finnish) + 유전자(Genome) 자발적 참여자의 유전자정보 수집, 확자의 의료정보까지 통합 구축 50만명 목표, 23만명 수집, 15만명 유전자 정보 보호 6개월마다 데이터 업데이트: 전세계 연구자와 공유 관절염/당뇨병 등 자가면역질환 연구 수행 공부할 내용 데이터과학을 위한 통계 개념과 지식 공유데이터와 오픈소스 빅데이터분석을 위한 첫걸음: 데이터 중심 위치, 산포 정도 확인 데이터 시각화 데이터과학에서 확률분포 의미 2. 통계가 상식이 된 사회통계가 왜 필요한가? 통계는 올바른 의사결정을 도움 빅데이터 → 요약된 정보 제공, 과거 데이터로 미래 예측, 데이터에 숨겨진 패턴 발견 → 올바른 의사결정 의사결정에서 통계의 역할 ex. 고객 이동경로 분석 결과 활용 예시 고객 금융 검색 경로 추적 고객 이동경로(customer journey) 분석 이탈 가능성 높은 고객을 붙잡고 신규 고객 유입하는 데 사용 정부 정책 근거자료: 영국의회 노령연금 도입 1886년 영국 사회학자 찰스 부스 산업혁명으로 부유해진 런던에서 시민 빈곤 상황을 12년간 조사 가난을 8단계로 분류하여 절대빈곤이 30.7%에 달한다는 결과 발표 1908년 영국의회 노령연금 도입 서울시 심야버스 노선정책 자정 ~ 새벽 5시까지의 시민 휴대폰 전화 데이터 수집 유동인구 분포 및 밀도 파악하여 심야버스 노선 수립 요약 개인의 일상활동은 데이터화되어 예측분석이 가능하도록 수량화, 객관화됨 통계는 수많은 데이터로부터 요약된 정보 제공, 미래 데이터 예측, 숨겨진 패턴 발견 → 올바른 의사결정을 하게 함 통계치는 금융권 관리전략, 정부 정책 수립, 법정소송 시 근거자료로 활용될 수 있음 3. 데이터 분선과 윤리데이터 정직성 데이터 정직성? 데이터가 잘못되지 않았는지를 나타냄 ex. 한강 수질 검사 한강에서 물을 한웅큼 채취 방 안에 있는 보온병에 보관 수질 검사를 하면 그 결과를 신뢰할 수 있는가? 데이터를 잘못 수집했다 한강이 넓은데 어디서 수집?: 표본의 수 어제 산성비가 내렸을 수 있음: 데이터 수집 기간/시기 손으로 채집하여 오염 가능성: 수집 방법 데이터를 잘못 보관했다 보온병 세균이 옮으면? 차가운 데 보관해야 하는 것 아닌지? 다른 이가 손대지 못하게 안전히 보관해야 함: data storage 기사 내 통계치 해석/평가 ex. 비정규직 vs 정규직 임금 격차 동등한 조건으로 비교되었는지 성, 연령, 근속년수 등 요인이 통제된 상태의 시급으로 비교되었는지 ex. 세무사 월 최고 소득 편향된 표본 추출 세무사 39명 조사, 우연히 연봉 3~4억원 대인 자영업 세무사가 많았음 요약 데이터과학 윤리-데이터를 올바르게 분석할 뿐 아니라 올바른 방법으로 수집해야 함 정직하지 못한 데이터의 주요 원인 데이터분석자의 무지함, 비윤리성, 환경의 제약 데이터 수집 시, 너무 적은 양의 데이터, 편향된 표본 추출, 데이터의 왜곡 및 훼손에 주의 결측치 문제 고려 4. 공유데이터와 오픈소스(github)공유데이터 공유데이터? 모든사람이 자유롭게 사용/재사용/재배포 가능한 데이터 이용성 및 접근성 재사용과 재배포 보편적 참여 공유데이터 서비스 머신러닝 기법 분석에 활용 가능한 데이터 저장소 Machine Learning Repository in UC, Irvine 정부 제공 통계청 공공데이터포털 서울열린데이터광장 네이버 네이버 데이터랩: 국내 공공데이터를 기관별로 분류하여 접근성을 높임 오픈소스 오픈소스? 저작권자가 소스코드를 공개하여 누구나 복제, 개작, 배포할 수 있는 소프트웨어 R, Python: 오픈소스 통계분석 프로그램 C++, JAVA, Python 등 다른 프로그래밍 언어와 쉽게 연동 빅데이터 시스템인 스파크와도 일부 기능을 연동함으로써 응용범위가 더욱 넓어짐 인공지능에서의 오픈소스 텐서플로우: 구글에서 머신러닝과 신경망 연구를 위해 만튼 소프트웨어 딥마인드랩: 구글 딥마인드에서 공개한 인공지능 개발 플랫폼 공유데이터 vs 오픈소스 공유데이터: 단순히 수치로 표현되는 측정치 또는 결괏값 오픈소스: 단순 데이터가 아닌 지적 창작물 github git: 프로그램 등 소스 코드 관리를 위한 분산 관리 툴 github은 git에 프로젝트 관리지원기능을 확장한 웹 호스팅 서비스 2008년 미국 github사에서 서비스 시작 2018년 마이크로소프트가 인수","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"통계","slug":"통계","permalink":"https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 2: ⅩⅥ. 딥러닝과 텍스트 마이닝","slug":"Study/Postech/머신러닝R/ⅩⅥ_딥러닝과_텍스트_마이닝","date":"2020-12-24T00:00:00.000Z","updated":"2021-02-01T01:09:58.633Z","comments":true,"path":"/2020/12/24/Study/Postech/머신러닝R/ⅩⅥ_딥러닝과_텍스트_마이닝/","link":"","permalink":"https://ne-choi.github.io/2020/12/24/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A5_%EB%94%A5%EB%9F%AC%EB%8B%9D%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%A7%88%EC%9D%B4%EB%8B%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다. ⅩⅥ. 딥러닝과 텍스트 마이닝1. Neural NetworksConcepts 인공신경망은 기계학습(Machine Learning)의 통계적 학습 알고리즘 중 하나 컴퓨터 비전, 자연어 처리, 음석 인식 등 영역에서 사용 AI ⊂ Machine Learning ⊂ Neural Network 신경망 모델(Neural Network) Perceptron을 한 단위로 하는 네트워크를 구축하여, 인간의 신경세포(Neuron)와 유사한 기능을 하도록 제안 Perceptron - Single Layer 하나의 Perceptron은 단순히 다수의 입력과 가중치의 선형 결합을 계산하는 역할을 수행 Activation 함수에 따라 선형결합으로 생성되는 출력값이 결정 Binary Threshold Sigmoid ReLU(Rectified Linear Unit) tanh Multi-layer perceptron Perceptron으로 구성된 Single Layer들이 Multi-layer를 만듦 Input layer와 Output layer 사이에는 Hidden layer가 존재하여 Non-linear transformation 수행 Output layer에서 Softmax 함수를 통해 가장 큰 값을 손쉽게 알 수 있음 Exponential 함수로 항상 양수 결과치가 도출되고 이를 통해 확률값을 도출 Neural Network 수행 mxnet 패키지는 R4.0 버전 이후부터 실행 불가 R 3.6.2버전 설치해서 실행 필요# require mxnet install.packages(&quot;https:&#x2F;&#x2F;github.com&#x2F;jeremiedb&#x2F;mxnet_winbin&#x2F;raw&#x2F;master&#x2F;mxnet.zip&quot;,repos &#x3D; NULL) library(mxnet) # If you have Error message &quot;no package called XML or DiagrmmeR&quot;, then install install.packages(&quot;XML&quot;) install.packages(&quot;DiagrammeR&quot;) #library(XML) #library(DiagrammeR) #devtools::install_github(&quot;rich-iannone&#x2F;DiagrammeR&quot;) #install.packages(&#39;devtools&#39;) #library(devtools) 데이터 불러오기# read csv file iris&lt;-read.csv(&quot;data&#x2F;week16_1&#x2F;iris.csv&quot;) attach(iris) # change to numeric from character variable : Species iris[,5] &#x3D; as.numeric(iris[,5])-1 table(Species) # check the data head(iris, n&#x3D;10) 학습데이터와 검증데이터# split train &amp; test dataset # training (n&#x3D;100)&#x2F; test data(n&#x3D;50) set.seed(1000,sample.kind&#x3D;&quot;Rounding&quot;) N&lt;-nrow(iris) tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # split train data and test data train&lt;-data.matrix(iris[tr.idx,]) test&lt;-data.matrix(iris[-tr.idx,]) # feature &amp; Labels # 객체별 Feature와 Label로 분리 # Label은 5번째 열에 위치 train_feature&lt;-train[,-5] trainLabels&lt;-train[,5] test_feature&lt;-test[,-5] testLabels &lt;-test[,5] Hidden Layer 구성# Build nn model # first layers require(mxnet) my_input &#x3D; mx.symbol.Variable(&#39;data&#39;) fc1 &#x3D; mx.symbol.FullyConnected(data&#x3D;my_input, num.hidden &#x3D; 200, name&#x3D;&#39;fc1&#39;) # 200개의 뉴런 형성 relu1 &#x3D; mx.symbol.Activation(data&#x3D;fc1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu1&#39;) # second layers fc2 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu1, num.hidden &#x3D; 100, name&#x3D;&#39;fc2&#39;) # 100개의 뉴런 형성 relu2 &#x3D; mx.symbol.Activation(data&#x3D;fc2, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu2&#39;) Output Layer 구성# third layers fc3 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu2, num.hidden &#x3D; 3, name&#x3D;&#39;fc3&#39;) # 3개로 분류(0,1,2)해야 하므로 3개의 Output 뉴런 생성 # softmax softmax &#x3D; mx.symbol.SoftmaxOutput(data&#x3D;fc3, name&#x3D;&#39;sm&#39;) # sofrmax 결과를 통해 가장 큰 값 선택 모델 학습# training mx.set.seed(123, sample.kind&#x3D;&quot;Rounding&quot;) device &lt;- mx.cpu() model &lt;- mx.model.FeedForward.create(softmax, optimizer &#x3D; &quot;sgd&quot;, # Stochastic Gradient Descent array.batch.size&#x3D;10, # 총 10개 그룹 num.round &#x3D; 500, learning.rate&#x3D;0.1, X&#x3D;train_feature, y&#x3D;trainLabels, ctx&#x3D;device, eval.metric &#x3D; mx.metric.accuracy, array.layout &#x3D; &quot;rowmajor&quot;, epoch.end.callback&#x3D;mx.callback.log.train.metric(100)) graph.viz(model$symbol) 모델 테스트# testing predict_probs &lt;- predict(model, test_feature, array.layout &#x3D; &quot;rowmajor&quot;) predicted_labels &lt;- max.col(t(predict_probs)) - 1 table(testLabels, predicted_labels) sum(diag(table(testLabels, predicted_labels)))&#x2F;length(predicted_labels) 2. Convolutional Neural NetworksFeatures 신경망 모델(Neural Net) 입력값으로 객체의 특성(feature)을 받고, 출력된 값과 실제 값을 비교하는 과정을 거침(지도학습; Supervised Learning) 하나의 이미지는 수많은 픽셀들이 모여 형성하며, 특정 색에 해당하는 특정 값을 가짐 이미지의 모든 픽셀값을 입력값으로 갖는 신경망 모델을 만들 수 있음 Intuitions 고해상도 이미지의 경우 특성 feature 수가 너무 많아짐 모든 뉴런이 모든 픽셀과 연결(fully connected)되어 있을 경우, 모델 학습에 큰 어려움이 있음 각 뉴런들이 이미지의 일부 특성 feature에만 연결될 수 있는 구조가 더 더 적합함 Convolution operation을 통해 구현 가능 Convolutional Neural Network Feed forward Network: xi^n^을 구함 Convilution Max Pooling Activation function Backpropagation: Error 최소화 Concolution Operation 임의의 값으로 설정된 filter가 전체 이미지 중 일부의 선형 결합을 계산 각각 결괏값은 하나의 Neuron이 되며, filter는 해당 Neuron의 가중치과 됨 결괏값의 사이즈를 정하기 위해서는 Stride, Padding, Depth 고려 필요 Stride: filter를 몇 칸 이동할지 결정 Padding: input 주변에 0으로 padding을 삽입 Depth number of filter: 3차원상의 neuron의 깊이를 결정 Pooling Convolutional Layer 사이에 Pooling Layer를 넣는 방법이 많이 사용됨 추출한 이미지에서 지역적인 부분 특징만 뽑아 다음 layer로 넘겨줌 이를 통해 ① 가중치의 수를 줄일 수 있으며, ② 과적합(overfitting)을 방지할 수 있음 대표적으로 가장 큰 값(Local Maxima)만을 뽑아내는 Max Pooling이 많이 사용됨 MNIST Modified National Institute of Standards and Technology 손으로 쓴 숫자를 인식하기 위해 사용되는 데이터 28x28 pixel(784)의 흑백 이미지(0-255)들이 있음 0부터 9까지 총 70,000개의 손글씨 이미지가 있음 CNN in R 신경망 모델 생성을 위한 패키지: mxnet# Load MNIST mn1 # 28*28, 1 channel images mn1 &lt;- read.csv(&quot;mini_mnist.csv&quot;) set.seed(123,sample.kind&#x3D;&quot;Rounding&quot;) N&lt;-nrow(mn1) tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # split train data and test data train_data&lt;-data.matrix(mn1[tr.idx,]) test_data&lt;-data.matrix(mn1[-tr.idx,]) # 0과 1 사이에 분포하도록 Nomalized(0: 검정색 &#x2F; 255: 흰색) test&lt;-t(test_data[,-1]&#x2F;255) features&lt;-t(train_data[,-1]&#x2F;255) labels&lt;-train_data[,1] # data preprocession features_array &lt;- features # 입력 데이터의 차원을 설정(픽셀*객체 개수) # ncol(features): 학습 데이터 수(866) dim(features_array) &lt;- c(28,28,1,ncol(features)) test_array &lt;- test dim(test_array) &lt;- c(28,28,1,ncol(test)) ncol(features) table(labels) Convolutional Layer 구성# Build cnn model # first conv layers my_input &#x3D; mx.symbol.Variable(&#39;data&#39;) conv1 &#x3D; mx.symbol.Convolution(data&#x3D;my_input, kernel&#x3D;c(4,4), stride&#x3D;c(2,2), pad&#x3D;c(1,1), num.filter &#x3D; 20, name&#x3D;&#39;conv1&#39;) relu1 &#x3D; mx.symbol.Activation(data&#x3D;conv1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu1&#39;) mp1 &#x3D; mx.symbol.Pooling(data&#x3D;relu1, kernel&#x3D;c(2,2), stride&#x3D;c(2,2), pool.type&#x3D;&#39;max&#39;, name&#x3D;&#39;pool1&#39;) # second conv layers conv2 &#x3D; mx.symbol.Convolution(data&#x3D;mp1, kernel&#x3D;c(3,3), stride&#x3D;c(2,2), pad&#x3D;c(1,1), num.filter &#x3D; 40, name&#x3D;&#39;conv2&#39;) relu2 &#x3D; mx.symbol.Activation(data&#x3D;conv2, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu2&#39;) mp2 &#x3D; mx.symbol.Pooling(data&#x3D;relu2, kernel&#x3D;c(2,2), stride&#x3D;c(2,2), pool.type&#x3D;&#39;max&#39;, name&#x3D;&#39;pool2&#39;) # fully connected fc1 &#x3D; mx.symbol.FullyConnected(data&#x3D;mp2, num.hidden &#x3D; 1000, name&#x3D;&#39;fc1&#39;) relu3 &#x3D; mx.symbol.Activation(data&#x3D;fc1, act.type&#x3D;&#39;relu&#39;, name&#x3D;&#39;relu3&#39;) fc2 &#x3D; mx.symbol.FullyConnected(data&#x3D;relu3, num.hidden &#x3D; 3, name&#x3D;&#39;fc2&#39;) # softmax sm &#x3D; mx.symbol.SoftmaxOutput(data&#x3D;fc2, name&#x3D;&#39;sm&#39;) # training mx.set.seed(100,sample.kind&#x3D;&quot;Rounding&quot;) device &lt;- mx.cpu() model &lt;- mx.model.FeedForward.create(symbol&#x3D;sm, optimizer &#x3D; &quot;sgd&quot;, array.batch.size&#x3D;30, num.round &#x3D; 70, learning.rate&#x3D;0.1, X&#x3D;features_array, y&#x3D;labels, ctx&#x3D;device, eval.metric &#x3D; mx.metric.accuracy, epoch.end.callback&#x3D;mx.callback.log.train.metric(100)) graph.viz(model$symbol) # test predict_probs &lt;- predict(model, test_array) predicted_labels &lt;- max.col(t(predict_probs)) - 1 table(test_data[, 1], predicted_labels) sum(diag(table(test_data[, 1], predicted_labels)))&#x2F;length(predicted_labels) 네트워크 시각화 함수: graph.viz(model$symbol)graph.viz(model$symbol) 3. 텍스트 마이닝텍스트 마이닝이란? 텍스트마이닝(Text mining) 웹페이지, 이메일, 소셜네트워크 기록 등 전자문서 파일로부터 특정 연관성(동시적으로 빈도 높은 단어 추출)을 분석하는 방법 다양한 방식의 알고리즘을 이용해 대용량의 텍스트로부터 트렌드와 관심어를 찾아내는 기법으로 사용 텍스트 마이닝 필요 패키지 Natural language processing install.packages(‘NLP’) text mining package install.packages(‘tm’) visualizing install.packages(‘wordcloud’) color displaying install.packages(‘RColorBrewer’) Korean processing install.packages(‘KoNLP’) import twitter data install.packages(‘twitteR’) # set library (set in order) library(NLP) library(tm) library(RColorBrewer) library(wordcloud) 사용 데이터: tm에 포함된 crude# 20 new articles from Reuter- 21578 data set data(crude) # To know abour crude data help(crude) 텍스트 마이닝- 함수 함수|설명 및 예제코드|—|—|str(x[[1]])|데이터 구조 정보(첫 번째 파일 구조) / str(crude[[1]])content(x[[1]])|문서 내용(첫 번째 분서 내용)meta(X)|메타정보(x에 기록된 저자, 날짜, id 등 정보) / meta(ctude[[1]])inspect(X)|코퍼스, 텍스트, 문서행렬 등 정보 제공 / data(crude), inspect(crude[1:3]), inspect(crude[[1]]), tdm &lt;- TermDocumentMatrix(crude), inspect(tdm)lapply(x, content)|파일 내용을 보여줌 / lapply(crude, content) # information about the first file in crude data str(crude[[1]]) content(crude[[1]]) meta(crude[[1]]) lapply(crude, content) # inspect function inspect(crude[1:3]) inspect(crude[[1]]) 텍스트 마이닝- 전처리 함수함수|설명|—|—|tm_map(x, removePunctuation)|문장부호 제거(. , “” ‘’)tm_map(x, stripWhitespace)|공백문자 제거tm_map(x, removeNumbers)|숫자 제거 텍스트 마이닝 실습 문장부호 없애기: tm_map(x, removePunctuation) # 1. remove punctuation in documnet crude&lt;-tm_map(crude, removePunctuation) content(crude[[1]]) 숫자 제거: tm_map(x, removeNumbers) # 2. remove numbers crude&lt;-tm_map(crude, removeNumbers) content(crude[[1]]) 스톱워즈 제거: 언어별로 다르며 별도 지정 가능 # 3. remove stopwords crude&lt;-tm_map(crude, function(x) removeWords(x,stopwords())) content(crude[[1]]) 참고: 스톱워즈 리스트 stopwords() #stopwords(&quot;en&quot;) 174개, stopwords(&quot;SMART&quot;)&quot; 517개 문서 행렬 구성: TermDocumentMatrix(문서이름) # 4. contruct term-doucument matrix tdm&lt;-TermDocumentMatrix(crude) inspect(tdm) 결과 해석 DOCS 144: crude 문서 번호 144에 나오는 단어들의 빈도 문서 행렬을 행렬로 변환 # 5. read tdm as a matrix m&lt;-as.matrix(tdm) head(m) 단어의 빈도 순서로 정렬 # 6. sorting in high frequency to low frequency v&lt;-sort(rowSums(m), decreasing&#x3D;TRUE) v[1:10] # 가장 높은 것부터 [1:10]번까지 텍스트마이닝 수행 단어 이름과 빈도를 결합한 행렬을 데이터 프레임으로 저장 crude 관련 기사 파일로부터 962개 단어를 분류해 빈도 계산# 7. match with freq and word names d&lt;-data.frame(word&#x3D;names(v), freq&#x3D;v) head(d) #가장 빈도 높은 단어 6개 d[957:962, ] #가장 빈도 낮은 단어 6개 텍스트마이닝 결과 그리기 # 7. plot a word cloud wordcloud(d$word, d$freq) # help(wordcloud) # 7-1. Now lets try it with frequent words plotted first # par(mfrow &#x3D; c(1, 1),mar&#x3D;c(1,2,3,1)) wordcloud(d$word,d$freq,c(8,.3),2,,FALSE,.1) # 7-2. color plot with frequent words plotted first pal &lt;- brewer.pal(9,&quot;BuGn&quot;) pal &lt;- pal[-(1:4)] wordcloud(d$word,d$freq,c(8,.3),2,,FALSE,,.15,pal)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"딥러닝","slug":"딥러닝","permalink":"https://ne-choi.github.io/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"텍스트마이닝","slug":"텍스트마이닝","permalink":"https://ne-choi.github.io/tags/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 2: ⅩⅤ. 주성분 분석과 부분 최소자승법","slug":"Study/Postech/머신러닝R/ⅩⅤ_주성분_분석과_부분_최소자승법","date":"2020-12-23T00:00:00.000Z","updated":"2021-02-01T00:51:25.404Z","comments":true,"path":"/2020/12/23/Study/Postech/머신러닝R/ⅩⅤ_주성분_분석과_부분_최소자승법/","link":"","permalink":"https://ne-choi.github.io/2020/12/23/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A4_%EC%A3%BC%EC%84%B1%EB%B6%84_%EB%B6%84%EC%84%9D%EA%B3%BC_%EB%B6%80%EB%B6%84_%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다. ⅩⅤ. 주성분 분석과 부분 최소자승법1. 주성분분석주성분분석(PCA) 주성분분석(Principle Component Analysis) 다변량 분석기법 ‘주성분’이라고 불리는 선형조합으로 표현하는 기법 주성분은 공분산(X^T^X)로부터 eigenvector와 eigenvalue를 도출하여 계산됨 주성분 간의 수직 관계 1st 주성분(PC1): 독립변수들의 변동(분산)을 가장 많이 설명하는 성분 2nd 주성분(PC2): PC1과 수직인 주성분(1st 주성분이 설명하지 못하는 변동을 두 번째로 설명하는 성분) iris 데이터(4개변수)의 주성분 도출: 차원축소&amp;예측력 향상 목적iris&lt;-read.csv(&quot;data&#x2F;week15_1&#x2F;iris.csv&quot;) iris$Species&lt;-as.factor(iris$Species) attach(iris) head(iris) 독립변수 간 상관관계 확인# Check correlation cor(iris[1:4]) 결과 확인 0.96, 0.87 등 높은 상관계수가 관찰됨 주성분분석을 위한 함수 prcomp(독립변수들, center= , scale= ) 옵션을 주지 않으면 center=T, scale=F center=T, scale=T는 변수들의 평균을 빼고 편차로 나누어 표준화한다는 의미# 1.PCA(center&#x3D;T-&gt;mean&#x3D;0, scale.&#x3D;T-&gt;variance&#x3D;1) ir.pca&lt;-prcomp(iris[,1:4],center&#x3D;T,scale.&#x3D;T) ir.pca summary(ir.pca) 결과 해석 PC1 = 0.5211xSepal.Length - 0.2693xSepal.Width + 0.5804xPetal.Length + 0.5649xPetal.Width summary의 Proportion of Variance 전체 분산 중 각 주성분이 설명하는 비율 PC1: 전체 분산의 72.96%를 설명 PC2: 22.85%, PC3: 3.67%, PC4: 0.5%→ 누적설명비율을 보면, PC1과 PC2 두 성분으로 전체 분산의 95.81%를 설명 최적 주성분 수 찾기 scree plot을 그려보고 급격히 떨어지기 전까지의 PC를 선택# 2.scree plot : to choose the number of components plot(ir.pca,type&#x3D;&quot;l&quot;) 결과 해석 3rd PC에서 설명력이 급격하게 떨어짐 기울기가 꺾이는 PC3을 elbow point라고 부름→ PC1, PC2까지 사용하는 것을 추천 BAR Chart로 보기: screeplt(pca 결과)# either way to draw scree plot screeplot(ir.pca) PC계산 = X_data(n* p) % * % PCA_weight(p*p)# %*%: 행렬의 계산 # 3. Calculate component &#x3D; x_data%*% PCA_weight PRC&lt;-as.matrix(iris[,1:4])%*%ir.pca$rotation head(PRC) 결과 해석 PRC는 n*p행렬, 여기서는 150x4 PC1 = 0.5211xSepal.Length - 0.2693xSepal.Width + 0.5804xPetal.Length + 0.5649xPetal.Width 주성분을 이용한 분류모형 iris data → iris.pc data 구성# 4. classification using principal components # make data with components iris.pc&lt;-cbind(as.data.frame(PRC), Species) iris.pc$Species&lt;-as.factor(iris.pc$Species) head(iris.pc) 주성분을 이용한 서포트벡터머신 수행 주성분이 input# 5. support vector machine # install.packages(&quot;e1071&quot;) library (e1071) # classify all data using PC1-PC4 using support vector machine m1&lt;- svm(Species ~., data &#x3D; iris.pc, kernel&#x3D;&quot;linear&quot;) # m2&lt;- svm(Species ~PC1+PC2, data &#x3D; iris.pc, kernel&#x3D;&quot;linear&quot;) summary(m1) 서포트벡터머신 결과 vs PCA 결과# predict class for all data x&lt;-iris.pc[, -5] pred &lt;- predict(m1, x) # check accuracy between true class and predicted class y&lt;-iris.pc[,5] table(pred, y) 결과 해석 주성분을 이용한 분류의 오분류율: 2/150 = 0.013(1.33%)→ 이 데이터에서는 SVM(오분류율: 4/150 = 2.66%)보다 PCA 분류가 오분류율이 적다 2. 주성분 회귀분석주성분회귀 주성분회귀(Principle Component Regression) 독립변수 차원을 줄이기 위해 사용 가능, 주성분을 이용해 타겟변수(Y)의 설명력(예측력)을 높일 수 있음 Y = b0 + b1PC1 + b2PC2 독립변수의 전체분산을 가장 잘 설명하는 component를 사용해 독립변수 간 다중공선성 문제를 해결할 수 있음 주요 component score가 Y의 예측력을 보장하지는 않음 주요 component score는 X의 분산을 가장 잘 설명하는 방향의 축을 기준으로 변환된 것으로 Y와의 관계에는 상관성이 없을 수도 있음 주성분 회귀 분석 순서 데이터에 다중공선성이 있는지 체크 주성분 분석을 위한 데이터 전처리(mean-centering or scaling) 주성분분석 주성분 개수 결정 주성분으로 회귀분석모형 수행 주성분회귀분석 여러 변수를 주성분이라는 새로운 변수로 축소하여 회귀모형 수행 다중공선성 문제 해결 가능 적절한 주성분을 사용해 회귀모형 구현 필요 주성분 순서대로 Y변수에 대한 설명력이 높은 것은 아님 wine 데이터: 독립변수 9개, 타겟변수 Aroma rating wine&lt;-read.csv(&quot;data&#x2F;week15_2&#x2F;wine_aroma.csv&quot;) attach(wine) head(wine) # Check correlation cor(wine[1:9]) 주성분분석 # 1. PCA(center&#x3D;T → mean&#x3D;0, scale.&#x3D;T → variance&#x3D;1) wi.pca&lt;-prcomp(wine[1:9], center&#x3D;T, scale.&#x3D;F) wi.pca summary(wi.pca) 최적 주성분 수 찾기 # 2.scree plot : to choose the number of components plot(wi.pca,type&#x3D;&quot;l&quot;) 결과 해석 2rd PC에서 설명력이 급격히 떨어짐→ 이 경우 PC1만 사용해도 됨 PC 계산 # 3. calculate component&#x3D;x_data%*% PCA weight PRC&lt;-as.matrix(wine[,1:9])%*%wi.pca$rotation head(PRC) wine.pc data 구성 # 4. Principal component regression # make data with components wine.pc&lt;-cbind(as.data.frame(PRC),Aroma) head(wine.pc) 주성분을 이용한 회귀모형 다중회귀모형과 주성분회귀분석 다중회귀모형 Y = β0 + β1X1 + β2X2 + … + βKXK 주성분회귀모형 Y = β0 + β1PC1 + β2PC2 + … 타겟값(Y)를 가장 잘 예측하는 선형모형 주성분을 이용한 회귀분석모형 1(WINE DATA: PC1-PC4 포함)# regression(PC1-PC4) fit1&lt;-lm(Aroma~PC1+PC2+PC3+PC4, data&#x3D;wine.pc) fit1 summary(fit1) 결과 해석 PC3의 p-value는 0.7로 매우 높음 → 별로 중요하지 않음 R^2^= .494 주성분을 이용한 회귀분석모형 2(WINE DATA: PC1-PC4 포함)# regression(PC1-PC9) fit2&lt;-lm(Aroma~., data&#x3D;wine.pc) fit2 summary(fit2) 결과 해석 R^2^= .741 일반 회귀분석모형(wine data: raw data)# Multiple regression with the raw data fit3&lt;-lm(Aroma ~., data&#x3D;wine) summary(fit3) 잔차에 대한 가정 확인# residual diagnostic plot layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page plot(fit3) 3. Partial Least Square Regression최소자승회귀법(PLS) 주성분분석의 component vs 최소자승회귀법의 component PLS는 공정변수의 변동을 설명하는 벡터 t를 구하는 데 X 정보만을 이용하지 않고 타겟변수 y 정보를 동시에 고려하여 도출 t1(PLS): Y쪽으로 Shift Chemometrics, Marketing 분야의 고차원데이터, 독립변수 간 상관성 높은 데이터에 적용 PLS 수행을 위한 패키지 설치# install package for Partial Least Square #install.packages(&#39;pls&#39;) library(pls) 사용 데이터 설명 가솔린 데이터(근적외선 흡광고, 60개의 가솔린 표본) 독립변수 차원:401 타겟변수(Y): 옥탄가(octane numbers) data(gasoline) #help(&quot;gasoline&quot;) attach(gasoline) 데이터 요약 설명(타겟변수 Y: 옥탄가)# descriptive statistics par(mfrow&#x3D;c(1,1)) hist(octane, col&#x3D;3) summary(octane) 결과 해석 옥탄가의 최솟값 83.4, 최댓값 89.6 히스토그램은 옥탄가의 분포를 보여줌 훈련데이터와 검증데이터(50개/10개) # train and test set gasTrain &lt;- gasoline[1:50, ] gasTest &lt;- gasoline[51:60, ] # 1.check how many principal components ga.pca&lt;-prcomp(gasoline$NIR,center&#x3D;T,scale.&#x3D;F) ga.pca summary(ga.pca) plot(ga.pca,type&#x3D;&quot;l&quot;) 결과 해석 최소 5개 정도 사용하기 PLS함수: plsr plsr(타겟변수~독립변수, ncomp= , data= ) 옵션사항 ncomp: 잠재변수 수 validation=c(“none”, “CV”, “LOO”) (CV: cross-validaton, LOO: leave-one-out)# pls model by training set (find LV by leave-one-out) # 1. start with 10 component PLS model gas1 &lt;- plsr(octane ~ NIR, ncomp &#x3D; 10, data &#x3D; gasTrain, validation &#x3D; &quot;LOO&quot;) # NIR에 401차원의 값이 들어있음, ncomp: 잠재변수의 수 summary(gas1) 결과 해석 CV 1개의 잠재변수 → 10개의 잠재변수 1개의 잠재변수: 평균오차 1.357, 2개의 잠재변수: 평균오차 0.297 … TRAINING X들의 분산설명비율: 2개의 LV로 85.58% Y값의 변동분 설명비율: 96.85% 설명 PLS 모형에서의 최적 잠재변수 수# 2. to choose the number of components plot(RMSEP(gas1), legendpos &#x3D; &quot;topright&quot;, pch&#x3D;46, cex&#x3D;1.0, main&#x3D;&quot;Cross-validation for # of LV&quot;) # for gasoline data, # of LV&#x3D;2 결과 해석 최적 잠재변수의 수는 RMSEP가 최저이고 변화가 없는 지점에서 결정 2개의 components (LV)를 추천 예측모형 평가척도: 평균오차 최적 PLS 모형의 실제값과 예측값 산점도# 3. Display the PLS model with LV&#x3D;2 # scatterplot with true and predicted plot(gas1, ncomp &#x3D; 2, asp &#x3D; 1, line &#x3D; TRUE, cex&#x3D;1.5,main&#x3D;&quot;Measured vs Predicted&quot;, xlab&#x3D;&quot;Measured&quot; ) 잠재변수 수에 따른 전체분산의(독립변수들) 설명 정도# Check explained variances proportion for X explvar(gas1) 결과 해석 2개의 잠재변수가 전체 분산의 85.58% 설명 검증데이터의 RMSEP 계산 # 4. predicted Y for test data ypred&lt;-predict(gas1, ncomp &#x3D; 2, newdata &#x3D; gasTest) y&lt;-gasoline$octane[51:60] # check : RMSEP for test data sqrt((sum(y-ypred)^2)&#x2F;10) # 5. compare with the one from #4 : RMSEP for test data RMSEP(gas1, newdata &#x3D; gasTest) PLS 예측값 내보내기 # output of y and predicted y out1&lt;-cbind(y, ypred) # data exporting write.csv(out1,file&#x3D;&quot;out1.csv&quot;, row.names &#x3D; FALSE)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"주성분분석","slug":"주성분분석","permalink":"https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"},{"name":"최소자승법","slug":"최소자승법","permalink":"https://ne-choi.github.io/tags/%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 2: ⅩⅣ. 연관규칙과 로지스틱회귀분석","slug":"Study/Postech/머신러닝R/ⅩⅣ_연관규칙과_로지스틱회귀분석","date":"2020-12-22T00:00:00.000Z","updated":"2021-02-01T01:09:49.587Z","comments":true,"path":"/2020/12/22/Study/Postech/머신러닝R/ⅩⅣ_연관규칙과_로지스틱회귀분석/","link":"","permalink":"https://ne-choi.github.io/2020/12/22/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A3_%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99%EA%B3%BC_%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다. ⅩⅣ. 연관규칙과 로지스틱회귀분석1. 연관규칙 분석 Ⅰ연관규칙 연관규칙(Association Rule) 대용량 데이터베이스의 트랜잭션에서 빈번하게 발생하는 패턴을 발견 거래 간의 상호 관련성을 분석 연관규칙 예시 신발을 구매하는 고객의 10%는 양말을 동시에 구매한다. 빵과 우유를 고매한 고객의 50%가 쥬스도 함께 구매한다. 용어 설명 시장바구니(market basket) 고객이 구매한 물품에 관련한 정보(구매 시기, 지불 방법, 매장 정보 포함) 트렌잭션(transaction) 고객이 거래한 정보를 하나의 트랜잭션이라고 함 시장바구니 분석(market basket analysis) 시장바구니 데이터로부터 연관규칙 탐색 분석 연관규칙 평가 척도 지지도(Support) $\\frac{A와 B를 동시에 포함하는 거래 수}{전체 거래 수}$ 신뢰도(Confidence) $\\frac{A와 B를 동시에 포함하는 거래 수}{A를 포함하는 거래 수}$ 향상도(Lift) $\\frac{A와 B를 동시에 포함하는 거래 수}{A를 포함하는 거래 수 x B를 포함하는 거래 수}$ - 지지도가 어느 정도 수준에 도달해야 함 - 신뢰도가 높을 경우에는 두 항목 A → B에서 항목 B의 확률이 커야 연관규칙이 의미가 있음 - 향상도가 1보다 큰 값을 주어야 유용한 정보를 준다고 볼 수 있음 향상도(Lift) 향상도: A가 거래된 경우, 그 거래가 B를 포함하는 경우와 B가 임의로 거래되는 경우의 비율 향상도 의미 1 두 항목의 거래 발생이 독립적인 관계 &lt; 1 두 항목의 거래 발생이 서로 음의 상관관계 &gt; 1 두 항목의 거래 발생이 서로 양의 상관관계 정리 각 항목의 구매가 상호관련이 없다면 P(B|A)가 P(B)와 같아 향상도는 1이 됨 1보다 크면 결과 예측에 관해 우연적 기회(random chance)보다 우수함을 의미 향상도 값이 클수록 A의 거래 여부가 B의 거래 여부에 큰 영향을 미침 연관규칙 수행 패키지 arules 데이터가 transaction data로 Dataframe과 구조가 다름 ID, 거래된 아이템, 거래된 일자 정보 등# association rule analysis package #install.packages(&quot;arules&quot;) library(arules) # data import-&gt; make transaction data dvd1&lt;-read.csv(&quot;data&#x2F;week14_1&#x2F;dvdtrans.csv&quot;) dvd1 arules package를 통해 transaction 데이터 변환과 연관규칙 분석 수행 Split을 통해 ID별로 item을 as함수를 통해 transaction 데이터로 변환dvd.list&lt;-split(dvd1$Item,dvd1$ID) dvd.list dvd.trans&lt;-as(dvd.list,&quot;transactions&quot;) dvd.trans inspect(dvd.trans) transaction 데이터의 요약# summary of dvd.trans summary(dvd.trans) 결과 해석 10 트랜잭션 / 10 항목 밀도(density): 0.3 ← 10*10 cell 중, 30%dml cell에 거래가 발생해 데이터가 있다는 뜻 거래 항목 중 Gladiator = 7번, Patriot = 6번, Six Sense = 6번 순으로 나왔음을 의미 연관규칙 수행함수 apriori(transaction, parameter=list(support=0.0#, confidence=0.##))# for running dvdtras data dvd_rule&lt;-apriori(dvd.trans, parameter &#x3D; list(support&#x3D;0.2,confidence &#x3D; 0.20,minlen &#x3D; 2)) dvd_rule # same code with short command # dvd_rule&lt;-apriori(dvd.trans, parameter &#x3D; list(supp&#x3D;0.2,conf&#x3D; 0.20,minlen &#x3D; 2)) 결과 해석 support=0.2, confidence=0.2 이상인 13개의 연관규칙 생성 연관규칙의 해석summary(dvd_rule) inspect(dvd_rule) 결과 해석 지지도: Green Mile과 Sixth Sense를 동시에 구매할 확률: 20% 신뢰도: Green Mile을 구매한 경우는 모두 Sixth Sense를 구매: 100% 향상도: Green Mile을 구매하면 Six Sense 구매비율이 1.667배 향상됨 그래프로 표현한 연관규칙: 지지도 &gt; 0.2 항목들의 상대빈도 # Bar chart for support&gt;0.2 itemFrequencyPlot(dvd.trans,support&#x3D;0.2,main&#x3D;&quot;item for support&gt;&#x3D;0.2&quot;, col&#x3D;&quot;green&quot;) 2. 연관규칙 분석 Ⅱ데이터 설명(Groceries) Groceries data(“arules” package에 탑재된 데이터) data(“Groceries”)로 불러옴 실제 식료품점의 30일치 transaction 데이터 9835 트랜젝션 / 169 항목 밀도: 0.026% ← 9835*169 cell 중, 0.026%의 cell에 거래가 발생하여 숫자가 차 있다는 의미 Element(itemset/transaction) length distribution: 하나의 거래 장바구니(row 1개당)에 item의 개수별로 몇 번의 거래가 있었는지 나타냄 # association rule analysis package # install.packages(&quot;arules&quot;) library(arules) #association rule analysis data(&quot;Groceries&quot;) summary(Groceries) 지지도에 따른 시각화 지지도 5% 이상의 item 막대 그래프 # Bar chart for item with support&gt;&#x3D;5% itemFrequencyPlot(Groceries,supp&#x3D;0.05,main&#x3D;&quot;item for support&gt;&#x3D;5%&quot;, col&#x3D;&quot;green&quot;, cex&#x3D;0.8) 지지도 상위 20개 막대 그래프 # Bar chart for the top support 20 items itemFrequencyPlot(Groceries,topN&#x3D;20,main&#x3D;&quot;support top 20 items&quot;, col&#x3D;&quot;coral&quot;, cex&#x3D;0.8) 연관규칙 분석결과 연관규칙분석 support, confidence, length는 minimum값으로 너무 높게 잡을 경우 연관규칙 분석이 되지 않음# Association rule with support&gt;5%, confidence&gt;20% in minimum length 2 Grocery_rule&lt;-apriori(data&#x3D;Groceries, parameter &#x3D; list(support&#x3D;0.05, confidence &#x3D; 0.20, minlen &#x3D; 2)) Grocery_rule 연관규칙 조회 및 평가 #analyzing result summary(Grocery_rule) inspect(Grocery_rule) 결과 해석 향상도 최솟값이 1보다 큼 6개의 rules이 items 2개로 구성되어 있음 연관규칙: 향상도(lift) 순서로 정렬 sort() 함수를 통해 분석가가 보려는 기준으로 정렬하여 조회 가능# sorting by Lift inspect(sort(Grocery_rule,by&#x3D;&quot;lift&quot;)) # inspect(sort(Grocery_rule, by&#x3D;&quot;support&quot;)) 연관규칙: 품목별 연관성 탐색 subset() 함수를 통해 원하는 item이 포함된 연관규칙만 선별해 조회 가능 %in%. %pin%, %ain% 등 옵션 이용 가능# searching association for interesting items rule_interest&lt;-subset(Grocery_rule, items %in% c(&quot;yogurt&quot;,&quot;whole milk&quot;)) inspect(rule_interest) 연관규칙결과를 data.frame으로 저장 # save as dataframe Grocery_rule_df&lt;-as(Grocery_rule,&quot;data.frame&quot;) Grocery_rule_df #saving results as csv file write(Grocery_rule, file&#x3D;&quot;Grocery_rule.csv&quot;, sep&#x3D;&quot;,&quot;, quote&#x3D;TRUE, row.names&#x3D;FALSE) 3. 로지스틱회귀분석로지스틱 회귀모형 로지스틱회귀분석(logistic regression) 종속변수가 범주형인 경우 사용 2개의 범주(양성/음성, 불량/양품 등) 또는 3개 이상의 범주를 다룸 3개 이상 범주일 경우, 서열형 데이터(ordinal data), 명목형 데이터(nominal data)로 나누어 다른 모형 사용 log it(p) = β0 + β1X 회귀계수 β1의 의미가 선형회귀모형에서와는 다름 β1: X가 한 단위 증가할 때, logit(P)(승산비) 로그값의 증가분을 이야기함 → 승산비가 배로 증가함을 의미 Y가 (0/1, cancer/no cancer, present/absent) 등의 값을 취하는 경우, 독립변수들과 Y 관계를 설명하기 위해 로지스틱 함수 사용 f(y) = $\\frac{1}{1 + e^-y^}$ 로지스틱 회귀모형 실습 데이터 살펴보기 암 재발 확률 구하기 Y: Remiss(0,1) 6 explanatory variables: risk factor related cancer remission(Cell, Smear, infill, Li, blast, temp) 로짓 변환을 통해 p헷(확률값)을 계산한 후, 확률값을 보고 0,5 또는 최적 임계치를 기준으로 구분하는 것 데이터 불러오기 # remiss data re &lt;- read.csv(&quot;data&#x2F;week14_3&#x2F;remiss.csv&quot;) head(re) str(re) attach(re) 로지스틱회귀모형 실시(y는 binomial variable, logit function 선택) #logistic regression (full model) t1&lt;-glm(remiss~cell+smear+infil+li+blast+temp, data&#x3D;re,family&#x3D;binomial(logit)) summary(t1) cor(re) 결과 해석 smear, infil은 거의 1에 가까운 값을 보임 → 둘 중 하나 제거하는 것이 좋음 blast의 p-value가 1에 가까움: blast 변수를 추가적으로 설명할 수 있는 부분이 거의 없다 → 제거하는 것이 좋음 smear, blast 삭제 로지스틱 회귀모형의 평가 척도: -2Log(Deviance), AIC, likelihood ratio test(G^2) AIC가 낮은 것이 좋은 모델이라고 평가# logisitic regression (reduced model 1) t2&lt;-glm(remiss~cell+smear+li+temp, data&#x3D;re,family&#x3D;binomial(logit)) summary(t2) # logisitic regression (reduced model 2) t3&lt;-glm(remiss~cell+li+temp, data&#x3D;re,family&#x3D;binomial(logit)) summary(t3) 결과 해석 logit(P) = 67.63 + 9.65Cell + 3.87Li - 82.07Temp e.g. Li의 beta1 해석: li 한 단위 증가 시 재발 확률은 exp(3.867) = 47.79배 예측확률값 출력: 원래 데이터 + 예측확률값# output data with predicted probability dat1_pred&lt;-cbind(re,t3$fitted.values) write.table(dat1_pred,file&#x3D;&quot;dat1_pred.csv&quot;, row.names&#x3D;FALSE, sep&#x3D;&quot;,&quot;, na&#x3D;&quot; &quot;)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"연관규칙","slug":"연관규칙","permalink":"https://ne-choi.github.io/tags/%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99/"},{"name":"로지스틱회귀분석","slug":"로지스틱회귀분석","permalink":"https://ne-choi.github.io/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 2: ⅩⅢ. 군집분석","slug":"Study/Postech/머신러닝R/ⅩⅢ_군집분석","date":"2020-12-21T00:00:00.000Z","updated":"2021-02-01T00:50:02.752Z","comments":true,"path":"/2020/12/21/Study/Postech/머신러닝R/ⅩⅢ_군집분석/","link":"","permalink":"https://ne-choi.github.io/2020/12/21/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9%E2%85%A2_%EA%B5%B0%EC%A7%91%EB%B6%84%EC%84%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅱ 과정입니다. ⅩⅢ. 군집분석1. 군집분석과 유사성 척도군집분석 군집분석은 비지도학습(Unsupervised Learning) 주어진 데이터(X 변수들)의 속성으로 군집화 계층형 군집 분석 k-means 유사한 속성을 가진 객체를 군집(cluster)으로 묶는 데이터 마이닝 기법 예제: 고객 구매패턴을 반영하는 속성 데이터가 수집된다고 할 때, 군집분석을 통해 유사한 구매패턴을 보이는 고객을 군집화하고 판매전략을 도출(소득/충성정도 등) 군집분석 종류 계층적 군집(Hierarchical Clustering) 사전에 군집 수 k를 정하지 않고 단계적으로 군집 트를 제공 비계층적 군집(Non-hierarchical Clustering) 사전에 군집 수 k를 정한 후 각 객체를 k개 중 하나의 군집에 배정 유사성 척도 객체 간의 유사성 정도를 정량적으로 나타내기 위해 척도가 필요 거리(distance) 척도 거리가 가까울수록 유사성이 커짐 상관계수척도 객체 간 상관계수가 클수록 유사성이 커짐 # similarity measures - distance m1 &lt;- matrix( c(150, 50, 130, 55, 80, 80, 100, 85, 95, 91), nrow &#x3D; 5, ncol &#x3D; 2, byrow &#x3D; TRUE) # m1 is a matrix m1 is.data.frame(m1) # m1 is defined as dataframe m1&lt;-as.data.frame(m1) 거리척도 객체 i의 p차원 공간에서의 좌표는 열벡터로 표현 p개의 속상을 가진 객체 i에 관해, j번째 속성은 Xji로 표현 거리 계산 함수: dist(데이터, method= ), default는 “euclidean” 유클리디안 거리(Euclidean distance) Distance = $\\sqrt{(x12 - x11)^2^ + (x21 - x22)^2^}$# 1. Euclidean distance D1 &lt;- dist(m1) D1 민코프스키 거리(Minkowski distance 유클리디안 거리의 일반화된 방법(m = 2일 때는 유클리디안 거리와 동일) d(xi,xj) = (시그마|Xki - Xkj|^m^)^1/m^# 2. Minkowski distance D2&lt;- dist(m1, method&#x3D;&quot;minkowski&quot;, p&#x3D;3) D2 마할라노비스 거리(Mahalanobis distance) 변수 간의 상관관계가 존재할 때 사용 d(xi,xj) = $\\sqrt{(xi - xj)^T^S^-1^(xi - xj)}$ 상관계수를 유사성 척도로 사용 상관관계가 클수록 두 객체의 유사성이 크다고 추정# 3. correlation coefficient m2 &lt;- matrix( c(20, 6, 14, 30, 7, 15, 46, 4, 2), nrow &#x3D; 3, ncol &#x3D; 3, byrow &#x3D; TRUE, dimnames &#x3D; list( c(&quot;obs1&quot;, &quot;obs2&quot;, &quot;obs3&quot;), c(&quot;age&quot;, &quot;exp&quot;, &quot;time&quot;))) m2 상관계쑤 측정# correlation between Obs1~Obs2 cor(m2[1,],m2[2,]) # correlation between Obs1~Obs3 cor(m2[1,],m2[3,]) 결과 해석 객체 1(obs1)과 객체 2의 유사성이, 객체 1과 객체 3 간 유사성보다 큼(0.9674 &gt; 0.7984) 퀴즈# 2. Minkowski distance D4&lt;- dist(m1, method&#x3D;&quot;manhattan&quot;) D4 2. 계층적 군집분석계측적 군집분석 사전에 군집 수 k를 정하지 않고 단계적으로 군집 형성 유사한 객체를 군집으로 묶고, 그 군집을 기반으로 유사한 군집을 새로운 군집으로 묶어가면서 군집을 계층적으로 구성 단일연결법(single linkage method) 완전연결법(complete linkage method) 평균연결법(average linkage method) 중심연결법(centroid linkage method) 1. 단일연결법 군집 i와 군집 j의 유사성 척도로 두 군집의 모든 객체 쌍의 거리 중 가장 가까운 거리를 사용 객체 쌍이 가장 짧은 거리가 유사할수록 두 군집이 더 유사하다고 평가 2. 완전연결법 두 군집의 모든 객체 쌍의 거리 중 가장 먼 거리를 사용 3. 평균연결법 두 군집의 모든 객체 쌍의 평균 거리를 사용 4. 중심연결법 두 군집의 중심 좌표 (무게 중심, 객체가 아닌 위치) 완전연결법 vs 평균연결법 데이터 설명 1833년 영국 Lancashire 방직 공장 임금 DAAG package built in 데이터 총 51개 객체 객체별 5개 속성 나이(age), 남성 근로자 수(mnum), 남성 근로자 평균 임금(mwage), 여성 근로자 수(fnum), 여성 근로자 평균 임금(fwage) # needs &quot;lattice&quot;, &quot;DAAG&quot; package for loading dataset # install.packages(&quot;lattice&quot;) # install.packages(&quot;DAAG&quot;) library(lattice) library(DAAG) 데이터 불러오기(DAAG 패키지 안에 든 데이터)# read csv file wages1833&lt;-read.csv(file&#x3D;&quot;data&#x2F;week13_2&#x2F;wages1833.csv&quot;) head(wages1833) # remove observations with the missing values dat1&lt;-wages1833 dat1&lt;-na.omit(dat1) str(dat1) 계층적 군집분석: hclust(거리계산결과, method=” “)# calculate distance between each nodes dist_data&lt;-dist(dat1) 완전연결법 적용 결과(거리 계산은 유클리디안)# prepare hierarchical cluster # complete linkage method hc_a &lt;- hclust(dist_data, method &#x3D; &quot;complete&quot;) plot(hc_a, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;complete&quot;) 설명 single: 단일, complete: 완전, average: 평균, centriod: 중심 평균연결법 적용 결과(거리 계산은 유클리디안)# average linkage method # check how different from complete method hc_c &lt;- hclust(dist_data, method &#x3D; &quot;average&quot;) plot(hc_c, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;average&quot;) 와드 연결방법 Ward’s method 많이 사용됨 와드방법을 적용한 결과(거리계산은 유클리디안)# Ward&#39;s method hc_c &lt;- hclust(dist_data, method &#x3D; &quot;ward.D2&quot;) plot(hc_c, hang &#x3D; -1, cex&#x3D;0.7, main &#x3D; &quot;Ward&#39;s method&quot;) 3. 비계층적 군집분석비계층적 군집분석 사전에 군집 수 k를 정한 후, 각 객체를 k개 중 하나의 군집에 배정 비계층적 군집(Non-hierarchical Clustering) K-means 알고리즘 K-medoids 알고리즘 PAM(Partitioning Around Medoids) CLARA(Clustering LARge Applications) k-means 군집분석 비계층적 군집분석 중 가장 널리 사용 k개 군집의 중심좌표를 고려하여 각 객체를 가장 가까운 군집에 배정하는 것을 반복 0단계. 초기 객체 선정: k개 객체 좌표를 초기 군집 중심좌표로 선정 1단계. 객체 군집 배정: 각 객체와 k개 중심좌표와의 거리 산출 후, 가장 가까운 군집에 객체 배정 2단계. 군집 중심좌표 산출: 새로운 군집의 중심좌표 산출 3단계. 수렴 조건 점검: 새로 산출된 중심 좌표값과 이전 좌표값을 비교하여 수렴 조건 내에 들면 종료, 아니면 단계 1 반복 k-means 군집분석 예제 데이터 불러오기 # read csv file wages1833&lt;-read.csv(file&#x3D;&quot;data&#x2F;week13_3&#x2F;wages1833.csv&quot;) head(wages1833) # preprocessing dat1&lt;-wages1833 dat1&lt;-na.omit(dat1) head(dat1, n&#x3D;5) 군집수 k결정 최적 군집수에 관한 시각화 최적값은 “silhouette(실루엣)”, “gap_stat”, “wss(그룹내합계제곱)”으로 산출 그래프가 완만해지는 지점을 k의 값으로 추정# to choose the optimal k # install.packages(&quot;factoextra&quot;) library(factoextra) fviz_nbclust(dat1, kmeans, method &#x3D; &quot;wss&quot;) 결과 해석 최적 k = 3 k-means (k=3) # compute kmeans set.seed(123,sample.kind&#x3D;&quot;Rounding&quot;) km &lt;- kmeans(dat1, 3, nstart &#x3D; 25) # random set의 수 (nstart) km # visualize fviz_cluster(km, data &#x3D; dat1, ellipse.type&#x3D;&quot;convex&quot;, # Convex 모양으로 구역 표시 repel &#x3D; TRUE) # repel을 통해 관측치 표기 K-medoids 군집분석 K-medoids 군집분석은 각 군집의 대표 객체(medoid)를 고려 군집의 대표 객체란, 군집 내 다른 객체들과의 거리가 최소가 되는 객체 K-medoids 군집분석은 객체를 k개의 군집으로 구분하는데, 객체와 속하는 군집의 대표 객체와의 거리 총합을 최소로 하는 방법 PAM(Partitioning Around Medoids) 알고리즘: 모든 객체에 관해 대표 객체가 변했을 때 발생하는 거리 총합의 변화를 계산, 데이터 수가 많아질수록 연산량이 크게 증가 CLARA 알고리즘: 적절한 수의 객체를 샘플링한 후, PAM 알고리즘을 적용해 대표 객체 선정, 샘플링을 여러 번 한 후 가장 좋은 결과를 선택, 편향된 샘플링은 잘못된 결괏값을 도출할 수 있음 PAM 알고리즘 살펴보기 # compute PAM library(&quot;cluster&quot;) pam_out &lt;- pam(dat1, 3) pam_out # freq of each cluster table(pam_out$clustering) # visualize fviz_cluster(pam_out, data &#x3D; dat1, ellipse.type&#x3D;&quot;convex&quot;, repel &#x3D; TRUE)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"k-인접기법","slug":"k-인접기법","permalink":"https://ne-choi.github.io/tags/k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95/"},{"name":"판별분석","slug":"판별분석","permalink":"https://ne-choi.github.io/tags/%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 1: Ⅹ. k-인접기법과 판별분석","slug":"Study/Postech/머신러닝R/Ⅹ_k-인접기법과_판별분석","date":"2020-12-14T00:00:00.000Z","updated":"2021-02-01T00:49:04.678Z","comments":true,"path":"/2020/12/14/Study/Postech/머신러닝R/Ⅹ_k-인접기법과_판별분석/","link":"","permalink":"https://ne-choi.github.io/2020/12/14/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A9_k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95%EA%B3%BC_%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다. Ⅹ. k-인접기법과 판별분석1. k-인접기법 k-nearest neighbor 분류(Classification) 분류(Classification): 지도학습(Supervised Learning) 타겟범주를 알고 있는 데이터로 분류 규칙을 생성하고 새로운 데이터를 특정 범주에 분류하는 기법 군집화(Clustering): 비지도학습(Unsupervised Learning) 독립변수들의 속성을 기반으로 객체들을 그룹화하는 방법 k-인접기법 k-인접방법 (kNN): k개의 가장 가까운 이웃을 사용해서 분류하는 방법 거리만 고려하거나, 거리에 따라 가중치를 부여하는 2가지 방법 사용되는 변수에 결측치가 있는 경우, 미리 처리하고 수행해야 함 k개의 인접한 관측치의 다수 범주로 할당하는 방법 최적 k는? k가 너무 크면 데이터 구조를 파악하기 어렵고, 너무 작으면 과적합(overfitting) 위험이 있음 교차검증(Cross-validation)으로 정확도가 높은 k를 선정 장점 단순하며 효율적 데이터 분산을 추정할 필요가 없음 빠른 훈련 단계 단점 모델을 생성하지 않음 느린 분류 단계 많은 메모리 필요 결측치는 추가 작업 필요 kNN 수행을 위한 패키지 설치 # packages # install.packages(&quot;class&quot;) #no weighted value knn # install.packages(&quot;gmodels&quot;) #검증에 사용되는 cross table을 위한 패키지 # install.packages(&quot;scales&quot;) #for graph library(class) library(gmodels) library(scales) train/test 데이터 분할(cross-validation) # read csv file iris &lt;- read.csv(&quot;data&#x2F;week10_1&#x2F;iris.csv&quot;) # head(iris) # str(iris) attach(iris) # training&#x2F; test data : n&#x3D;150 set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;) N&#x3D;nrow(iris) tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # attributes in training and test iris.train&lt;-iris[tr.idx,-5] iris.test&lt;-iris[-tr.idx,-5] # target value in training and test trainLabels&lt;-iris[tr.idx,5] testLabels&lt;-iris[-tr.idx,5] train&lt;-iris[tr.idx,] test&lt;-iris[-tr.idx,] kNN 수행과 결과 kNN 함수: knn(train=학습데이터, test=검증데이터, cl=타겟변수, k= )# knn (5-nearest neighbor) md1 &lt;- knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;5) md1 kNN(k=5)의 결과: 정확도 # accuracy of 5-nearest neighbor classification CrossTable(x&#x3D;testLabels,y&#x3D;md1, prop.chisq&#x3D;FALSE) # x: 타겟변수의 실제값, y: 타겟변수의 예측값 결과 해석 정확도: 47/50 → 94% versicolor를 virginica로 오분류(2개) virginica를 versicolor로 오분류(1개) 오분류율: 3/50 → 6% 2. k-인접기법: 가중치kNN에서 최적 k 탐색 최적 k의 탐색: 1 to nrow(tran_data)/2 (여기서는 1 to 50) # optimal k selection (1 to n&#x2F;2) accuracy_k &lt;- NULL # try k&#x3D;1 to nrow(train)&#x2F;2, may use nrow(train)&#x2F;3(or 4,5) depending the size of n in train data nnum&lt;-nrow(iris.train)&#x2F;2 for(kk in c(1:nnum)) &#123; set.seed(1234, sample.kind&#x3D;&quot;Rounding&quot;) knn_k&lt;-knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;kk) accuracy_k&lt;-c(accuracy_k,sum(knn_k&#x3D;&#x3D;testLabels)&#x2F;length(testLabels)) &#125; # plot for k&#x3D;(1 to n&#x2F;2) and accuracy test_k&lt;-data.frame(k&#x3D;c(1:nnum), accuracy&#x3D;accuracy_k[c(1:nnum)]) plot(formula&#x3D;accuracy~k, data&#x3D;test_k,type&#x3D;&quot;o&quot;,ylim&#x3D;c(0.5,1), pch&#x3D;20, col&#x3D;3, main&#x3D;&quot;validation-optimal k&quot;) with(test_k,text(accuracy~k,labels &#x3D; k,pos&#x3D;1,cex&#x3D;0.7)) # minimum k for the highest accuracy min(test_k[test_k$accuracy %in% max(accuracy_k),&quot;k&quot;]) 결과 해석 k=7에서 정확도(.98)가 가장 높음 최종 kNN 모형 (k=7)#k&#x3D;7 knn md1&lt;-knn(train&#x3D;iris.train,test&#x3D;iris.test,cl&#x3D;trainLabels,k&#x3D;7) CrossTable(x&#x3D;testLabels,y&#x3D;md1, prop.chisq&#x3D;FALSE) 결과 해석 정확도: 49/50 → 98% versicolor를 virginica로 오분류 (1개) 오분류율: 1/50 → 2% kNN(k=7)의 결과: 그래픽# graphic display plot(formula&#x3D;Petal.Length ~ Petal.Width, data&#x3D;iris.train,col&#x3D;alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7)[trainLabels], main&#x3D;&quot;knn(k&#x3D;7)&quot;) points(formula &#x3D; Petal.Length~Petal.Width, data&#x3D;iris.test, pch &#x3D; 17, cex&#x3D; 1.2, col&#x3D;alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7)[md1] ) legend(&quot;bottomright&quot;, c(paste(&quot;train&quot;,levels(trainLabels)),paste(&quot;test&quot;,levels(testLabels))), pch&#x3D;c(rep(1,3),rep(17,3)), col&#x3D;c(rep(alpha(c(&quot;purple&quot;,&quot;blue&quot;,&quot;green&quot;),0.7),2)), cex&#x3D;0.9 ) 결과 해석 Petal.width와 Petal.length에 산점도를 그리면 setosa는 잘 분류됨 virginica와 versicolor는 분류가 잘 되지 않음 Weighted kNN 거리에 따라 가중치를 부여하는 두 가지 알고리즘이 존재 # Weighted KNN packages # install.packages(&quot;kknn&quot;) # weighted value knn library(kknn) k=5, distance=1 # weighted knn md2 &lt;- kknn(Species~., train&#x3D;train, test&#x3D;iris.test, k&#x3D;5, distance&#x3D;1, kernel&#x3D;&quot;triangular&quot;) md2 # to see results for weighted knn md2_fit&lt;-fitted(md2) md2_fit cross table로 오분류율 보기 # accuracy of weighted knn CrossTable(x&#x3D;testLabels,y&#x3D;md2_fit,prop.chisq&#x3D;FALSE,prop.c&#x3D;FALSE) k=7, distance=2로 옵션 변경 결과 # weighted knn (k&#x3D;7, distance&#x3D;2) md3&lt;-kknn(Species~., train&#x3D;train,test&#x3D;iris.test,k&#x3D;7,distance&#x3D;2,kernel&#x3D;&quot;triangular&quot;) md3 # to see results for weighted knn md3_fit&lt;-fitted(md3) md3_fit # accuracy of weighted knn CrossTable(x&#x3D;testLabels,y&#x3D;md3_fit,prop.chisq&#x3D;FALSE,prop.c&#x3D;FALSE) 3. 판별분석 Ⅰ판별분석 데이터 마이닝 분류 기법 중 하나 객체를 몇 개의 범주로 분류 범주를 가장 잘 구분하는 변수 파악 및 범주간 차이를 가장 잘 표현하는 함수 도출 피셔(Fisher) 방법 의사결정이론 선형판별분석(LDA; Liner DA): 정규분포 분산-공분산 행렬이 범주에 관계 없이 동일한 경우 이차판별분석(QDA; Quadratic DA): 정규분포의 분산-공분산 행렬이 범주별로 다른 경우 예제 데이터 # install.packages(&quot;gmodels&quot;) #crosstable library(gmodels) train / test 분할 # read csv file iris&lt;-read.csv(&quot;data&#x2F;week10_3&#x2F;iris.csv&quot;) attach(iris) # training&#x2F; test data : n&#x3D;150 set.seed(1000,sample.kind&#x3D;&quot;Rounding&quot;) N&#x3D;nrow(iris) tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # attributes in training and test iris.train&lt;-iris[tr.idx,-5] # 독립변수 4개 포함한 100개 데이터 iris.test&lt;-iris[-tr.idx,-5] # 독립변수 4개 포함한 50개 데이터 # target value in training and test trainLabels&lt;-iris[tr.idx,5] # 학습데이터의 타겟변수 testLabels&lt;-iris[-tr.idx,5] # 검증데이터의 타겟변수 train&lt;-iris[tr.idx,] test&lt;-iris[-tr.idx,] LDA함수: lda(종속변수~독립변수, data=학습데이터이름, prior=사전확률) 사전확률(prioir probability): 원인 A가 발생할 확률인 P(A)와 같이, 결과가 나타나기 전에 결정되어 있는 확률# install the MASS package for LDA # install.packages(&quot;MASS&quot;) library(MASS) # Linear Discriminant Analysis (LDA) with training data n&#x3D;100 iris.lda &lt;- lda(Species ~ ., data&#x3D;train, prior&#x3D;c(1&#x2F;3,1&#x2F;3,1&#x2F;3)) # .의 의미: 전체 데이터를 사용하겠다(4개) iris.lda 결과 해석 Coefficients ~ 첫 번째 범주 판별 함수: LD1 = 0.89 Sepal.Length + 1.71 Sepal.Width - 2.15 Petal.Length - 2.91 Petal.Width 두 번째 범주 판별 함수: LD2 = -0.11 Sepal.Length - 2.23 Sepal.Width + 0.74 Petal.Length - 2.39 Petal.Width LD1이 between-group variance의 99%를 설명 LD2가 between-group variance의 1%를 설명 검증 데이터에 LDA 결과를 적용하여 범주 추정 # predict test data set n&#x3D;50 testpred &lt;- predict(iris.lda, test) testpred 결과 해석 $class: 추정 범주 세 개 범주의 사후 확률(posterior probability)을 구한 후, max 값의 범주로 할당 정확도 산정: 오분류율(검증데이터) # accuracy of LDA CrossTable(x&#x3D;testLabels,y&#x3D;testpred$class, prop.chisq&#x3D;FALSE) 결과 해석 정확도: 49/50 → 98% 오분류율: 1/50 → 2% 퀴즈iris.lda &lt;- lda(Species ~ ., data=train, prior=c(1/2,1/4,1/4)) # .의 의미: 전체 데이터를 사용하겠다(4개)iris.lda testpred &lt;- predict(iris.lda, test)testpred CrossTable(x=testLabels,y=testpred$class, prop.chisq=FALSE) testpred1 &lt;- round(testpred$posterior, 2) id=150 4. 판별분석 Ⅱ선형판별분석 vs 이차판별분석 LDA QDA 분산-공분산 행렬이 범주 관계 없이 동일한 경우 분산-공분산 행렬이 범주별로 다른 경우 (+) 적은 파라미터 사용, 낮은 분산 (-) 많은 파라미터 사용, 높은 분산 (-) 낮은 유연성 (=) 높은 유연성 이차판별분석(QDA) 모집단 등분산 검정 분산-공분산 행렬이 범주별로 다른 경우, 이차판별분석 실시 Box’s M-test 귀무가설: 모집단의 분산-공분산 행렬이 동일 대립가설: 모집단의 분산-공분산 행렬이 동일하지 X 등분산 검정을 위한 패키지: biotools # install.packages(&quot;biotools&quot;) library(biotools) boxM(iris[1:4], iris$Species) 결과 해석 p-value~0: p-value는 거의 0에 가까움 귀무가설(등분산 가정)이 기각 → QDA 실시 QDA 함수 qda(종속변수~독립변수, data=학습데이터이름, prior=사전확률) # Quadratic Discriminant Analysis (QDA) iris.qda &lt;- qda(Species ~ ., data&#x3D;train, prior&#x3D;c(1&#x2F;3,1&#x2F;3,1&#x2F;3)) iris.qda 추가 설명 prior은 경우에 따라 다르게 줄 수 있음 독립변수에 대한 그룹별 평균값 검증 데이터에 QDA 결과를 적용하여 범주 추정 # predict test data set n&#x3D;50 testpredq &lt;- predict(iris.qda, test) testpredq 결과 해석 $class: 추정 범주 세 개 범주의 사후 확률(posterior probability)을 구한 후, max 값의 범주로 할당 정확도 산정: 오분류율(검증데이터)# accuracy of QDA CrossTable(x&#x3D;testLabels,y&#x3D;testpredq$class, prop.chisq&#x3D;FALSE) 결과 해석 선형판별로 했을 때와 동일(단, 데이터가 많아지면 결과가 다르게 나올 수 있음) Partition Plot# partimat() function for LDA &amp; QDA # install.packages(&quot;klaR&quot;) library(klaR) partimat(as.factor(iris$Species) ~ ., data&#x3D;iris, method&#x3D;&quot;lda&quot;) partimat(as.factor(iris$Species) ~ ., data&#x3D;iris, method&#x3D;&quot;qda&quot;)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"k-인접기법","slug":"k-인접기법","permalink":"https://ne-choi.github.io/tags/k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95/"},{"name":"판별분석","slug":"판별분석","permalink":"https://ne-choi.github.io/tags/%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 1: XⅡ. 의사결정나무와 랜덤 포레스트","slug":"Study/Postech/머신러닝R/XⅡ_의사결정나무와_랜덤_포레스트","date":"2020-12-13T00:00:00.000Z","updated":"2021-02-01T00:44:48.244Z","comments":true,"path":"/2020/12/13/Study/Postech/머신러닝R/XⅡ_의사결정나무와_랜덤_포레스트/","link":"","permalink":"https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/X%E2%85%A1_%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4%EC%99%80_%EB%9E%9C%EB%8D%A4_%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/","excerpt":"","text":"setup, includeknitr::opts_chunk$set(echo &#x3D; TRUE) POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다. XⅡ. 의사결정나무와 랜덤 포레스트1. 의사결정나무 Ⅰ의사결정나무 Decision Tree 기계학습 중 하나로, 의사결정 규칙을 나무 형태로 분류해가는 분석 기법 분석과정이 직관적이고 이해하기 쉬움 연속형/범주형 변수를 모두 사용할 수 있음 분지규칙은 불순도(범주가 섞여 있는 정도)를 최소화함 과정 tree 형성(Growing tree): 너무 많이 키워서 분류하면 과적합 문제가 생김 tree 가지치기(Pruning tree): 과적합 예방을 위해 최적 tree로 분류(Classification) #decision tree packages download #install.packages(&quot;tree&quot;) #load library library(tree) #package for confusion matrix #install.packages(&quot;caret&quot;) library(caret) # read csv file iris&lt;-read.csv(&quot;data&#x2F;week12_1&#x2F;iris.csv&quot;) attach(iris) 학습데이터 / 검증데이터 분할# training (n&#x3D;100)&#x2F; test data(n&#x3D;50) set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;) N&lt;-nrow(iris) tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # split train data and test data train&lt;-iris[tr.idx,] test&lt;-iris[-tr.idx,] #dim(train) #dim(test) 의사결정나무 함수tree(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용 step 1: Growing tree 학습데이터의 tree 결과 마디 6에서는 더이상 분지할 필요 없음treemod &lt;- tree(Species~., data&#x3D;train) treemod plot(treemod) text(treemod,cex&#x3D;1.5) table(train$Species) step 2. Pruning cv.tree(tree모형결과, FUN= ) 최적 tree 모형을 위한 가지치기 cv.trees &lt;- cv.tree(treemod, FUN&#x3D;prune.misclass) cv.trees plot(cv.trees) 최종 트리모형# final tree model with the optimal node prune.trees&lt;-prune.misclass(treemod, best&#x3D;3) plot(prune.trees) text(prune.trees,pretty&#x3D;0, cex&#x3D;1.5) #help(prune.misclass) step 3.# step 3: classify test data treepred&lt;-predict(prune.trees,test,type&#x3D;&#39;class&#39;) # classification accuracy confusionMatrix(treepred,test$Species) 2. 의사결정나무 Ⅱrpart 패키지# other package for tree # install.packages(&quot;rpart&quot;) # install.packages(&quot;party&quot;) library(rpart) library(party) rpart(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용cl1&lt;-rpart(Species~., data&#x3D;train) plot(cl1) text(cl1, cex&#x3D;1.5) Prunning rpart 패키지는 과적합 우려가 있음(iris의 경우에는 필요 x) printcp에서 xerror(cross validation error) 값이 최소가 되는 마디를 선택#pruning (cross-validation)-rpart printcp(cl1) plotcp(cl1) rpart를 사용한 최종 tree 모형pcl1&lt;-prune(cl1, cp&#x3D;cl1$cptable[which.min(cl1$cptable[,&quot;xerror&quot;]),&quot;CP&quot;]) plot(pcl1) text(pcl1) 결과 해석 tree 함수를 이용했을 때와 동일한 결과 정확도 test data에 대한 정확도#measure accuracy(rpart) #package for confusion matrix #install.packages(&quot;caret&quot;) library(caret) pred2&lt;- predict(cl1,test, type&#x3D;&#39;class&#39;) confusionMatrix(pred2,test$Species) party 패키지 ctree(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용 p-value를 기준으로 분지되는 포인트를 잡음#decision tree(party)-unbiased recursive partioning based on permutation test partymod&lt;-ctree(Species~.,data&#x3D;train) plot(partymod) 3. 랜덤 포레스트랜덤 포레스트 Random Forest 2001년 Leo Breiman이 제안 의사결정나무의 단점(과적합)을 개선한 알고리즘 앙상블(Ensemble, 결합) 기법을 사용한 모델로, 주어진 데이터를 리샘플링하여 다수의 의사결정나무를 만든 다음, 여러 모델의 예측 결과를 종합해 정확도를 높이는 방법 Bagging Bagging(Bootstrap Aggregating) 전체 데이터에서 학습데이터를 복원추출(resampling)하여 트리 구성 Training Data에서 Random Sampling 클래스 값 중 가장 많이 voting된 값이 결정 랜덤 포레스트 패키지 randomForest install.packages(&quot;randomForest&quot;) library(randomForest) iris 데이터 사용 # read csv file iris&lt;-read.csv(&quot;data&#x2F;week12_3&#x2F;iris.csv&quot;) attach(iris) # training&#x2F; test data : n &#x3D; 150 set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;) N&lt;-nrow(iris) tr.idx&lt;-sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # split training and test data train&lt;-iris[tr.idx,] test&lt;-iris[-tr.idx,] #dim(train) #dim(test) 랜덤포레스트 함수 ramdomForest(종속변수~x1+x2+x3+x4, data= ) #종속변수를 모두 사용#Random Forest : mtry&#x3D;2 (default&#x3D;sqrt(p)) iris$Species &lt;- as.numeric(iris@Species) rf_out1&lt;-randomForest(Species~.,data&#x3D;train,importance&#x3D;T) rf_out1 변수의 중요도: ramdom forest 결과로부터 중요 변수 확인 # important variables for RF round(importance(rf_out1), 2) 추가 #Random Forest : mtry&#x3D;4 rf_out2&lt;-randomForest(Species~.,data&#x3D;train,importance&#x3D;T, mtry&#x3D;4) rf_out2 # important variables for RF round(importance(rf_out2), 2) 정확도 #measuring accuracy(rf) rfpred&lt;-predict(rf_out2,test) confusionMatrix(rfpred,test$Species)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"의사결정나무","slug":"의사결정나무","permalink":"https://ne-choi.github.io/tags/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4/"},{"name":"랜덤포레스트","slug":"랜덤포레스트","permalink":"https://ne-choi.github.io/tags/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 1: Ⅸ. 데이터 마이닝 기초","slug":"Study/Postech/머신러닝R/Ⅸ_데이터_마이닝_기초","date":"2020-12-13T00:00:00.000Z","updated":"2021-02-01T00:46:10.533Z","comments":true,"path":"/2020/12/13/Study/Postech/머신러닝R/Ⅸ_데이터_마이닝_기초/","link":"","permalink":"https://ne-choi.github.io/2020/12/13/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/%E2%85%A8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%A7%88%EC%9D%B4%EB%8B%9D_%EA%B8%B0%EC%B4%88/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다. Ⅸ. 데이터 마이닝 기초1. 다중 회귀 분석 Ⅰ데이터 마이닝 기법 데이터 마이닝 예측(prediction) 야구선수의 연봉(차기 년도) 주식 변동(t+1 시점) 일기예보(비 올 확률) 수질오염(오염 수치)→ 회귀분석, 선형모형, 비선형모형 분류(classification) 대출심사(허가/불가) 신용등급(A, B, C 등급) 고객 분류(구매빈도, 구매액) 품종분류→ 의사결정나무, 서포트벡터머신, 판별분석, 로지스틱회귀모형 다중회귀모형 다중회귀모형(multiple regression) 종속변수 Y를 설명하는 데 k개의 독립변수 X1, …, Xk가 있을 때, 다중회귀모형은 아래와 같이 정의 Yi = β0 + β1X1i + β2X2i + … + βkXki + εi, i = 1, 2, …, nεiN(0, σ^2^) 회귀계수 βk의 해석: 다른 독립변수가 일정할 때, Xk의 한 단위 변화에 따른 평균 변화량 autompg 데이터 # autompg data car&lt;-read.csv(&quot;data&#x2F;week9_1&#x2F;autompg.csv&quot;) head(car) str(car) attach(car) 다중회귀모형: lm(y변수~x1+x2+x3, data= ) 1st model: 전체변수를 모두 포함한 회귀모형 # multiple regression : 1st full model r1&lt;-lm(mpg ~ disp+hp+wt+accler, data&#x3D;car) summary(r1) 결과 해석 선형회귀식: mpg = 40.88 - 0.011 disp + 0.0048 hp - 0.0061 wt + 0.17 accler 선형회귀식의 결정계수: R^2^ = 0.7006 추가: 의문점 확인 check point 1: 마력(hp)가 높을수록 연비가 좋은가? (상식적으로는 음의 선형관계여야 함)→ 데이터 탐색 필요 데이터 탐색(Explanatory Data Analysis) # pariwise plot - Explanatory Data Analysis var1&lt;-c(&quot;mpg&quot;,&quot;disp&quot;,&quot;hp&quot;,&quot;wt&quot;, &quot;accler&quot; ) pairs(car[var1], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car$cyl)) 2. 다중회귀분석 Ⅱ다중회귀분석: 변수선택방법 변수선택방법: 다수의 독립변수들이 있을 때 최종모형은? 전진선택벅(forward selection): 독립변수 중, 종속변수에 가장 큰 영향을 주는 변수부터 모형에 포함 후진제거법(backward elimination): 독립변수를 모두 포함한 모형에서 가장 영향이 적은(중요하지 않은) 변수부터 제거 단계별방법(stepwise method): 전진선택법에 의해 변수 추가, 변수 추가 시 기존 변수의 중요도가 정해진 유의수준(threshold)에 포함되지 않으면 앞에서 넣은 변수 제거할 수 있음 단계별 방법 step(모형, direction = “both”)# 2rd model using variable selection method # step(r1, direction&#x3D;&quot;forward&quot;) # step(r1, direction&#x3D;&quot;backward&quot;) # stepwise selection step(r1, direction&#x3D;&quot;both&quot;) #step(lm(mpg ~ disp+hp+wt+accler, data&#x3D;car), direction&#x3D;&quot;both&quot;) 결과 해석 변수 제거: hp 최종 변수 선택: disp, wt, accler 단계별 방법에 따른 최종 다중회귀모형 2nd model: 단계별 선택 방법에 의한 회귀모형# final multiple regression r2&lt;-lm(mpg ~ disp+wt+accler, data&#x3D;car) summary(r2) 결과 해석 선형회귀식: mpg = 41.30 = 0.011 disp - 0.0062 wt + 0.17 accler 선형회귀식의 결정계수: R^2^ = 0.7004 회귀분석의 가정과 진단# residual diagnostic plot layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page plot(r2) 다중공선성 다중공선성(Multicollinearity) 독립변수들 사이에 상관관계가 있는 현상 다중공선성이 존재하는 경우 회귀계수 해석 불가능 독립변수들간의 상관계수 # check correlation between independent variables var2&lt;-c(&quot;disp&quot;,&quot;hp&quot;,&quot;wt&quot;, &quot;accler&quot; ) cor(car[var2]) # get correlation for each pair # cor(disp, wt) # cor(disp, accler) # cor(wt, accler) 분산팽창계수(VIF; Variance Inflation Factor): 다중공선성의 척도 VIFj = $\\frac{1}{1 - R^2^j}$, j = 1, 2, …, k VIF는 다중공산성으로 인한 분산의 증가를 의미 R^2^j은 Xj를 종속변수로 하고 나머지 변수를 독립변수로 하는 회귀모형에서의 결정계쑤 VIFj값 &gt; 10이면 다중공선성을 고려 정리 변수 선택 과정에서 상관계수가 높은 두 변수 중 하나만을 선택 더 많은 데이터 수집 능형회귀(ridge regression), 주성분회귀(principal components regression) car 패키지 내장 함수 # check multicollinearity # variance inflation factor(VIF) # install.packages(&quot;car&quot;) library(car) vif(lm(mpg ~ disp+hp+wt+accler, data&#x3D;car)) 결과 해석 check point 1: coefficients &amp; R^2^ 선형회귀식: mpg = 41.30 - 0.011 disp - 0.0062 wt + 0.17 accler 선형회귀식의 결정계수: R^2^ = 0.7004 check point 2: multi-collinearlity disp와 wt의 VIF가 10에 가까움→ 크게 분제되지 않는다고 할 수 있음 check point 3: residual plot check point 4: outlier or other suspicious trend 퀴즈 자료 변수 선택에 대한 R^2^ 확인# compare R-sqaured in regression # which one is the most important variable? summary(lm(mpg ~ disp)) summary(lm(mpg ~ hp)) summary(lm(mpg ~ wt)) summary(lm(mpg ~ accler)) 다중회귀모형: 데이터 탐색 데이터 탐색(EDA: Explanatory Data Analysis) 3rd model: a possible fitting method# more checking point plot(hp, mpg, col&#x3D;&quot;blue&quot;) subset 생상 (hp &lt; 50)## 2nd model : data split # subset data hp&lt;50 par(mfrow&#x3D;c(1,1)) car_s1&lt;-subset(car, hp&lt;50) plot(car_s1$hp, car_s1$mpg,col&#x3D;10, main&#x3D;&quot;hp&lt;50&quot;) # regression for hp&lt;50 summary(lm(car_s1$mpg ~ car_s1$hp)) 결과 해석 선형회귀식: mpg = 53.06 - 0.33 hp 선형회귀식의 결정계수: R^2^ = 0.45 퀴즈step(r1, direction&#x3D;&quot;backward&quot;) 3. 데이터 마이닝과 분류 분류규칙과 과적합 분류 분류분석(Classification Analysis) 다수의 속성(attribute, variable)d을 갖는 객체(object)를 그룹 또는 범주(class, category)로 분류 학습 표본(training sample)으로부터 효율적인 분류규칙(classification rule)을 생성 → 오분류율 최소화(minimize cost function) 분류규칙 예제 오분류율(misclassification rate) 오분류 객체 수 / 전체 객체 수 과적합(overfitting) 분류모형에서 훈련데이터에 과적합이 일어나면, 실제 데이터를 적용했을 때 더 높은 오분류율 발생 예측 모형에서의 과적합 예측 모형에서 훈련 데이터에 대한 과적합 모델을 선택하는 경우, 실제 데이터 적용 시 더 높은 오차가 발생 이를 방지하기 위해, 학습데이터와 검증데이터를 5:5, 6:4, 7:3, 8:2로 분리하여 모형 성능을 비교 평가함 교차검증 교차검증(cross-validation) 데이터 수집 training data / test data training data에 Classifier(분류기) 적용 output k-fold cross validation method 교차타당성 검증 5-fold cross-validation: n=100이면, 5등분으로 나누어 4등분은 학습데이터로 예측 모형을 구성하고, 나머지 5등분째 데이터로 검증 4. 학습테이터와 검증데이터iris 데이터 설명 iris 데이터(붓꽃 데이터) 목적: 꽃잎 폭과 길이에 관한 4개 변수로 꽃의 종류를 예측하는 것 타겟변수(y): setosa, versicolor, virginica # read csv file iris&lt;-read.csv(&quot;data&#x2F;week9_4&#x2F;iris.csv&quot;) head(iris) str(iris) attach(iris) input 변수(독립변수): Sepal.Length, Sepal.Width, Petal.Length, Petal.Width output 변수(종속변수): Species 학습데이터와 검증데이터 생성 iris 데이터: 150개 데이터# training&#x2F; test data : n&#x3D;150 set.seed(100, sample.kind&#x3D;&quot;Rounding&quot;) # seed 1000 지정한 이유: 동일한 데이터를 사용하기 위해 임의 숫자로 고정 N&#x3D;nrow(iris) # ramdom sampling을 위해 데이터에 number 부여 tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # 100개는 학습데이터, 50개는 검증데이터로 사용하라 tr.idx # attributes in training and test iris.train &lt;- iris[tr.idx, -5] # 5번째 열의 종속변수를 제외한 100개의 데이터 iris.test &lt;- iris[-tr.idx, -5] # 5번째 열의 종속변수를 제외한 50개의 데이터 iris 데이터의 타겟변수(학습데이터의 타겟변수, 검증데이터의 타겟변수)# target value in training and test trainLabels &lt;- iris[tr.idx, 5] testLabels &lt;- iris[-tr.idx, -5] 퀴즈# to get frequency of class in test set table(testLabels)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"데이터마이닝","slug":"데이터마이닝","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/"},{"name":"다중회귀모형","slug":"다중회귀모형","permalink":"https://ne-choi.github.io/tags/%EB%8B%A4%EC%A4%91%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"}],"author":"ne-choi"},{"title":"머신러닝 기법과 R 프로그래밍 1: XI. 서포트벡터머신","slug":"Study/Postech/머신러닝R/XI_서포트벡터머신","date":"2020-12-12T00:00:00.000Z","updated":"2021-02-01T00:44:57.709Z","comments":true,"path":"/2020/12/12/Study/Postech/머신러닝R/XI_서포트벡터머신/","link":"","permalink":"https://ne-choi.github.io/2020/12/12/Study/Postech/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9DR/XI_%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 머신러닝기법과 R프로그래밍 Ⅰ 과정입니다. XI. 서포트벡터머신1. 서포트벡터머신 ⅠSupport Vector Machine 머신러닝 지도학습 분류모델 kNN모델 SVM(서포트벡터머신) 비지도학습 군집모델 서포트벡터머신 장단점 장점 단점 상대적으로 정확도가 높음 해석상 어려움 발생 다양한 데이터(연속형, 범주형)를 다룰 수 있음 데이터가 많을 때 속도와 메모리가 많이 듦 선형 SVM 각 클래스를 분류해주는 margin(각 클래스를 분류하는 하이퍼플레인)을 최대화하는 벡터를 찾는 분석 기법 비선형 SVM 대부분 패턴은 선형적 분리가 불가능 비선형 패턴의 임력공간을 선형 패턴의 feature space로 변환 Kernel method로 비선형 경계면 도출 서포트백터머신 패키지와 함수 서포트벡터머신 수행을 위한 패키지: e1071 서포트벡터머신 함수: svm(y변수~x변수, data= ) # install.packages(&quot;e1071&quot;) library (e1071) iris&lt;-read.csv(&quot;data&#x2F;week11_1&#x2F;iris.csv&quot;) attach(iris) ## classification # 1. use all data iris$Species &lt;- as.factor(iris$Species) m1&lt;- svm(Species ~., data &#x3D; iris) summary(m1) svm 옵션(default) kernel = radial basis function gamma = 1/(# of dimension) (1/4=0.25) 서포트벡터머신 결과 svm 모델에 적용한 예측범주와 실제범주 비교(전체 데이터 사용 결과)# classify all data using svm result (m1) # first 4 variables as attribute variables x&lt;-iris[, -5] # iris 데이터엣 타겟값인 5번째 열을 제외한 데이터, 즉 4개의 독립변수만 있는 데이터 pred &lt;- predict(m1, x) # svm모델 m1을 적용해 예측된 범주값을 pred로 저장 # Check accuracy (compare predicted class(pred) and true class(y)) # y &lt;- Species or y&lt;-iris[,5] y&lt;-iris[,5] table(pred, y) 결과 해석 오분류율: (4+1)/150 = 0.033% 서포트벡터머신 시각화 iris 데이터의 서포트벡터머신 결과(전체 데이터 사용 결과)# visualize classes by color plot(m1, iris, Petal.Width~Petal.Length, slice&#x3D;list(Sepal.Width&#x3D;3, Sepal.Length&#x3D;4)) 결과 해석 4개 변수 중, petal.width와 petal.length가 중요한 변수 2. 서포트벡터머신 Ⅱ서포트벡터머신(kernel 함수) 커널(kernel)이란? Input Space → Feature Space f(x) = φ(x)^T^w + b 커널함수(kernel function) x의 기저함수(basis function) x에 대한 새로운 특징을 추출하는 변환함수 좋은 커널함수: x 데이터의 모든 정보를 보존하면서 class를 잘 분류할 수 있는 커널함수 커널함수와 기저함수의 관계: K(xi,xj) = φ(xi)’φ(xj) radial: K(xi,xj) = exp($\\frac{-||xi - xj||^2^}{2σ^2^}$) polynomial: K(xi,xj) = (xi‘xj + 1)^r^ sigmoid: K(xi,xj) = tanh(kxi‘xj - σ) 패키지 설치 서포트벡터머신 수행 패키지: e1071 오분류율 교차표(confusion matrix) 생성 패키지: caret# install.packages(&quot;caret&quot;) library(e1071) library(caret) iris 데이터 (학습데이터와 검증데이터 분할) # training (100) &amp; test set (50) set.seed(1000, sample.kind&#x3D;&quot;Rounding&quot;) N&#x3D;nrow(iris) tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) # target variable y&#x3D;iris[,5] # split train data and test data train&#x3D;iris[tr.idx,] test&#x3D;iris[-tr.idx,] 커널함수 적용 #svm using kernel m1&lt;-svm(Species~., data &#x3D; train) # default: radial summary(m1) m2&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;polynomial&quot;) summary(m2) m3&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;sigmoid&quot;) summary(m3) m4&lt;-svm(Species~., data &#x3D; train,kernel&#x3D;&quot;linear&quot;) summary(m4) 서포트벡터머신 결과 정확도 측정: kernel-radial basis function#measure accuracy pred11&lt;-predict(m1,test) # radial basis confusionMatrix(pred11, test$Species) #table(pred11, y[-tr.idx]) 결과 해석 예측범주: Prediction 실제범주: Reference 정확도: 0.96 정확도 측정: kernel-polynomialpred12&lt;-predict(m2,test) # polynomial confusionMatrix(pred12, test$Species) #table(pred12, y[-tr.idx]) 결과 해석 정확도: 0.9 정확도 측정: kernel-sigmoidpred13&lt;-predict(m3,test) # simoid confusionMatrix(pred13, test$Species) #table(pred13, y[-tr.idx]) 결과 해석 정확도: 0.9 정확도 측정: kernel-linearpred14&lt;-predict(m4,test) # linear confusionMatrix(pred14, test$Species) #table(pred14, y[-tr.idx]) 결과 해석 정확도: 0.9393 서포트벡터머신 정확도 정리 linear로만 처리해도 정확도 높은 경우가 많음 3. 서포트벡터머신 ⅢBreast Cancer 데이터# install.packages(&quot;e1071&quot;) # load package for support vector machine library(e1071) #svm model # install.packages(&quot;caret&quot;) # load package for Confusion matrix library(caret) # read data cancer&lt;-read.csv(&quot;data&#x2F;week11_3&#x2F;cancer.csv&quot;) head(cancer, n&#x3D;10) # remover X1 column(ID number) # ID number 필요 없는 feature로 제거 cancer&lt;-cancer[, names(cancer) !&#x3D; &quot;X1&quot;] attach(cancer) # 종속변수형을 factor 함수로 변경해야 함(R 4.0버전 이후) cancer$Y &lt;- as.factor(cancer$Y) Breast Cancer Wisconsin (Diagnostic) Data Set 세침흡인 세포검사를 통해 얻은 683개 유방조직의 9개 특성을 나타냄 input변수(독립변수): 9개, output변수(종속, 타겟변수): 1개 학습데이터 / 검증데이터 분할# training (455) &amp; test set (228) # set.seed(1000) N&#x3D;nrow(cancer) set.seed(998, sample.kind&#x3D;&quot;Rounding&quot;) # split train data and test data tr.idx&#x3D;sample(1:N, size&#x3D;N*2&#x2F;3, replace&#x3D;FALSE) train &lt;- cancer[ tr.idx,] test &lt;- cancer[-tr.idx,] 서포트벡터머신 결과 kernel 함수에 따른 서포트벡터머신#svm using kernel m1&lt;-svm(Y~., data &#x3D; train) summary(m1) #measure accuracy pred11&lt;-predict(m1,test) # radial basis confusionMatrix(pred11, test$Y) #table(pred11, y[-tr.idx]) 결과 해석 악성으로 오분류 4, 정상으로 오분류 1 정확도: 0.9781 m2&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;polynomial&quot;) summary(m2) pred12&lt;-predict(m2,test) # polynomial confusionMatrix(pred12, test$Y) #table(pred12, y[-tr.idx]) 결과 해석 악성으로 오분류 0, 정상으로 오분류 12 정확도: 0.9474 정확도는 94% 이상이나, 암 진단 특성상 정상으로 오분류하는 경우가 많으면 안 됨 False Positive(예측: P, 실제: N)가 더 위험한 경우 m3&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;sigmoid&quot;) summary(m3) pred13&lt;-predict(m3,test) # sigmoid confusionMatrix(pred13, test$Y) #table(pred13, y[-tr.idx]) 결과 해석 악성으로 오분류 5, 정상으로 오분류 3 정확도: 0.9649 m4&lt;-svm(Y~., data &#x3D; train,kernel&#x3D;&quot;linear&quot;) summary(m4) pred14&lt;-predict(m4,test) # linear confusionMatrix(pred14, test$Y) #table(pred14, y[-tr.idx]) 결과 해석 악성으로 오분류 1, 정상으로 오분류 3 정확도: 0.9825 전체 결론 정확도만 보았을 때 linear 모델이 적합하나, True Negative가 위험하기 때문에 radial 모델이 더 적합할 수 있음","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"SVM","slug":"SVM","permalink":"https://ne-choi.github.io/tags/SVM/"},{"name":"서포트벡터머신","slug":"서포트벡터머신","permalink":"https://ne-choi.github.io/tags/%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/"}],"author":"ne-choi"},{"title":"빅데이터 분석과 R 프로그래밍 2: Ⅷ. 선형회귀모형과 텍스트마이닝","slug":"Study/Postech/빅데이터분석R/Ⅷ_선형회귀모형과_텍스트마이닝","date":"2020-12-09T15:00:00.000Z","updated":"2021-02-01T00:42:26.217Z","comments":true,"path":"/2020/12/10/Study/Postech/빅데이터분석R/Ⅷ_선형회귀모형과_텍스트마이닝/","link":"","permalink":"https://ne-choi.github.io/2020/12/10/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A7_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95%EA%B3%BC_%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다. Ⅷ. 선형회귀모형과 텍스트마이닝1. 상관분석상관계수 상관계수: cor(변수1, 변수2) car &lt;- read.csv(&quot;week8_1&#x2F;autompg.csv&quot;) car1 &lt;- subset(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 | cyl&#x3D;&#x3D;8) attach(car1) cor의 디폴트는 pearson의 상관계수 kendall 상관계수 or spearman의 상관계수를 구할 때: cor(변수1, 변수2, method=c(“spearman”)) 상관계수와 산점도# new variable lists vars1&lt;-c(&quot;disp&quot;, &quot;wt&quot;, &quot;accler&quot;, &quot;mpg&quot;) # pariwise plot pairs(car1[vars1], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1)) 결과 해석 차량 무게와 배기량은 정비례관계 = 양의 상관계수 mpg(연비)와 (wt, disp)는 상관성이 높음 = 반비례, 음의 상관계수 cylinder별로 색상 표시(파랑: 4, 핑크: 6, 회색: 8) 상관분석 상관계수(r)은 절댓값이 0-1 사이 절댓값이 0에 가까울수록 상관관계가 없음 절댓값이 1에 가까울수록 강한 상관성이 있음 통계치와 그래프 Monkey 데이터 + King Kong 한 마리## Monkey data monkey&lt;-read.csv(&quot;week8_1&#x2F;monkey.csv&quot;) attach(monkey) # correlation coefficients cor(height, weight) # scatterplot for weight and height par(mfrow&#x3D;c(1, 1)) plot(height, weight, pch&#x3D;16, col&#x3D;3,main&#x3D;&quot;Monkey data&quot;) # add the best fit linear line (lec4_3.R) abline(lm(weight~height), col&#x3D;&quot;blue&quot;, lwd&#x3D;2, lty&#x3D;1) # linear model and summary of linear model m1&lt;-lm(weight~height) summary(m1) ## Monkey data + Kingkong monkey1 &lt;- read.csv(&quot;week8_1&#x2F;monkey_k.csv&quot;) head(monkey1) dim(monkey1) attach(monkey1) # correlation coefficients cor(height, weight) # scatterplot for weight and height par(mfrow&#x3D;c(1, 1)) plot(height, weight, pch&#x3D;16, col&#x3D;3,main&#x3D;&quot;Monkey data&quot;) # add the best fit linear line (lec4_3.R) abline(lm(weight~height), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) Monkey 데이터에 King Kong 한 마리 데이터 추가# linear model and summary of linear model for monkey+king kong m2&lt;-lm(weight~height) summary(m2) 결과 해석 한 마리의 킹콩 데이터가 몸무게와 신장의 상관관계 해석을 완전히 바꿀 수 있음 2. 선형회귀모형회귀분석: 단순회귀모형 단순회귀모형: lm(y변수~x변수, data= ) 종속변수: mpg(연비), 독립변수: wt(차량 무게) r1&lt;-lm(mpg~wt, data&#x3D;car1) summary(r1) anova(r1) 결과 해석 선형회귀식: y(mpg)= 46.60 - 0.0077(wt) 선형회귀식의 결정계수: R^2^= 0.709 산점도에 회귀선 그리기# scatterplot with best fit lines par(mfrow&#x3D;c(1,1)) plot(wt, mpg, col&#x3D;as.integer(car1$cyl), pch&#x3D;19) # best fit linear line abline(lm(mpg~wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) 코드 해석 col=as.factor(cyl): 배기통별로 컬러를 칠해라 plot(x축변수, y축변수) abline: add line(선 추가 함수) lm(y변수~x변수) lm = linear model(선형모형) 회귀분석의 목적 예측(prediction)과 추정(estimation) 선형모형: 독립변수와 종속변수 관계가 선형식으로 적합 최소자승법(least squares method): 예측값과 관측치간의 오차를 최소화하는 회귀계수를 추정 휘귀분석: 모형 적합도 회귀식에 의해 설명되는 부분(SSR), 설명되지 않는 부분(SSE) R^2^ = $\\frac{SSR}{SST}$ = %\\frac{17029}{24017} = 0.709 R^2^는 1에 가까울수록 회귀식에 의해 적합되는 부분이 높음 R^2^는 0에 가까우면 주어진 독립변수들에 의해 설명(예측 혹 적합)되는 부분이 없다고 할 수 있음 참고 SST = Total Sum of Squares SSR = Regression Sum of Squares SSE = error(residual) sum of squares 모형의 적합도와 결정계수 R^2^: 0 ≤ R^2^ ≤ 1 회귀모형: 단순회귀모형 종속변수: mpg(연비), 독립변수: disp(배기량) r2&lt;-lm(mpg~disp, data&#x3D;car1) summary(r2) anova(r2) 결과 해석 선형회귀식 y(mpg) = 35.49 - 0.0614(disp) 선형회귀식의 결정계수: R^2^ = 0.67 pariwise scatterplot# pariwise plot pairs(car1[,1:6], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1)) 퀴즈r1&lt;-lm(mpg~wt, data=car1) summary(r1) r3 &lt;-lm(mpg~wt+accler, data=car1) summary(r3) 3. 다중회귀모형 다중회귀모형: lm(y~x1+x2+x3, data= )# multiple Regression r2&lt;-lm(mpg~wt+accler, data&#x3D;car1) summary(r2) anova(r2) 결과 해석 선형회귀식 y(mpg) = 44.03 - 0.0057(wt) - 0.0176(disp) p-value도 매우 적은 값→ 두 변수 모두 유의함 r1 &lt;- lm(mpg~wt+disp, data&#x3D;car1) summary(r1) 결과 해석 선형회귀식의 결정계수 R^2^ = 0.7159 (비교: 단순회귀 wt, R^2^=0.709) 회귀분석: 잔차의 산점도 잔차산점도: 오차의 가정에 대한 적합성 # residual diagnostic plot layout(matrix(c(1,2,3,4),2,2)) # optrional 4 graphs&#x2F;page plot(r2) 오차에 대한 가정 잔차가 정규분포한다 평균은 0, 분산은 모두 동일하다 (선형회귀의 가정) 정규확률도(Normal Q-Q) 점선에 거의 붙어있으면 정규분포한다고 함 Residuals vs Leverage outlier를 찾아낼 때 사용 Residuals vs Fitted 분포 정도를 살펴봄 그룹별 회귀모형 mpg=f(wt), cylinder별# filtered data : regression by group car2&lt;-filter(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 ) car3&lt;-filter(car, cyl&#x3D;&#x3D;8) # car cyl&#x3D;4,6 vs cyl&#x3D;8 par(mfrow&#x3D;c(1,2)) plot(car2$wt, car2$mpg, col&#x3D;as.integer(car2$cyl), pch&#x3D;19, main&#x3D;&quot;cyl&#x3D;4 or 6&quot;) # best fit linear line abline(lm(car2$mpg~car2$wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) plot(car3$wt, car3$mpg, col&#x3D;&quot;green&quot;, pch&#x3D;19, main&#x3D;&quot;cyl&#x3D;8&quot;) # best fit linear line abline(lm(car3$mpg~car3$wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) # compare with total m2 &lt;- lm(mpg~wt, data&#x3D;car2) summary(m2) m3&lt;-lm(mpg~wt, data&#x3D;car3) summary(m3) m0&lt;-lm(mpg~wt, data&#x3D;car1) summary(m0) 4. 회귀분석의 진단과 평가회귀분석 데이터# subset of flight data in SFO (n&#x3D;2974) # dest&#x3D;&quot;SFO&quot;, origin&#x3D;&#x3D;&quot;JFK&quot;, arr_delay&lt;420, arr_delay&gt;0 SF&lt;-read.csv(&quot;week8_4&#x2F;SF_2974.csv&quot;, stringsAsFactors &#x3D; TRUE) attach(SF) library(ggplot2) library(dplyr) 데이터 기술통계치head(SF) str(SF) dim(SF) 데이터 산점도# 1. graphic analytics SF %&gt;% ggplot(aes(arr_delay)) + geom_histogram(binwidth &#x3D; 15) # 2. graphic analytics SF %&gt;% ggplot(aes(x &#x3D; hour, y &#x3D; arr_delay)) + geom_boxplot(alpha &#x3D; 0.1, aes(group &#x3D; hour)) + geom_smooth(method &#x3D; &quot;lm&quot;) + xlab(&quot;Scheduled hour of departure&quot;) + ylab(&quot;Arrival delay (minutes)&quot;) + coord_cartesian(ylim &#x3D; c(0, 120)) 회귀분석: 단순회귀모형 종속변수: arr_delay, 독립변수: hour(출발시간)# linear regression m1&lt;- lm(arr_delay ~ hour , data &#x3D; SF) summary(m1) 결과 해석 선형회귀식 y(arr_delay) = 7.54 + 2.55(hour) R^2^ = 0.03965 설명력이 높다고 하기 어려움 산점도에 회귀선 그리기# scatterplot with best fit lines library(dplyr) par(mfrow&#x3D;c(1,1)) SF %&gt;% plot(hour, arr_delay, col&#x3D;as.integer(SF$carrier), pch&#x3D;19)%&gt;% # best fit linear line abline(lm(arr_delay~hour), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) 결과 해석 출발 시간이 도착 지연 시간을 정확히 나타내지는 않음 회귀분석: 잔차의 산점도 회귀분석의 가정과 진단# residual diagnostic plot layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs&#x2F;page plot(m1) 잔차에 대한 해석을 하고 선형회귀모형을 적욯해도 되는지 확인하기 # pariwise plot pairs(car1[,1:6], main &#x3D;&quot;Autompg&quot;,cex&#x3D;1, col&#x3D;as.integer(car1$cyl),pch &#x3D;substring((car1$cyl),1,1))","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"}],"author":"ne_choi"},{"title":"빅데이터 분석과 R 프로그래밍 2: Ⅶ. R을 이용한 통계분석","slug":"Study/Postech/빅데이터분석R/Ⅶ_R을_이용한_통계분석","date":"2020-12-08T15:00:00.000Z","updated":"2021-02-01T00:42:19.485Z","comments":true,"path":"/2020/12/09/Study/Postech/빅데이터분석R/Ⅶ_R을_이용한_통계분석/","link":"","permalink":"https://ne-choi.github.io/2020/12/09/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A6_R%EC%9D%84_%EC%9D%B4%EC%9A%A9%ED%95%9C_%ED%86%B5%EA%B3%84%EB%B6%84%EC%84%9D/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다. Ⅶ. R을 이용한 통계분석1. 두 그룹간 평균비교(t-test)단일표본의 평균검정 t.test(변수, mu=검정하고자 하는 평균값) 가설 1: G3(최종성적)의 평균은 10인가? H0(null Hypothesis: 귀무가설): μ = 10stud&lt;-read.csv(&quot;week7_1&#x2F;stud_math.csv&quot;,stringsAsFactors &#x3D; TRUE) attach(stud) # single t-test t.test(G3, mu&#x3D;10) 결과 해석 t: t검정통계량, df: 자유도, p-value p-value 유의 수준 0.05에서는 G3 평균이 10이라고 할 수 있는 근거가 있음 alternative hypothesis 대립가설: 모형균은 10이 아니다 95% 신뢰구간: (9.96, 10.86), 표본평균값: 10.415 두 집단의 평균검정 두 평균 차이 비교 두 집단 표본평균 비교검정: t.test(타겟변수~범주형변수, data= ) 가설 2: 거주지역(R, U)에 따른 G3(최종성적) 평균에 차이가 있는가? # to test whether or not mean of G3 is same between Urban and Rural t.test(G3~address, data&#x3D;stud) boxplot(G3~address, boxwet&#x3D;0.5, col&#x3D;c(&quot;yellow&quot;, &quot;coral&quot;)) 결과 해석 p-value=0.03으로 유의수준 0.05에서 거주지역에 따라 G3은 유의한 차이가 있다고 할 수 있음 단측검정: 기각역이 한 쪽에만 있는 경우, alternative=c(“less”) or alternative=c(“greater”) 같으냐 다르냐가 아니라, 도심지역이 더 높으냐에 관해 test 하고 싶은 경우# alternative H : mu(Rural)&lt; mu(Urban) t.test(G3~address, data&#x3D;stud, alternative &#x3D; c(&quot;less&quot;)) 결과 해석 p-value=0.018로 유의수준을 0.05로 했을 때, 성적(Rural) &lt; 성적(Urban)이라고 할 수 있음 가설 3: 방과후 활동 여부(yes, no)에 따른 G3(최종성적) 평균에 차이가 있는가?# to test whether or not mean of G3 is equal for activities t.test(G3~activities, data&#x3D;stud) boxplot(G3~activities, boxwex &#x3D; 0.5, col &#x3D; c(&quot;blue&quot;, &quot;red&quot;)) 결과 해석 p-value=0.75로 유의수준 0.05보다 많이 큼= 검정통계량 값이 기각역에 있지 않다= 귀무가설(평균이 같다)을 기각할 수 없다= 방과후 활동 여부는 G3에 유의한 영향이 없다는 결론 신뢰구간(-1.05, 0.79) 사이에 0값이 있으면 차이가 없음을 의미 두 집단의 비모수적 비교검정 두 모집단의 비모수적 방법(Wilcoxon rank sum Test): wilcox.test(x,y) wilcos.test는 타겟변수가 등간척도(통증 정도, 만족도,,)일 때 사용 가능 # wilcoxon signed-rank test # wilcox.test(G3, mu&#x3D;10) wilcox.test(G3~address) 퀴즈t.test(G3~internet, data&#x3D;stud) 2. 짝을 이룬 그룹 간 비교짝을 이룬 그룹 간 비교(paired t-test) paired t-test: t.test(before, after, mu=0, paired=T) 특정 처리(treatment)의 효과를 비교분석할 때 사용 동일한 실험표본 before &amp; after 측정 예제: 혈압강하제 투약 효과, 방과후 프로그램 성과, 다이어트 프로그램 효과, 직무교육 후 생산성 향상 효과 방식: Before/After 차이를 구한 뒤, 평균과 편차를 계산해서 검정통계량을 구함 paired: 양측 검정, 유의한 차이가 있는지 없는지를 검정 예제 1: 고혈압 환자 10명에게 혈압강하제를 12주 동안 투여한 후, 복용 전의 혈압과 복용 후의 혈압을 비교하였다. 새로운 혈압강하제가 효과 있다고 말할 수 있는가? bp&lt;-read.csv(&quot;week7_2&#x2F;bp.csv&quot;) attach(bp) # two-sided t.test(bp_pre, bp_post, mu&#x3D;0, paired&#x3D;T) 결과 해석 p-value = 0.0015(매우 작음)로 유의수준 0.05보다 작기 때문에 H0를 기각→ 투약 전과 투약 후 혈압에 유의한 차이가 있다고 볼 수 있음 단측검정 옵션 주기 paired t-test: t.test(before, after, mu=0, alternative=”greater”, paired=T) 혈압(투약 전 - 투약 후) 차이가 0보다 큰가?# paired t-test (one-sided) t.test(bp_pre, bp_post, mu&#x3D;0, alternative&#x3D;&quot;greater&quot;, paired&#x3D;T) 결과 해석 p-value=0.0007로 유의수준 0.05보다 매우 작으므로 H0를 기간→ 투약효과가 매우 유의하다고 볼 수 있음 예제 2: 비만 대상자(성인)들에게 12주 동안 극저 칼로리 식이요법(very low-calorie diet: VLCD)을 실시한 후 효과를 비교하였다. 이 프로그램에 체중감소에 효과가 있다고 할 수 있는가? diet &lt;- read.csv(&quot;week7_2&#x2F;weight.csv&quot;) attach(diet) # paired t-test # 양측 검정: 극저칼로리 식이요법이 체중감량에 유의한 효과가 있는지 없는지에 관한 검정 t.test(wt_pre, wt_post, mu&#x3D;0, paired&#x3D;T) 결과 해석 p-value: 0.000001357 (0.001 = 1e-3) 3. 분산분석(ANOVA)분산분석의 개념 ANOVA(Analysis of Variance) 전체 분산(variance)을 분할(분석, analysis)하여 어떤 요인(factor)의 영향이 유의(significant)한지 검정하는 방법 factor가 한 개일 때 분산분석모형 적용 거주 지역에 따른 학업성취도: 거주지역(factor: R/U), 학업성적(1-20) 가설 1: 거주지역에 따라 G3에 유의한 차이가 있나? aov(타겟변수~factor)# ANOVA a1 &lt;- aov(G3~address) summary(a1) 결과 해석 p-value=0.035로 유의수준을 0.05로 할 때, 0,05보다 작으므로 거주지역에 따른 학업성적에 유의한 차이가 있다고 할 수 있음 # tapply - give FUN value by address round(tapply(G3, address, mean),2) 통학 시간에 따른 학업성취도: 통학시간(factor: 1-4), 학업성적(1-20) 가설 2: 통학 시간에 따라 G3에는 유의한 차이가 있나?traveltime&lt;-as.factor(traveltime) a2 &lt;- aov(G3~traveltime) summary(a2) 결과 해석 p-value=0.139로 유의수준 0.05 하에서는 통학 시간에 따른 학업성적에는 유의한 차이가 없다고 할 수 있음 단, p-value가 0.139로 너무 크지 않기 때문에 어느정도 차이가 존재함을 알 수 있음 # tapply - give FUN value by address round(tapply(G3, traveltime, mean),2) 사후검정(post-hoc analysis) 사후검정: ANOVA에서 어떤 factor의 유의성이 검정되면, 그 이후에 하는 검정 Tukey’s Honest Significant Difference Test# should be foctor for Tukey&#39;s Honest Significant Difference test TukeyHSD(a2, &quot;traveltime&quot;, ordered&#x3D;TRUE) plot(TukeyHSD(a2, &quot;traveltime&quot;)) 결과 해석 모든 pairwise 신뢰구간에 0이 포함됨→ 유의한 차이가 없음 추가 예제: 분산분석 연애경험 여부에 따른 학업성취도: 연애경험(yes, no), 학업성적(1-20)# 4. ANOVA by romantic a4 &lt;- aov(G3~romantic) summary(a4) # tapply - give FUN value by address round(tapply(G3,romantic, mean),2) # boxplot boxplot(G3~romantic, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;coral&quot;), main&#x3D;&quot;G3 by romantic&quot;) # posthoc analysis TukeyHSD(a4, &quot;romantic&quot;, ordered&#x3D;TRUE) plot(TukeyHSD(a4, &quot;romantic&quot;)) 퀴즈studytime &lt;- as.factor(studytime) a0 &lt;- aov(G3~studytime)summary(a0) tapply(G3, studytime, mean) 4. 이원분산분석 이원분산분석(two-way ANOVA) 데이터: High-Density Lipoprotein (HDL) 콜레스테롤 HDL(고밀도 리포 단백질): 높을수록 좋은 것으로 알려진 콜레스테롤, 40mg/dl 이상이 정상 범위 factor가 2개 투약효과가 있는가? 5mg, 10mg, 위약 연령그룹(young/old)에 따른 영향이 있는가? 가설 1. 신약 투약효과가 있나? HDL을 상승시키는 효과가 있나? 가설 2. 연령그룹에 따라 투약효과(HDL변화)에 차이가 있나? 가설 3. 신약의 투약과 연령그룹 간 상호작용 효과가 있나? 이원분산분석: aov(타겟변수~factor1 + factor2) dat &lt;- read.csv(&quot;week7_4&#x2F;chol_ex.csv&quot;) attach(dat) # two-way ANOVA a6 &lt;- aov(value ~ drug + age) summary(a6) 결과 해석 drug effect: p-value~0이므로 HDL값에 통계적으로 유의한 차이가 있음 age: p-value=0.19로 유의수준 0.05에서 유의한 차이는 없음 aov(타겟변수~factor1 + factor2 + 상호작용) 두 개의 factor 간 상호작용의 유의성을 검정하기 위한 분석# two-way ANOVA with interaction a7 &lt;- aov(value ~ drug + age+ drug*age) summary(a7) 결과 해석 drug와 age 그룹 간 상호작용: p-value=0.286으로 유의수준 0.05에서 유의한 차이는 없음 두 요인의 상자그림 투약용량과 연령그룹에 따른 상자그림# two-way ANOVA par(mfrow&#x3D;c(1,2)) boxplot(value ~ drug, boxwex &#x3D; 0.7, main&#x3D;&quot;HDL by drug dose&quot;, col &#x3D; c(&quot;yellow&quot;,&quot;orange&quot;, &quot;green&quot;)) boxplot(value ~ age, boxwex &#x3D; 0.5, main&#x3D;&quot;HDL by Age&quot;, col &#x3D; c(&quot;blue&quot;, &quot;coral&quot;)) drug effect: 10mg인 경우, HDL 상승효과가 가장 높음# tapply - give FUN value by drug round(tapply(value, drug, mean),2) age: young 그룹(18-40)의 HDL 상승효과가 더 높음# tapply - give FUN value by age round(tapply(value, age, mean),2) 상호작용 그래프# interaction plot par(mfrow&#x3D;c(1,2)) interaction.plot(drug, age, value) interaction.plot(age, drug, value) 결과 해석 투약용량 10mg에서 young 그룹 상승효과가 old 그룹보다 훨씬 높음 5mg에서와 placebo에서는 연령그룹 차이가 거의 없음 # Create an interaction plot for the HDL data par(mfrow&#x3D;c(1,2)) interaction.plot(dat$drug, dat$age, dat$value) interaction.plot(dat$age, dat$drug, dat$value) # two-way ANOVA model with interaction results &#x3D; lm(value ~ drug + age + drug*age, data&#x3D;dat) anova(results) 퀴즈a00 &lt;- aov(G3studytime+sex)summary(a00)boxplot(G3sex, boxwex = 0.5, col = c(“yellow”, “coral”))","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"}],"author":"ne_choi"},{"title":"빅데이터 분석과 R 프로그래밍 2: Ⅵ. 데이터 탐색","slug":"Study/Postech/빅데이터분석R/Ⅵ_데이터_탐색","date":"2020-12-07T15:00:00.000Z","updated":"2021-02-01T00:42:10.517Z","comments":true,"path":"/2020/12/08/Study/Postech/빅데이터분석R/Ⅵ_데이터_탐색/","link":"","permalink":"https://ne-choi.github.io/2020/12/08/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A5_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%83%90%EC%83%89/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다. Ⅵ. 데이터 탐색1. 데이터 다루기 데이터 결합, 분할, 정렬 데이터 결합: merge merge(data1, data2, by=”ID”)# practice data dat1&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data1.csv&quot;) dat2&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data2.csv&quot;) # data merging dat12&lt;-merge(dat1, dat2, by&#x3D;&quot;ID&quot;) 데이터 결합: rbind 두 개 데이터를 행(row)로 결합# add more data (combine in a row) dat3&lt;-read.csv(file&#x3D;&quot;week6_1&#x2F;data3.csv&quot;) dat123&lt;-rbind(dat12, dat3) dat123 데이터 정렬: order 데이터명[order(변수1, 변수2)] # data sorting dats1&lt;-dat12[order(dat12$age),] dats1 dats2&lt;-dat12[order(dat12$gender, dat12$age), ] dats2 데이터 추출: subset subset(데이터명, 조건1 &amp; 조건2) # data subset (selecting data) #newdat&lt;-dat12[which(dat12$gender&#x3D;&#x3D;&quot;F&quot; &amp; dat12$age&gt;15)] newdat&lt;-subset(dat12, dat12$gender&#x3D;&#x3D;&quot;F&quot; &amp; dat12$age&gt;15) newdat 일부 변수 제거 데이터이름[!names(데이터) %in% c(“변수1”, “변수2)] # excluding variables exdat&lt;-dat12[!names(dat12) %in% c(&quot;age&quot;,&quot;gender&quot;)] exdat 퀴즈new &lt;- subset(dat123, dat123$hourwk &gt;&#x3D; 10 &amp; dat123$alcohol&#x3D;&#x3D; &quot;yes&quot;) new 2. 데이터 기술통계치 요약 데이터: 학업성취도(포르투칼 고등학생 수학점수) stud&lt;-read.csv(&quot;week6_2&#x2F;stud_math.csv&quot;) head(stud) dim(stud) str(stud) attach(stud) summary summary(데이터명): 변수별로 요약통계량 제공 문자변수에는 빈도를 주고, 숫자변수에는 최솟값, 25%, 중위값, 75%, 최댓값 제공 # 1-1 Numeriacl analytics summary(stud) mean(G3) # 평균 sd(G3) # 표준편차(분산의 제곱근) var(G3) # 분산 vars 특정 변수에 관한 요약통계량: var &lt;- c(“변수1”, “변수2”, “변수3”) # creating interested variable list vars&lt;-c(&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;) head(stud[vars]) summary(stud[vars]) describe summary보다 더 구체적인 요약통계량 얻을 수 있음 psych 패키지 필요# install.packages(&quot;psych&quot;) library(psych) # require &quot;psych&quot; for &quot;describe&quot; function describe(stud[vars]) table 범주형 변수 요약: table(변수이름) # categorical data table(health) health_freq&lt;-table(health) names(health_freq) &lt;- c (&quot;very bad&quot;, &quot;bad&quot;, &quot;average&quot;, &quot;good&quot;, &quot;very good&quot;) barplot(health_freq, col&#x3D;3) 2*2 분할표# 2*2 contingency table table(health,studytime) 3. 그래프를 이용한 데이터 탐색히스토그램 1학년, 2학년, 3학년 성적의 분포# 1. histogram with color and title, legend par(mfrow&#x3D;c(2,2)) hist(G1, breaks &#x3D; 10, col &#x3D; &quot;lightblue&quot;, main&#x3D;&quot;Histogram of Grade 1&quot; ) hist(G2, breaks &#x3D; 10, col &#x3D; &quot;green&quot;, main&#x3D;&quot;Histogram of Grade 2&quot; ) hist(G3, breaks &#x3D; 10, col &#x3D; &quot;coral&quot;, main&#x3D;&quot;Histogram of Grade 3&quot; ) 상자그림 거주 지역에 따른 G3, 통학 시간에 따른 G3par(mfrow&#x3D;c(1,2)) boxplot(G3~address, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;coral&quot;), main&#x3D;&quot;G3 by (Urban, Rural)&quot;) boxplot(G3~traveltime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;red&quot;,&quot;orange&quot;,&quot;yellow&quot;,&quot;green&quot;), main&#x3D;&quot;G3 by traveltime&quot;) 결과 해석 도심지역 학생 성적이 외곽지역 학생보다 높음 통학시간이 짧은(15분 이내) 학생 성적이 높음 자유시간에 따른 G3, 공부시간에 따른 G3par(mfrow&#x3D;c(1,2)) boxplot(G3~freetime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;grey&quot;, &quot;red&quot;)) boxplot(G3~studytime, boxwex &#x3D; 0.5, col &#x3D; c(&quot;yellow&quot;, &quot;blue&quot;, &quot;grey&quot;, &quot;red&quot;)) 결과 해석 방과후 자유시간에 따른 G3 차이: 적은편이라고 응답한 학생 성적이 다소 높음 주중 공부 시간이 5시간 이상인 학생 성적이 높은 편 lattice 패키지 통학시간과 최종성적(G3)의 멀티패널 그림, 성별 library(lattice) xyplot(G3~traveltime | sex, data&#x3D;stud, pch&#x3D;16, main&#x3D;&quot;G3 ~ traveltime | sex&quot;) 결과 해석 대부분 학생은 30분 이내 통학거리 통학거리가 짧은 학생 성적 평균이 다소 높게 나타남 통학거리 1시간 이상은 표본이 상대적으로 적음 추가: 0점인 데이터가 확인되어 점검이 필요함 # find a data G3 &#x3D; 0 s1 &lt;- subset(stud, G3&#x3D;&#x3D;0) s1 산점도library(ggplot2) ggplot(stud, aes(x&#x3D;G1, y&#x3D;G3, color&#x3D;sex, shape&#x3D;sex)) + geom_point(size&#x3D;2) 막대그림# bar chart for romantic by sex ggplot(data&#x3D;stud, aes(factor(romantic)))+geom_bar(aes(fill&#x3D;factor(sex)), width&#x3D;.4, colour&#x3D;&quot;black&quot;)+ ggtitle(&quot;Romantic by sex&quot;) 결과 해석 연애 경험 있는 경우, 여학생 비율이 높음 # bar chart for internet use by (Urban, Rural) ggplot(data&#x3D;stud, aes(factor(internet)))+geom_bar(aes(fill&#x3D;factor(address)), width&#x3D;.4, colour&#x3D;&quot;black&quot;)+ggtitle(&quot;Internet use by (Urban, Rural)&quot;) 결과 해석 인터넷 사용자 중에는 도심지역에 사는 경우가 훨씬 많음 pariwise plot pairwise scatterplot: pairs(변수리스트) # new variable lists vars1&lt;-c(&quot;G1&quot;, &quot;G2&quot;, &quot;G3&quot;) # pariwise plot pairs (stud[vars1], main &#x3D; &quot;Student Math Data&quot;, pch &#x3D; 21,bg &#x3D; c (&quot;red&quot;,&quot;green3&quot;)[unclass(stud$sex)]) 결과 해석 G1, G2, G3 상관성은 매우 높음 성별 차이는 없음 4. 데이터의 정규성검정과 신뢰구간데이터의 정규성검정 정규확률도(Normal Q-Q plot): 데이터가 정규분포하는가? # Testing for normality # multiple plot (2 by 2) par(mfrow&#x3D;c(2,2)) #Quantile plot qqnorm(G1) qqline(G1, col &#x3D; 2, cex&#x3D;7) qqnorm(G2) qqline(G2, col &#x3D; 2, cex&#x3D;7) qqnorm(G3) qqline(G3, col &#x3D; 2, cex&#x3D;7) 정규분포 정규분포(Normal distribution) 정규분포 적합성검정: 데이터가 정규분포하는지에 대한 검정 Shapiro-Wilks 검정#Shapiro-Wilks test shapiro.test(G3) 결과 해석 p-value가 0에 가까워서 정규분포한다고 볼 수 없음 Anderson-Darling 검정(패키지 설치 필요)#Anderson-Darling test require installing package &quot;nortest&quot; # install.packages(&#39;nortest&#39;) library(nortest) ad.test(G3) 결과 해석 p-value가 0에 가까워서 정규분포한다고 볼 수 없음 확률분포함수 사용 함수 분포함수 설명 binom(x) 이항분포 rbinom(5, size=100, prob-.2) exp(x) 지수분포 gamma(X) 감마분포 rgamma(5, shape=3, rate=2) norm(x) 정규분포 rnorm(50, mean=10, sd=5) pois(x) 포아송분포 rpois(n, lambda) unif(x) 균일분포 runif(30) p: 누적함수 / d: 확률밀도함수 / q: quantile 함수 / r: 랜덤넘버 생성 확률분포함수로부터 데이터 생성# Simulation examples runif(5,min&#x3D;1,max&#x3D;5) rnorm(5,mean&#x3D;5,sd&#x3D;1) rgamma(5,shape&#x3D;3,rate&#x3D;2) rbinom(5,size&#x3D;100,prob&#x3D;.2) # from normal distribution x&lt;-rnorm(1000) plot(density(x),xlim&#x3D;c(-5,10)) 데이터 생성(정규분포(평균=500, 편차=100)에서 100개 데이터 생성) set.seed(5)mean(rnorm(1000)) # confidence interval of normal distribution nreps &lt;- 100 ll &lt;- numeric(nreps) ul &lt;- numeric(nreps) n &lt;- 100 mu &lt;- 500 sigma &lt;- 100 for(i in 1:nreps) &#123; set.seed(i) x &lt;- rnorm(n, mu, sigma) ll[i] &lt;- mean(x) - qnorm(0.975)*sqrt(sigma^2&#x2F;n) ul[i] &lt;- mean(x) + qnorm(0.975)*sqrt(sigma^2&#x2F;n) &#125; # Draw 95% confidence interval(신뢰 구간) # 95%: 100개 중 5개는 모평균을 포함하고 있지 않다 # 95%: 100개 중, 5개는 오류가 있을 것이다 par(mfrow&#x3D;c(1,1)) plot(1:100, ylim&#x3D;c(min(ll), max(ul)), ylab&#x3D;&quot;95% Confidence Interval&quot;, xlab&#x3D;&quot;iterations&quot;) for(i in 1:100) lines(c(i, i), c(ll[i], ul[i])) abline(h&#x3D;mu, col&#x3D;&quot;red&quot;, lty&#x3D;2, lwd&#x3D;3) 신뢰구간의 의미 신뢰수준, 표본오차 전국 유권자 1,500명 조사 결과, A 후보 지지율은 45%이며, 95% 신뢰수준에서 오차한계는 3.5%이다.→ 지지도에 대한 95% 신뢰구간: 표본 지지율 +- 오차한계 (41.5%, 48.5%)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"}],"author":"ne_choi"},{"title":"빅데이터 분석과 R 프로그래밍 2: Ⅴ. R 그래픽 Ⅱ","slug":"Study/Postech/빅데이터분석R/Ⅴ_R_그래픽_Ⅱ","date":"2020-12-06T15:00:00.000Z","updated":"2021-02-01T00:42:03.814Z","comments":true,"path":"/2020/12/07/Study/Postech/빅데이터분석R/Ⅴ_R_그래픽_Ⅱ/","link":"","permalink":"https://ne-choi.github.io/2020/12/07/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A4_R_%EA%B7%B8%EB%9E%98%ED%94%BD_%E2%85%A1/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅱ 과정입니다. Ⅴ. R 그래픽 Ⅰ1. R 그래픽: ggplot2데이터 시각화 R 기본 그래픽스(Base program) Lattice 그래픽스: 직교형태의 멀티패널 툴 ggplot2 그래픽 시스템: Hadley Wickham이 구현 Grammar of Graphic 개념은 그래픽 생성 시, 각 요소를 구분해 취급한다는 의미 Incremental graphic: 기본 R 그래픽스보다 인터액티브한 그래프 그리기 가능, 기초 그림 생성 후 그래픽스 요소를 필요에 따라 붙이거나 수정 lattice lattice: 직교형태의 그래픽 툴 xyplot: 산점도 bwplot: box whiskers plot, 상자그림 dotplot levelplot stripplot: 점을 함께 표시한 상자그림 splom: 산점도 매트릭스 contourplot: 등고선그림 lattice 설치와 활용# install.packages(&#39;lattice&#39;) library(lattice) ethanol&lt;-read.csv(&quot;week5_1&#x2F;ethanol.csv&quot;) # ethanol data in lattice help(&quot;ethanol&quot;) head(ethanol) dim(ethanol) str(ethanol) lattice 함수 xyplot을 이용한 그래프(기본 산점도와 동일)# basic plot xyplot(NOx ~ E , data &#x3D; ethanol, main &#x3D; &quot;Single Panel by xyplot&quot;) xyplot을 이용한 멀티패널 산점도: xyplot(y변수 ~ x변수|조건부변수, data=)# multi panel graph xyplot(NOx ~ E | C , data &#x3D; ethanol, main &#x3D; &quot;NOx ~ E | C &quot;) xyplot을 이용한 멀티패널 산점도: subset 데이터# multi panel graph for subset xyplot(NOx ~ E | C , data &#x3D; ethanol, subset &#x3D; C &gt; 8, main &#x3D; &quot;NOx ~ E | C , data &#x3D; ethanol, subset &#x3D; C &gt; 8&quot;) 2. R 그래픽: ggplot2의 활용library(ggplot2) car&lt;-read.csv(&quot;week5_2&#x2F;autompg.csv&quot;) head(car) str(car) # subset of car : cyl (4,6,8) car1&lt;-subset(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 | cyl&#x3D;&#x3D;8) attach(car1) # 5-1 ggplot2 for scatterplot # Color and shape display by factor (categorical variable) # check the variable type(integer, numeric, factor) and define it str(car1) car1$cyl&lt;-as.factor(car1$cyl) # Now, you can draw one of plot using ggplot par(mfrow &#x3D; c(1, 1)) ggplot(car1, aes(x&#x3D;wt, y&#x3D;disp, color&#x3D;cyl, shape&#x3D;cyl)) + geom_point(size&#x3D;3, alpha&#x3D;0.6) 객체 해석 ggplot 함수에 데이터는 car1을 이용하고, x축에는 wt(차의 무게)를, y축에는 disp(배기량)의 산점도를 그리고, 점 색상은 cyl(실린더 수)로 표현한다. geom_point는 size=3(숫자 클수록 점 크기가 커짐) 그래프 그리기 mpg 크기를 표시한 그래프# mapping (continuous variable : mpg) on the scatterplot for wt and disp ggplot(car1, aes(x&#x3D;wt, y&#x3D;disp, color&#x3D;mpg, size&#x3D;mpg)) + geom_point(alpha&#x3D;0.6) 그래프 설명 차 무게와 배기량의 산점도에 연비의 높고 낮음을 원의 크기와 색으로 표시한 그래프 ggplot2 구조 ggplot 기본 ggplot: 새로운 ggplot을 생성 aes: aesthetic mapping을 구성(데이터, 그래프구조) qplot: 즉석 그림 geom(geometric) 함수군 geom_abline, geom_hline, geom_vline geom_bar geom_point geom_boxplot geom_map geom_smooth, stat_smooth ggplot2 그래프 geom_bar를 이용한 단계별 그래프 설명p1 &lt;- ggplot(car1, aes(factor(cyl), fill &#x3D; factor(cyl))) p1 # barplot define p1 &lt;- p1 + geom_bar(width&#x3D;5) p1 # output by origin(1,2,3) p1 &lt;- p1 + facet_grid(. ~ origin) p1 # 위와 똑같은 그래프 그리는 법 ggplot(car1, aes(factor(cyl), fill&#x3D;factor(cyl)))+ geom_bar(width&#x3D;.5)+ facet_grid(. ~ origin) geom_bar를 이용한 누적 막대그래프# geom_bar : asethetic mapping (4,6,8 cyl) p &lt;- ggplot(data&#x3D;car1, aes(factor(cyl))) p + geom_bar(aes(fill&#x3D;factor(origin)), colour&#x3D;&quot;black&quot;) # for Korean title # 5-2-2 : geom_bar : asethetic mapping (4,6,8 cyl) par(family&#x3D;&quot;나눔고딕&quot;, cex&#x3D;1.3) p &lt;- ggplot(data&#x3D;car1, aes(factor(cyl))) p &lt;- p + geom_bar(aes(fill&#x3D;factor(origin)), colour&#x3D;&quot;black&quot;) p &lt;- p+ggtitle(&quot;자동차 데이터&quot;) p ggplot 산점도에 회귀선 넣기# step1 : # Use hollow circles ggplot(car1, aes(x&#x3D;wt, y&#x3D;mpg))+geom_point(shape&#x3D;1) # by default includes 95% confidence region ggplot(car1, aes(x&#x3D;wt, y&#x3D;mpg)) + geom_point(shape&#x3D;1) + geom_smooth(method&#x3D;lm) lm: linear model 선형 회귀식 선형식의 95% 신뢰구간이 dafault로 그려짐 # excluding 95% confidence region ggplot(car1, aes(x&#x3D;wt, y&#x3D;mpg)) + geom_point(shape&#x3D;1) + geom_smooth(method&#x3D;lm, se&#x3D;FALSE) ggplot 산점도에 비선형회귀식 적합# geom_smooth() use loess ggplot(car1, aes(x&#x3D;wt, y&#x3D;mpg)) + geom_point(shape&#x3D;1) + geom_smooth(method&#x3D;&quot;loess&quot;) loess: local polynomial regression 3. R 그래픽: 3D와 히트맵3D scatterplotlibrary(scatterplot3d) # 5-3 3D scatterplot with data trees data(trees) par(mfrow &#x3D; c(1, 1)) s3d &lt;- scatterplot3d(trees, type&#x3D;&quot;h&quot;, highlight.3d&#x3D;TRUE, angle&#x3D;55, scale.y&#x3D;0.7, pch&#x3D;16, main&#x3D;&quot;scatterplot3d - 5&quot;) # to know about data &quot;trees&quot; help(trees) head(trees) # export to csv file write.csv(trees,file&#x3D;&quot;trees.csv&quot;, row.names &#x3D; FALSE) 3D 산점도에 선형식 추가 attach(trees) my.lm &lt;- lm(Volume ~ Girth + Height) s3d$plane3d(my.lm, lty.box &#x3D; &quot;solid&quot;) 히트맵(heatmap) 히트맵: 통계치를 구한 후, 크기에 비례하여 그라데이션 색상으로 표현한 시각화 기법 히트맵의 입력값 형태: 숫자형태의 행렬 Autompg 데이터의 상관계수를 이용한 히트맵 attach(car) par(mfrow&#x3D;c(1, 1)) cor.x&lt;-cor(car[,1:6]) heatmap(cor.x, symm&#x3D;TRUE) 데이터: USArrest # Crime rate by US State (1973) # Arrests per 100,000 residents for assault, murder, and rape # in each of the 50 US states in 1973 help(USArrests) head(USArrests) cor(USArrests) round(cor(USArrests), 2) # subset excluding 3th variable UrbanPop # matrix format for heatmap x &lt;- as.matrix(USArrests[, -3]) result &lt;- heatmap(x, scale&#x3D;&quot;column&quot;, Colv&#x3D;NA, cexCol&#x3D;1, main&#x3D;&quot;Violent Crime Rates by US State (1973)&quot;) row.names(USArrests)[result$rowInd[1:10]] row.names(USArrests)[result$rowInd[35:50]] 4. R 그래픽: 공간지도 분석추가 패키지 설치# maps : world map install.packages(&quot;maps&quot;) library(maps) # mapdata : more world map install.packages(&quot;mapdata&quot;) library(mapdata) # mapdata : latitude and longitude install.packages(&quot;mapproj&quot;) library(mapproj) 나라 지도 추출 # 1. Korea Map par(mfrow &#x3D; c(1, 2),mar&#x3D;c(2,2,2,2)) map(database &#x3D; &#39;world&#39;, region &#x3D; c(&#39;South Korea&#39;,&#39;North Korea&#39;), col&#x3D;&#39;green&#39;, fill &#x3D; TRUE) title(&quot;Korea&quot;) # using mapdata package map(database &#x3D; &#39;worldHires&#39;, region &#x3D; c(&#39;South Korea&#39;,&#39;North Korea&#39;), col&#x3D;&#39;green&#39;, fill &#x3D; TRUE) title(&quot;Korea&quot;) # 2.Italy par(mfrow &#x3D; c(1, 1),mar&#x3D;c(2,2,2,2)) map(database &#x3D; &#39;world&#39;, region &#x3D; c(&#39;Italy&#39;), col&#x3D;&#39;coral&#39;, fill &#x3D; TRUE) title(&quot;Italy&quot;) 위도, 경도 활용하여 독도 표시 # 3. Dokdo using mapproj package library(mapproj) par(mfrow &#x3D; c(1, 1),mar&#x3D;c(2,2,2,2)) map(&#39;world&#39;, proj &#x3D; &#39;azequalarea&#39;, orient &#x3D; c(37.24223, 131.8643, 0)) map.grid(col &#x3D; 2) points(mapproject(list(y &#x3D; 37.24223, x &#x3D; 131.8643)), col &#x3D; &quot;blue&quot;, pch &#x3D; &quot;x&quot;, cex &#x3D; 2) title(&quot;Dokdo&quot;) # for reading Korean : encoding to UTF-8 # file menu: Tools_global options_code_saving 공간지도분석 예제 1 국내 공항 및 노선 현황 # 4. Airport &amp; route data (source : https:&#x2F;&#x2F;www.data.go.kr&#x2F;) airport&lt;-read.csv(&quot;week5_4&#x2F;airport.csv&quot;) route &#x3D; read.csv(&quot;week5_4&#x2F;route.csv&quot;) head(airport) head(route) head(route[order(route$id),]) library(dplyr) # Korea map (kr.map) - using dplyr world.map &lt;- map_data(&quot;world&quot;) kr.map &lt;- world.map %&gt;% filter(region &#x3D;&#x3D; &quot;South Korea&quot;) # 5. Domestic airport location ggplot() + geom_polygon(data&#x3D;kr.map, aes(x&#x3D;long, y&#x3D;lat, group&#x3D;group)) + geom_label(data&#x3D;airport, aes(x &#x3D; lon, y &#x3D; lat, label&#x3D;iata)) + labs(title &#x3D; &quot;south korea airports&quot;) ggplot은 레이어를 추가하는 방식으로 그래픽을 구현함 공간지도분석 예제 2 미국 해엉 데이터: 지도 데이터베이스와 행정자료 결합(미국 1973년 범죄수 지도) # 6. Assault in US (1973) par(mfrow &#x3D; c(1, 1), mar&#x3D;c(1,1,1,1)) # library(maps) # excluding Alaska, Hawaii sub.usa &lt;- subset(USArrests,!rownames(USArrests) %in% c(&quot;Alaska&quot;, &quot;Hawaii&quot;)) # data with State name, Assult count usa.data &lt;- data.frame(states &#x3D; rownames(sub.usa), Assault &#x3D; sub.usa$Assault) # legend col.level &lt;- cut(sub.usa[, 2], c(0, 100, 150, 200, 250, 300, 350)) legends &lt;- levels(col.level) # displaying color for the size levels(col.level) &lt;- sort(heat.colors(6), decreasing &#x3D; TRUE) usa.data &lt;- data.frame(usa.data, col.level &#x3D; col.level) # Map map(&#39;state&#39;, region &#x3D; usa.data$states, fill &#x3D; TRUE, col &#x3D; as.character(usa.data$col.level)) title(&quot;USA Assault map&quot;) legend(-77, 34, legends, fill &#x3D; sort(heat.colors(6), decreasing &#x3D; TRUE), cex &#x3D; 0.7)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"lattice","slug":"lattice","permalink":"https://ne-choi.github.io/tags/lattice/"}],"author":"ne_choi"},{"title":"빅데이터 분석과 R 프로그래밍 1: Ⅳ. R 그래픽 Ⅰ","slug":"Study/Postech/빅데이터분석R/Ⅳ_R_그래픽_Ⅰ","date":"2020-12-05T15:00:00.000Z","updated":"2021-02-01T00:41:57.309Z","comments":true,"path":"/2020/12/06/Study/Postech/빅데이터분석R/Ⅳ_R_그래픽_Ⅰ/","link":"","permalink":"https://ne-choi.github.io/2020/12/06/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A3_R_%EA%B7%B8%EB%9E%98%ED%94%BD_%E2%85%A0/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅰ 과정입니다. Ⅳ. R 그래픽 Ⅰ1. R 그래픽 Ⅲ: 히스토그램히스토그램 (1차원) 데이터 불러오기# read brain data brain &lt;- read.csv(file&#x3D;&quot;week4_1&#x2F;brain2210.csv&quot;) # attach 적용하기 attach(brain) 히스토그램: hist(변수이름)hist(brain$wt) hist(wt) # 색상 선택 hist(wt, col &#x3D; &quot;lightblue&quot;) 히스토그램(색과 제목): hist(변수이름, col=”colname”, main=” “)# histogram with color and title, legend hist(wt, breaks &#x3D; 10, col &#x3D; &quot;lightblue&quot;, main&#x3D;&quot;Histogram of Brain weight&quot;) 색(657가지 색) colors() → 모든 색의 이름을 볼 수 있음# see rgb values for 657 colors, choose what you like # colors() # select colors including &quot;blue&quot; grep(&quot;blue&quot;, colors(), value&#x3D;TRUE) 밀도함수 그려보기# fit function(find density function) par(mfrow&#x3D;c(1,1)) d &lt;- density(brain$wt) plot(d) 그룹별 히스토그램(동일한 x축, y축 범위): xlim, ylim# histogram with same scale # 2 multiple plot par(mfrow&#x3D;c(2,1)) # 그래프 화면 분할을 2행 1열로 하라는 의미 brainf&lt;-subset(brain,brain$sex&#x3D;&#x3D;&#39;f&#39;) hist(brainf$wt, breaks &#x3D; 12,col &#x3D; &quot;green&quot;, xlim&#x3D;c(900,1700),ylim&#x3D;c(0,20),cex&#x3D;0.7, main&#x3D;&quot;Histogram with Normal Curve (Female)&quot;, xlab&#x3D;&quot;brain weight&quot;) brainm&lt;-subset(brain,brain$sex&#x3D;&#x3D;&#39;m&#39;) hist(brainm$wt, breaks &#x3D; 12,col &#x3D; &quot;orange&quot;,xlim&#x3D;c(900,1700),ylim&#x3D;c(0,20), main&#x3D;&quot;Histogram with Normal Curve (Male)&quot;, xlab&#x3D;&quot;brain weight&quot;) grep(“violet”, colors(), value=TRUE) 2. R 그래픽 Ⅱ: 상자그림, 파이차트상자그림(Boxplot, 1차원) 상자그림: boxplot(변수이름, col=”green”) # boxplot par(mfrow&#x3D;c(1,2)) # boxplot for all data boxplot(brain$wt, col&#x3D;c(&quot;coral&quot;)) 그룹별 상자그림: boxplot(변수이름~그룹이름, col=c(“col1”, “col2”) # boxplot by gender (female, male) boxplot(brain$wt~brain$sex, col &#x3D; c(&quot;green&quot;, &quot;orange&quot;)) #- 수평 상자그림: boxplot(변수이름, col&#x3D;&quot;colname&quot;, horizontal&#x3D;TRUE) &#96;&#96;&#96;&#123;r&#125; par(mfrow&#x3D;c(1,1)) boxplot(brain$wt~brain$sex, boxwex&#x3D;0.5, horizontal&#x3D;TRUE, col &#x3D; c(&quot;grey&quot;, &quot;red&quot;)) 박스플롯 폭 조정 옵션: boxwex= par(mfrow&#x3D;c(1,2)) boxplot(brain$wt, boxwex &#x3D; 0.25, col &#x3D; c(&quot;coral&quot;), main &#x3D; &quot;Boxplot for all data&quot;) boxplot(brain$wt, boxwex &#x3D; 0.5, col&#x3D;c(&quot;coral&quot;), main&#x3D;&quot;Boxplot for all data&quot;) 상자그림에 기술통계치 넣기: 관측치수(n) 넣기 # add text(n) over a boxplot par(mfrow&#x3D;c(1,2)) boxplot(brain$wt~brain$sex, col&#x3D;c(&quot;green&quot;, &quot;orange&quot;)) text(c(1:nlevels(brain$sex)), a$stats[nrow(a$stats),]+30, paste(&quot;n &#x3D; &quot;,table(brain$sex),sep&#x3D;&quot;&quot;)) 막대그림 차의 연비 데이터(autompg) car &lt;- read.csv(&quot;week4_2&#x2F;autompg.csv&quot;) attach(car) barplot(변수빈도, col=c(“col1”,”col2”,…)) # bar plot with cyliner count # par(mfrow&#x3D;c(1,1)) table(car$cyl) freq_cyl &lt;- table(cyl) names(freq_cyl) &lt;- c(&quot;3cyl&quot;, &quot;4cyl&quot;, &quot;5cyl&quot;, &quot;6cyl&quot;, &quot;8cyl&quot;) barplot(freq_cyl, col &#x3D; c(&quot;lightblue&quot;, &quot;mistyrose&quot;, &quot;lightcyan&quot;, &quot;lavender&quot;, &quot;cornsilk&quot;)) 파이차트 pie(변수빈도, labels=c(“ “, …)) 파이차트를 그리기 위해서는 table(변수이름)을 이용하여 빈도를 계산해야 함 # you can alse custom the labels freq_cyl &lt;- table(cyl) names(freq_cyl) &lt;- c(&quot;3cyl&quot;, &quot;4cyl&quot;, &quot;5cyl&quot;, &quot;6cyl&quot;, &quot;8cyl&quot;) pie(freq_cyl) 시계방향으로 파이차트 그리기 # pie chart clockwise pie(freq_cyl, labels &#x3D; c(&quot;3cyl&quot;, &quot;4cyl&quot;, &quot;5cyl&quot;, &quot;6cyl&quot;,&quot;8cyl&quot;), clockwise &#x3D; TRUE) 몇 개의 변수만 뽑아서 그래프 그리기 # 4-3 pie chart of subset # subset with cylinder (4,6,8) - refresh creating subset data lec3_2.R car1&lt;-subset(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 | cyl&#x3D;&#x3D;8) table(car1$cyl) freq_cyl1&lt;-table(car1$cyl) pie(freq_cyl1, labels &#x3D; c(&quot;4cyl&quot;,&quot;6cyl&quot;,&quot;8cyl&quot;), clockwise &#x3D; TRUE) 3. R 그래픽 Ⅲ: 산점도산점도: plot(x,y)par(mforw&#x3D;c(1,1)) x2 &lt;- c(1, 4, 9) y2 &#x3D; 2+x2 plot(x2, y2) par(mfrwo&#x3D;c(2,1)) x &lt;- seq(0, 2*pi, by&#x3D;0.001) y1 &lt;- sin(x) plot(x, y1, main &#x3D; &quot;sin curve (0:2*pi)&quot;) y2 &lt;- cos(x) plot(x,y2,main&#x3D;&quot;cosine curve (0:2*pi)&quot;) wt(차의 무게)과 mpg(연비) 간의 산점도: plot(wt, mpg) hp(마력)과 mpg(연비) 간의 산점도: plot(hp, mpg) par(mfrow&#x3D;c(2,1)) plot(wt, mpg) plot(hp, mpg) 산점도 해석 차 무게가 무거울수록 연비는 낮다. 마력과 연비 간 산점도에서는 두 개의 클러스터가 보임(클러스터 내에서는 마력이 높을수록 연비 낮음) plot(x, y, col=as.integer(그룹변수)) 색으로 표시 par(mfrow&#x3D;c(2,1), mar&#x3D;c(4,4,2,2)) plot(disp, mpg, col&#x3D;as.integer(car$cyl)) plot(wt, mpg, col&#x3D;as.integer(car$cyl)) Conditioning plot: coplot(y~x|z)는 factor(그룹) 그룹에 따른 (x와 y간) 산점도 그룹변수(factor변수)간 평균 차이를 제공car1&lt;-subset(car, cyl&#x3D;&#x3D;4 | cyl&#x3D;&#x3D;6 | cyl&#x3D;&#x3D;8) coplot(car1$mpg ~ car1$disp | as.factor(car1$cyl), data &#x3D; car1, panel &#x3D; panel.smooth, rows &#x3D; 1) 위 그래프로 확인 가능한 그룹별 산점도 cylinder에 따른 차이를 보여줌 4cyl, 6cyl, 8cyl별로 (배기량과 연비) 관계를 구체적으로 해석할 수 있음 pairwise scatterplot: pairs(변수리스트) # cross-tab plot to see how explanatory variables are related each other pairs(car1[,1:6], col&#x3D;as.integer(car1$cyl), main &#x3D; &quot;autompg&quot;) 최적 적합 함수 추정(선형회귀모형, 비선형회귀모형) lm(y변수~x변수): lm(linear model, 선형모형) abline: add line (선 추가 함수)# scatterplot with best fir lines plot(wt, mpg, col&#x3D;as.integer(car$cyl), pch&#x3D;19) # best fit linear line abline(lm(mpg~wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) # lwd: width, 선의 굵기, lty: type, 라인의 타입 최적 적합 함수 추정: 비선형회귀모형, lowess 이용 lowess: locally-weighted polynomial regression (see the references)# scatterplot with best fit lines par(mfrow&#x3D;c(1,1)) plot(wt, mpg, col&#x3D;as.integer(car$cyl), pch&#x3D;19) # best fit linear line abline(lm(mpg~wt), col&#x3D;&quot;red&quot;, lwd&#x3D;2, lty&#x3D;1) # lowess : smoothed line, nonparmetric fit line (locally weighted polynomial regression) lines(lowess(wt, mpg), col&#x3D;&quot;red&quot;, lwd&#x3D;3, lty&#x3D;2) help(lowess) 4. 그래픽과 레이아웃그래프의 기본 함수 그래프 종류: plot(), barplot(), boxplot(), hist(), pie(), persp() 그래프 조정 사항: 점/선 종류, 글자 크기, 여백 조정 점 그리기: points() 선 그리기: lines(), abline(), arrows() 문자 출력: text() 도형: rect(), ploygon() 좌표축: axis() 격자표현: grid() 그래픽 옵션 par(): 그래프 출력 조정- 화면 분할, 마진, 글자 크기, 색상 pty=”s”: x축과 y축을 동일 비율로 설정, square pty=”m”: 최대 크기로 설정, maximal legend = c(“name1”, “name2”) bty=”o”: box type 그래프 상자 모양 설정- o, l, 7, c, u pch=1(default): point character (1=동그라미, 2=세모, … , 19=채운 동그라미) Lty=(solid가 default): line type, 1=직선, 2=점선 cex=1(default): character expansion, 문자나 점의 크기, 숫자 클수록 글자 크기 커짐 mar: 아래, 왼쪽, 위쪽, 오른쪽 여백 선 그리기 abline(h=위치, v=위치, col=colname) par(mfrow&#x3D;c(1,1)) plot(wt, mpg, main &#x3D; &quot;abline on the scatterplot&quot;) # horizontal abline(h &#x3D; 20) abline(h &#x3D; 30) # 수평선을 y축 위치 20과 30에 # vertical abline(v &#x3D; 3000, col&#x3D;&quot;blue&quot;) # 수직선을 x축 위치 3000에, 색은 파란색 abline(절편값, 기울기값, lty=1, lwd=1, col=colname) lty=1(직선), lty=2(점선), lwd=1 (line width, 숫자 클수록 선 굵어짐)# y &#x3D; a + bx abline(a &#x3D; 40, b &#x3D; -0.0076, col&#x3D;&quot;red&quot;) # linear model coefficients, lty (line type), lwd (line with) # linear model (mpg&#x3D;f(wp)) z &lt;- lm(mpg ~ wt, data &#x3D; car) z abline(z, lty &#x3D; 2, lwd &#x3D; 2, col&#x3D;&quot;green&quot;) layout 함수 par(mfrow=c(2,2)) # 2*2 mulitple plot par(mfrow&#x3D;c(2,2)) plot(wt, mpg) plot(disp, mpg) plot(hp, mpg) plot(accler, mpg) margin 조정: mar(아래, 왼쪽, 위쪽, 오른쪽) # 2*2 mulitple plot adjusting margin par(mfrow&#x3D;c(2,2), mar&#x3D;c(4,4,2,2)) plot(wt, mpg) plot(disp, mpg) plot(hp, mpg) plot(accler, mpg) layout 조정 layout 행렬 m# top 1 plot, bottom 2 plot (m &lt;- matrix(c(1, 1, 2, 3), ncol &#x3D; 2, byrow &#x3D; T)) layout(mat &#x3D; m) plot(car$wt, car$mpg, main &#x3D; &quot;scatter plot of autompg&quot;, pch &#x3D; 19, col &#x3D; 4) hist(car$wt) hist(car$mpg) legend 달기 legend(x축 위치, y축 위치, legend=범례라벨, pch=1, col=c(번호 혹은 색으로 지정), lty=1) # scatterplot coloring group variable par(mfrow&#x3D;c(1,1), mar&#x3D;c(4,4,4,4)) plot(wt, mpg, col&#x3D;as.integer(car$cyl)) labels &#x3D; c(&quot;3cyl&quot;, &quot;4cyl&quot;, &quot;5cyl&quot;, &quot;6cyl&quot;,&quot;8cyl&quot;) legend(4000, 45, legend &#x3D; labels, pch &#x3D; 1, col &#x3D;c(3,4,5,6,8), lty &#x3D;1) R 그래픽 히스토그램과 밀도함수(histogram and density) 상자그림(boxplot) 파이차트(pie chart), 막대그림(bar plot) 산점도(scatterplot) ggplot2를 이용한 그래픽 덴드로그램, 애니메이션 지도분석(map) 3D, 히트맵","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"ggplot2","slug":"ggplot2","permalink":"https://ne-choi.github.io/tags/ggplot2/"}],"author":"ne_choi"},{"title":"ch06. 차원 축소","slug":"Study/Python/ML/ch06_차원_축소","date":"2020-12-03T15:00:00.000Z","updated":"2021-01-20T03:58:12.339Z","comments":true,"path":"/2020/12/04/Study/Python/ML/ch06_차원_축소/","link":"","permalink":"https://ne-choi.github.io/2020/12/04/Study/Python/ML/ch06_%EC%B0%A8%EC%9B%90_%EC%B6%95%EC%86%8C/","excerpt":"","text":"해당 자료는 파이썬 머신러닝 완벽가이드 공부를 위한 필사본입니다. Chapter 06. 차원 축소01. 차원 축소(Dimension Reduction) 개요 차원 축소는 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것이다. 피처가 많을 경우 개별 피처간에 상관관계가 높을 가능성이 크다. 선형모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중 공선성 문제로 모델의 예측 성능이 저하된다. 따라서 다차원의 피처를 차원 축소해 피처수를 줄이면 직관적으로 데이터를 해석할 수 있다. 차원 축소는 피처 선택(feature selection)과 피처 추출(feature extraction)로 나눌 수 있다. 피처 선택(특성 선택) 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것이다. 피처 추출(특성 추출) 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것이다. 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 된다. 피처 추출은 기존 피처를 단순 압축이 아닌, 기존 피처가 인지하기 어려웠던 잠재적인 요소(Latent Factor)를 추출하는 것을 의미한다. ex) 고등학생의 모의고사 성적, 내신성적, 수능성적, 봉사활동 등의 여러 피처를 학업 성취도, 커뮤니케이션 능력, 문제 해결력과 같은 함축적인 요약 특성으로 추출 차원 축소는 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는데 의미가 있다. 대표적인 차원 축소 알고리즘에는 PCA, SVD, NMF 가 있다.(이미지나 텍스트 차원 축소에 자주 활용) 02. PCA(Principal Component Analysis, 주성분 분석) PCA는 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분(Principal Component)을 추출해 차원을 축소하는 기법이다. PCA로 차원을 축소할 때, 기존 데이터의 정보 유실을 최소화하기 위해 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하게 되고, 이것이 PCA의 주성분이 된다. PCA는 제일 먼저 가장 큰 데이터 변동성(Variance)을 기반으로 첫 번째 벡터축을 생성하고, 두 번째 축은 이 벡터 축에 직각이 되는 벡터(직교 벡터)를 축으로 한다. 세 번째 축은 다시 두 번째 축과 각각이 되는 벡터를 설정하는 방식으로 축을 생성한다. 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소된다. PCA는 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법이다. 선형대수 관점에서 해석해 보면, 입력 데이터의 공분산 행렬(Covariance Matrix)을 고유값 분해하고, 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것이다. 이 고유벡터가 PCA의 주성분벡터로서 입력 데이터의 분산이 큰 방향을 나타낸다. 고유값(eigenvalue)은 이 고유벡터의 크기를 나타내며, 동시에 입력 데이터의 분산을 나타낸다. 선형 변환은 특정 벡터에 행렬A를 곱해 새로운 벡터로 변환하는 것을 의미한다.(행렬을 공간으로 가정) 보통 분산은 한 개의 특정한 변수의 데이터 변동을 의미하나, 공분산은 두 변수 간의 변동을 의미한다. Cov(X, Y) &gt; 0은 X가 증가할 때 Y도 증가한다는 의미이다. 공분산 행렬은 여러 변수와 관련된 공분산을 포함하는 정방형 행령이다. X Y Z X 3.0 -0.71 -0.24 Y -0.71 4.5 0.28 Z -0.24 0.28 0.91 위 표를 보자. 공분산 행렬에서 대각선 원소는 각 변수(X, Y, Z)의 분산을 의미하며, 대각선 이외의 원소는 가능한 모든 변수 쌍 간의 공분산을 의미하낟. 즉, X, Y, Z의 분산은 각각 3.0, 4.5, 0.91이고, X와 Y의 공분산은 -0.71, X와Z의 공분산은 -0.24, Y와Z의 공분산은 0.28이다. 고유벡터는 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터를 지칭한다.(Ax = ax, A는 행렬 x는 고유벡터 a는 스칼라값) 정방행렬은 최대 차원 수 만큼의 고유벡터를 가질 수 있다.(2*2행렬은 2개, 3*3행렬은 3개의 고유벡터를 가질 수 있음) 이렇게 고유벡터는 행렬이 작용하는 힘의 방향과 관계가 있어서 행렬을 분해하는 데 사용된다. 공분산 행렬은 정방행렬(Diagonal Matrix)이며 대칭행렬(Symmetric Matrix)이다. 정방행렬 : 열과 행이 같은 행렬 대칭행렬 : 정방행렬 중에서 대각 원소를 중심으로 원소 값이 대칭되는 행렬 공분산 행렬은 개별 분산값을 대각 원소로 하는 대칭행렬이다. 대칭행렬은 고유값 분해에 있어서, 항상 고유벡터를 직교행렬(orthogonal matrix)로, 고유값을 정방 행렬로 대각화할 수 있다. 입력 데이터의 공분산 행렬을 C라고 하면 공분산 행렬의 특성으로 인해 다음과 같이 분해할 수 있다. 이 때 P는 n*n의 직교행렬이며, 시그마는 n*n 정방행렬, P^T는 행렬 P의 전치행렬이다. 위 식은 고유벡터 행렬과 고유값 행렬로 다음과 같이 대응된다. 공분산C는 고유벡터 직교 행렬 * 고유값 정방 행렬 * 고유벡터 직교 행렬의 전치 행렬로 분해된다. ei는 i번째 고유벡터를, 람다i는 i번째 고유벡터의 크기를 의미한다. e1는 가장 분산이 큰 방향을 가진 고유벡터이며, e2는 e1에 수직이면서 다음으로 가장 분산이 큰 방향을 가진 고유벡터이다. 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있으모, 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변화하는 방식이 PCA라는 것이다. PCA의 스텝 입력 데이터 세트의 공분산 행렬을 생성한다. 공분산 행렬의 고유벡터와 고유값을 계산한다. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출한다. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력데이터를 변환한다. PCA는 많은 속성으로 구성된 원본 데이터를 그 핵심을 구성하는 데이터로 압축한 것이다. 붓꽃(iris)데이터 세트는 sepal length, sepal width, petal length ,petal width 4개의 속성으로 되어 있는데, 2개의 PCA의 차원으로 압축해 원래 데이터 세트와 압축된 데이터 세트가 어떻게 달라졌는지 확인해 보자. 먼저 사이킷런의 붓꽃 데이터를 load_iris() API를 이용해 로딩한 뒤 이 데이터를 더 편하게 시각화하기 위해 DataFrame으로 변환해보자. from sklearn.datasets import load_iris import pandas as pd import matplotlib.pyplot as plt %matplotlib inline # 사이킷런 내장 데이터 셋 API 호출 iris = load_iris() # 넘파이 데이터 셋을 Pandas DataFrame으로 변환 columns = ['sepal_length','sepal_width','petal_length','petal_width'] irisDF = pd.DataFrame(iris.data , columns=columns) irisDF['target']=iris.target irisDF.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; sepal_length sepal_width petal_length petal_width target 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 각 품종에 따라 원본 붓꽃 데이터 세트가 어떻게 분포돼 있는지 2차원으로 시각화해 보자. 2차원으로 표현하므로 두 개의 속성인 sepal length와 sepal width를 X축, Y축으로 해 품종 데이터 분포를 나탄낸다. #setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현 markers=['^', 's', 'o'] #setosa의 target 값은 0, versicolor는 1, virginica는 2. 각 target 별로 다른 shape으로 scatter plot for i, marker in enumerate(markers): x_axis_data = irisDF[irisDF['target']==i]['sepal_length'] y_axis_data = irisDF[irisDF['target']==i]['sepal_width'] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel('sepal length') plt.ylabel('sepal width') plt.show() Setosa 품종의 경우 sepal width가 3.0보다 크고, sepal length가 6.0 이하인 곳에 일정하게 분포되어 있다. Versiclor와 virginica의 경우 sepal width와 sepal length 조건만으로는 분류가 어려운 복잡한 조건임을 알 수 있다. 이제 2개의 PCA 속성으로 붓꽃 데이터의 품종 분포를 2차원으로 시각화해 보자. 먼저 붓꽃 데이터 세트에 바로 PCA를 적용하기 전에 개별 속성을 함께 스케일링해야 한다. PCA는 여러 속성의 값을 연산해야 하므로 속성의 스케일에 영향을 받는다. 따라서 여러 속성을 PCA로 압축하기 전에 각 속성값을 동일한 스케일로 변환하는 것이 필요하다. 사이킷런의 StandardScaler를 이용해 평균이 0, 분산이 1인 표준 정규 분포로 iris 데이터 세트의 속성값들을 변환한다. from sklearn.preprocessing import StandardScaler # Target 값을 제외한 모든 속성 값을 StandardScaler를 이용하여 표준 정규 분포를 가지는 값들로 변환 iris_scaled = StandardScaler().fit_transform(irisDF.iloc[:, :-1]) 이제 스케일링이 적용된 데이터 세트에 PCA를 적용해 4차원의 붓꽃 대이터를 2차원 PCA 데이터로 변환해 보자. 사이킷런은 PCA 변환을 위해 PCA 클래스를 제공한다. PCA 클래스는 생성 파라미터로 n_components를 입력받는다. n_components는 PCA로 변환할 차원의 수를 의미하므로 여기서는 2로 설정한다. 이후에 fit과 transform을 호출해 PCA로 변환을 수행한다. from sklearn.decomposition import PCA pca = PCA(n_components=2) #fit( )과 transform( ) 을 호출하여 PCA 변환 데이터 반환 pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) print(iris_pca.shape) (150, 2) PCA 객체의 transform() 메서드를 호출해 원본 데이터 세트를 (150,2)의 데이터 세트로 iris_pca 객체 변수로 반환했다. iris_pca는 변환된 PCA 데이터 세트를 150*2 넘파이 행렬로 가지고 있다. 이를 DataFrame으로 변환한 뒤 데이터값을 확인해 보자. # PCA 환된 데이터의 컬럼명을 각각 pca_component_1, pca_component_2로 명명 pca_columns=['pca_component_1','pca_component_2'] irisDF_pca = pd.DataFrame(iris_pca, columns=pca_columns) irisDF_pca['target']=iris.target irisDF_pca.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; pca_component_1 pca_component_2 target 0 -2.264703 0.480027 0 1 -2.080961 -0.674134 0 2 -2.364229 -0.341908 0 이제 2개의 속성으로 PCA 변환된 데이터 세트를 2차원상에서 시각화해보자. pca_component_1 속성을 X축으로, pca_component_2 속성을 Y축으로 해서 붓꽃 품종이 어떻게 분포되는지 확인해보자. markers=['^', 's', 'o'] #pca_component_1 을 x축, pc_component_2를 y축으로 scatter plot 수행. for i, marker in enumerate(markers): x_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_1'] y_axis_data = irisDF_pca[irisDF_pca['target']==i]['pca_component_2'] plt.scatter(x_axis_data, y_axis_data, marker=marker,label=iris.target_names[i]) plt.legend() plt.xlabel('pca_component_1') plt.ylabel('pca_component_2') plt.show() PCA로 변환한 후에도 pca_component_1 축을 기반으로 Setosa 품종은 명확하게 구분이 가능하다. Versicolor와 Virginica는 pca_component_1 축을 기반으로 서로 겹치는 부분이 일부 존재하지만, 비교적 잘 구분됐다. 이는 PCA 첫 번째 새로운 축인 pca_component_1이 원본 데이터의 변동성을 잘 반영했기 때문이다. PCA Component별로 원본 데이터의 변동성을 얼마나 반영하는 알아보자. PCA 변환을 수행한 PCA 객체의 explained_variance_ratio_ 속성은 전체 변동성에서 개별 PCA 컴포넌트 별로 변동성 비율을 제공하고 있습니다. print(pca.explained_variance_ratio_) [0.72962445 0.22850762] 첫 번째 PCA 변환 요소인 pca_component_1이 전체 변동성의 약 72.9%를 차지하며, 두 번째인 pca_component_2가 약 22,8%를 차지한다. 따라서 PCA 2개 요소로만 변환해도 원본 데이터의 변동성을 95% 설명할 수 있다. 이번에는 원본 붓꽃 데이터 세트와 PCA로 변환된 데이터 세트에 각각 분류를 적용한 후 결과를 비교하겠다. Estimator는 RandomForestClassifier를 이용하고 cross_val_score()로 3개의 교차 검증 세트로 정확도 결과를 비교한다. 먼저 원본 붓꽃 데이터에 랜덤 포레스트를 적용한 결과는 다음과 같다. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score import numpy as np rcf = RandomForestClassifier(random_state=156) scores = cross_val_score(rcf, iris.data, iris.target,scoring='accuracy',cv=3) print('원본 데이터 교차 검증 개별 정확도:',scores) print('원본 데이터 평균 정확도:', np.mean(scores)) 원본 데이터 교차 검증 개별 정확도: [0.98 0.94 0.96] 원본 데이터 평균 정확도: 0.96 이번에는 기존 4차원 데이터를 2차원으로 PCA 변환한 데이터 세트에 랜덤 포레스트를 적용해 보겠다. pca_X = irisDF_pca[['pca_component_1', 'pca_component_2']] scores_pca = cross_val_score(rcf, pca_X, iris.target, scoring='accuracy', cv=3 ) print('PCA 변환 데이터 교차 검증 개별 정확도:',scores_pca) print('PCA 변환 데이터 평균 정확도:', np.mean(scores_pca)) PCA 변환 데이터 교차 검증 개별 정확도: [0.88 0.88 0.88] PCA 변환 데이터 평균 정확도: 0.88 원본 데이터 세트 대비 예측 정확도는 PCA 변환 차원 개수에 따라 예측 성능이 떨어질 수 밖에 없다. 위 붓꽃 데이터의 경우는 4개의 속성이 2개의 변환 속성으로 감소하면서 예측 성능의 정확도가 원본 데이터 대비 10% 하락했다. 10%의 정확도 하락은 비교적 큰 성능 수치의 감소지만, 4개의 속성이 2개로, 속성 개수가 50% 감소한 것을 고려한다면 PCA 변환 후에도 원본 데이터의 특성을 상당부분 유지하고 있음을 알 수 있다. 다음으로 좀 더 많은 피처를 가진 데이터 세트를 적은 PCA 컴포넌트 기반으로 변화한 뒤, 예측 영향도가 어떻게 되는지 변환된 PCA 데이터 세트에 기반해서 비교해 보겠다. 사용할 데이터 세트는 UCI Machine Learning Repository에 있는 신용카드 고객 데이터 세트이다. 다운받는 법 https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients 에 접속한다. Data Folder 를 클릭 후 default of credit card clients.xls 를 클릭하여 데이터를 다운받는다. 파일명을 credit_card.xls로 바꿔준다. # Mount Google Drive from google.colab import drive # import drive from google colab ROOT = \"/content/drive\" # default location for the drive print(ROOT) # print content of ROOT (Optional) drive.mount(ROOT) # we mount the google drive at /content/drive /content/drive Mounted at /content/drive # import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH from os.path import join # path to your project on Google Drive # MY_GOOGLE_DRIVE_PATH = 'My Drive/Class_Python/MachineLearning/data' MY_GOOGLE_DRIVE_PATH = 'My Drive' PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH) print(PROJECT_PATH) /content/drive/My Drive %cd \"&#123;PROJECT_PATH&#125;\" /content/drive/My Drive # header로 의미 없는 첫 행 제거, iloc로 기존 id 제거 import pandas as pd df = pd.read_excel('credit_card.xls', sheet_name='Data', header = 1) df = df.iloc[:, 1:] print(df.shape) df.head(3) (30000, 24) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; LIMIT_BAL SEX EDUCATION MARRIAGE AGE PAY_0 PAY_2 PAY_3 PAY_4 PAY_5 PAY_6 BILL_AMT1 BILL_AMT2 BILL_AMT3 BILL_AMT4 BILL_AMT5 BILL_AMT6 PAY_AMT1 PAY_AMT2 PAY_AMT3 PAY_AMT4 PAY_AMT5 PAY_AMT6 default payment next month 0 20000 2 2 1 24 2 2 -1 -1 -2 -2 3913 3102 689 0 0 0 0 689 0 0 0 0 1 1 120000 2 2 2 26 -1 2 0 0 0 2 2682 1725 2682 3272 3455 3261 0 1000 1000 1000 0 2000 1 2 90000 2 2 2 34 0 0 0 0 0 0 29239 14027 13559 14331 14948 15549 1518 1500 1000 1000 1000 5000 0 신용카드 데이터 세트는 30,000개의 레코드와 24개의 속성을 가지고 있다. 이 중에서 ‘default payment next month’ 속성이 Target 값으로 ‘다음달 연체 여부’를 의미하며 ‘연체’일 경우 1, ‘정상납부’가 0이다. 원본 데이터 세트에 PAY_0 다음에 PAY_2 칼럼이 있으므로 PAY_0 칼럼을 PAY_1으로 칼럼명을 변환하고 ‘default payment next month’칼럼도 칼럼명이 너무 길어서 ‘default’로 칼럼명을 변경한다. 이후 Target 속성인 ‘default’ 칼럼을 y_target 변수로 별도로 저장하고 피처 데이터는 default 칼럼을 제외한 별도의 DataFrame으로 만들겠다. df.rename(columns=&#123;'PAY_0':'PAY_1','default payment next month':'default'&#125;, inplace=True) y_target = df['default'] X_features = df.drop('default', axis=1) 해당 데이터 세트는 23개의 속성 데이터 세트가 있으나 각 속성끼리 상관도가 매우 높다. DataFrame의 corr()를 이용해 각 속성 간의 상관도를 구한 뒤 이를 시본의 heatmap으로 시각화하겠다. import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline corr = X_features.corr() plt.figure(figsize=(14,14)) sns.heatmap(corr, annot=True, fmt='.1g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6dd94a6f98&gt; BILL_AMT1 ~ BILL_AMT6 6개의 속성끼리의 상관도가 대부분 0.9 이상으로 매우 높음을 알 수 있다. 이보다 낮지만 PAY_1 ~ PAY_6까지의 속성 역시 상관도가 높다. 이렇게 높은 상관도를 가진 속성들은 소수의 PCA만으로도 자연스럽게 이 속성들의 변동성을 수용할 수 있다. 이 BILL_AMT1 ~ BILL_AMT6까지 6개 속성을 2개의 컴포넌트로 변환한 뒤 개별 컴포넌트의 변동성을 explained_variance_ratio_ 속성으로 알아보자. from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler #BILL_AMT1 ~ BILL_AMT6 까지 6개의 속성명 생성 cols_bill = ['BILL_AMT'+str(i) for i in range(1,7)] print('대상 속성명:',cols_bill) # 2개의 PCA 속성을 가진 PCA 객체 생성하고, explained_variance_ratio_ 계산 위해 fit( ) 호출 scaler = StandardScaler() df_cols_scaled = scaler.fit_transform(X_features[cols_bill]) X_features.loc[:, cols_bill] = df_cols_scaled pca = PCA(n_components=2) pca.fit(df_cols_scaled) print('PCA Component별 변동성:', pca.explained_variance_ratio_) 대상 속성명: [&#39;BILL_AMT1&#39;, &#39;BILL_AMT2&#39;, &#39;BILL_AMT3&#39;, &#39;BILL_AMT4&#39;, &#39;BILL_AMT5&#39;, &#39;BILL_AMT6&#39;] PCA Component별 변동성: [0.90555253 0.0509867 ] 단 2개의 PCA 컴포넌트만으로도 6개의 속성의 변동성을 약 95% 이상 설명할 수 있으며 특히 첫 번째 PCA 축으로 90%의 변동성을 수용할 정도로 이 6개 속성의 상관도가 매우 높다. 이번에는 원본 데이터 세트와 6개의 컴포넌트로 PCA 변환한 데이터 세트의 분류 예측 결과를 상호 비교해 보겠다. 먼저 원본 데이터 세트에 랜덤 포레스트를 이용해 타깃 값이 디폴트 값을 3개의 교차 검증 세트로 분류 예측했다. import numpy as np from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score rcf = RandomForestClassifier(n_estimators=300, random_state=156) scores = cross_val_score(rcf, X_features, y_target, scoring='accuracy', cv=3 ) print('CV=3 인 경우의 개별 Fold세트별 정확도:',scores) print('평균 정확도:&#123;0:.4f&#125;'.format(np.mean(scores))) CV=3 인 경우의 개별 Fold세트별 정확도: [0.8081 0.8197 0.8232] 평균 정확도:0.8170 3개의 교차 검증 세트에서 평균 예측 정확도는 약 81.71%를 나타냈다. 이번에는 6개의 컴포넌트로 PCA 변환한 데이터 세트에 대해서 동일하게 분류 예측을 적용해 보겠다. from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler # 원본 데이터셋에 먼저 StandardScaler적용 scaler = StandardScaler() df_scaled = scaler.fit_transform(X_features) # 6개의 Component를 가진 PCA 변환을 수행하고 cross_val_score( )로 분류 예측 수행. pca = PCA(n_components=6) df_pca = pca.fit_transform(df_scaled) scores_pca = cross_val_score(rcf, df_pca, y_target, scoring='accuracy', cv=3) print('CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도:',scores_pca) print('PCA 변환 데이터 셋 평균 정확도:&#123;0:.4f&#125;'.format(np.mean(scores_pca))) CV=3 인 경우의 PCA 변환된 개별 Fold세트별 정확도: [0.7921 0.7963 0.8024] PCA 변환 데이터 셋 평균 정확도:0.7969 전체 23개 속성의 약 1/4 수준인 6개의 PCA 컴포넌트마능로도 원본 데이터를 기반으로 한 분류 예측 결과보다 약 1~2% 정도의 예측 성능 저하만 발생했다. 1~2%의 예측 성능 저하는 미비한 성능 저하로 보기는 힘들지만, 전체 속성의 1/4 정도만으로도 이정도 수치의 예측 성능을 유지할 수 있다는 것은 PCA의 뛰어난 압축 능력을 잘 보여주고 있다. PCA는 차원 축소를 통해 데이터를 쉽게 인지하는 데 활용할 수 있지만, 이보다 더 활발하게 적용되는 영역은 컴퓨터 비전(Computer Vision)분야이다. 03. LDA(Linear Discriminant Analysis)LDA개요LDA(Linear Discriminant Analysis)는 선형 판별 분석법으로 불리며, PCA와 매우 유사합니다. LDA는 PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법이지만, 중요한 차이는 LDA는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소합니다. PCA는 입력 데이터의 변동성의 가장 큰 축을 찾았지만, LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾습니다. LDA는 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화 하는 방식으로 차원을 축소합니다. 즉, 클래스 간 분산은 최대한 크게 가져가고, 클래스 내부의 분산은 최대한 작게 가져가는 방식입니다. 다음 그림은 좋은 클래스 분리를 위해 클래스 간 분산이 크고 클래스 내부의 분산이 작은 것을 표현한 것입니다. 일반적으로 LDA를 구하는 스텝은 PCA와 유사하난 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤, 이 행렬에 기반해 고유백터를 구하고 입력 데이터를 투영한다는 점입니다. LDA를 구하는 스텝은 다음과 같습니다. 클래스 내부와 클래스 간 분산 행렬을 구합니다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 백터를 기반으로 구합니다. 클래스 내부 분산은 행렬을 SW, 클래스 간 분산 행렬을 SB라고 하면 다음 식으로 두 행렬을 고유백터로 분해할 수 있습니다. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출합니다. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환합니다. 붓꽃 데이터 세트에 LDA 적용하기붓꽃 데이터 세트를 사이킷런의 LDA를 이용해 변환하고, 그 결과를 품종별로 시각화해 보겠습니다.사이킷런은 LDA를 LinearDiscriminantAnalysis 클래스로 제공합니다. 붓꽃 데이터 세트를 로드하고 표준 정규 분포로 스케일링합니다. from sklearn.discriminant_analysis import LinearDiscriminantAnalysis from sklearn.preprocessing import StandardScaler from sklearn.datasets import load_iris iris = load_iris() iris_scaled = StandardScaler().fit_transform(iris.data) 2개의 컴포넌트로 붓꽃 데이터를 LDA 변환하겠습니다. PCA와 다르게 LDA에서 한 가지 유의해야 할점은 LDA는 실제로는 PCA와 다르게 비지도학습이 아닌 지도학습이라는 것입니다. 즉, 클래스의 결정값이 변환 시에 필요합니다. 다음 lda 객체의 fit() 메서드를 호출할 때 결정값이 입력됐음에 유의하세요. lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(iris_scaled, iris.target) iris_lda = lda.transform(iris_scaled) print(iris_lda.shape) (150, 2) 이제 LDA 변환된 입력 데이터 값을 2차원 평면에 품종별로 표현해 보겠습니다. 소스 코드는 앞의 PCA 예제와 큰 차이가 없습니다. import pandas as pd import matplotlib.pyplot as plt %matplotlib inline lda_columns=['lda_component_1', 'lda_component_2'] irisDF_lda = pd.DataFrame(iris_lda, columns=lda_columns) irisDF_lda['target']=iris.target #setosa는 세모, versicolor는 네모, virginica는 동그라미로 표현 markers=['^', 's', 'o'] #setosa의 target 값은 0, versicolor는 1, virginica는 2, 각 target별로 다른 모양으로 산점도로 표시 for i, marker in enumerate(markers): x_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_1'] y_axis_data = irisDF_lda[irisDF_lda['target']==i]['lda_component_2'] plt.scatter(x_axis_data, y_axis_data, marker=marker, label=iris.target_names[i]) plt.legend(loc='upper right') plt.xlabel('lda_component_1') plt.ylabel('lda_component_2') plt.show() 04. SVD(Singular Value Decomposition)SVD 개요SVD 역시 PCA와 유사한 행렬 분해 기법을 이용합니다. PCA의 경우 정방행렬(즉, 행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있지만, SVD는 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있습니다. 일반적으로 SVD는 m X n 크기의 행렬 A를 다음과 같이 분해하는 것을 의미합니다. SVD는 특이값 분해로 불리며, 행렬 U와 V에 속한 백터는 특이벡터이며, 모든 특이벡터는 서로 직교하는 성질을 가집니다. Σ는 대각행렬이며, 행렬의 대각에 위치한 값만 0이 아니고 나머지 위치의 값은 모두 0입니다. Σ이 위치한 0이 아닌 값이 바로 행렬 A의 특이값입니다. SVD는 A의 차원이 m X n 일 때 U의 차원이 m X m, Σ의 차원이 m X n, V의 차원이 n X n으로 분해합니다. 하지만 일반적으로는 다음과 같이 Σ의 비대각인 부분과 대각원소 중에 특이값이 0인 부분도 모두 제거하고 제거된Σ에 대응되는 U와 V 원소도 함께 제거해 차원을 줄인 형태로 SVD를 적용합니다. 이렇게 컴팩트한 형태로 SVD를 적용하면 A의 차원이 m X n일 때, U의 차원을 m X p, Σ의 차원을 p X p, V의 차원을 p X n으로 분해 합니다. Truncated SVD는 Σ의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것입니다. 일반적인 SVD는 보통 넘파이나 사이파이 라이브러리를 이용해 수행합니다. 넘파이의 SVD를 이용해 SVD 연산을 수행하고, SVD로 분해가 어떤 식으로 되는지 간단한 예제를 통해 살펴보겠습니다. 새로운 주피터 노트불을 생성하고 넘파이의 SVD 모듈인 numpy.linalg.svd를 로딩합니다. 그리고 랜덤한 4 X 4 넘파이 행렬을 생성합니다. 랜덤 행렬을 생성하는 이유는 행렬의 개별 로우끼리의 의존성을 없애기 위해서입니다. # 넘파이의 svd 모듈 임포트 import numpy as np from numpy.linalg import svd # 4X4 랜덤 행렬 a 생성 np.random.seed(121) a = np.random.randn(4, 4) print(np.round(a, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] 이렇게 생성된 a 행렬에 SVD를 적용해 U, sigma, Vt를 도출하겠습니다. SVD 분해는 nummpy. linalg. svd에 파라미터로 원본 행렬을 입혁하면 U 행렬, Sigma 행렬, V 전치 행렬을 반환합니다. Sigma 행렬의 경우, S=UΣV 에서 Σ행렬의 경우 행렬의 대각에 위치한 값만 0이 아니고, 그렇지 않은 경우는 모두 0이므로 0이아닌 값의 경우만 1차원 행렬로 표현합니다. U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print('U matrix:\\n', np.round(U, 3)) print('Sigma value:\\n', np.round(Sigma, 3)) print('V transpose matrix:\\n', np.round(Vt, 3)) (4, 4) (4,) (4, 4) U matrix: [[-0.079 -0.318 0.867 0.376] [ 0.383 0.787 0.12 0.469] [ 0.656 0.022 0.357 -0.664] [ 0.645 -0.529 -0.328 0.444]] Sigma value: [3.423 2.023 0.463 0.079] V transpose matrix: [[ 0.041 0.224 0.786 -0.574] [-0.2 0.562 0.37 0.712] [-0.778 0.395 -0.333 -0.357] [-0.593 -0.692 0.366 0.189]] U 행렬이 4 X 4, Vt행렬이 4 X 4로 반환됐고, Sigma의 경우는 1차원 행렬인 (4,)로 반환됐습니다. 분해된 이 U, Sigma, Vt를 이용해 다시 원본 행렬로 정확히 복원되는지 확인해 보겠습니다. 원본 행렬로의 복원은 이 U, Sigma, Vt를 내적하면 됩니다. 한 가지 유의할 것은 Sigma의 경우 0이 아닌 값만 1차원으로 추출했으므로 다시 0을 포함한 대칭 행렬로 변환한 뒤에 내적을 수행해야 한다는 점입니다. # Sigma를 다시 0을 포함한 대칭행렬로 변환 Sigma_mat = np.diag(Sigma) a_ = np.dot(np.dot(U, Sigma_mat), Vt) print(np.round(a_, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.014 0.63 1.71 -1.327] [ 0.402 -0.191 1.404 -1.969]] U, Sigma, Vt를 이용해 a_는 원본 행렬 a와 동일하게 복원됨을 알 수 있습니다. 이번에는 데이터 세트가 로우 간 의존성이 있을 경우 어떻게 Sigma 값이 변하고, 이에 따른 차원 축소가 진행될 수 있는지 알아보겠습니다. 일부러 의존성을 부여하기 위해 a 행렬의 3번째 로우를 ‘첫 번째 로우 + 두 번째 로우’로 업데이트하고, 4번째 로우는 첫 번째 로우와 같다고 업데이트 하겠습니다. a[2] = a[0] + a[1] a[3] = a[0] print(np.round(a, 3)) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] 이제 a 행렬은 이전과 다르게 로우 간 관계가 매우 높아졌습니다. 이 데이터를 SVD로 다시 분해해 보겠습니다. # 다시 SVD를 수행해 Sigma 값 확인 U, Sigma, Vt = svd(a) print(U.shape, Sigma.shape, Vt.shape) print('Sigma Value:\\n', np.round(Sigma, 3)) (4, 4) (4,) (4, 4) Sigma Value: [2.663 0.807 0. 0. ] 이전과 차원은 같지만 Sigma 값 중 2개가 0으로 변했습니다. 즉, 선형 독립인 로우 백터의 개수가 2개라는 의미입니다(즉, 행렬의 랭크가 2입니다). 이렇게 분해된 U, Sigma, Vt를 이용해 다시 원본 행렬로 복원해 보겠습니다. 이번에는 U, Sigma, Vt의 전체 데이터를 이용하지 않고 Sigma의 0에 대응되는 U, Sigma, Vt의 데이터를 제외하고 복원해 보겠습니다. 즉, Sigma의 경우 앞의 2개 요소만 0 이 아니므로 U 행렬 중 선행 두개의 열만 추출하고, Vt의 경우는 선행 두개의 행만 추출해 복원하는 것입니다. # U 행렬의 경우는 Sigma와 내적을 수행하므로 Sigma의 앞 2행에 대응되는 앞 2열만 추출 U_ = U[:, :2] Sigma_ = np.diag(Sigma[:2]) # V 전치 행렬의 경우는 앞 2행만 추출 Vt_ = Vt[:2] print(U_.shape, Sigma_.shape, Vt_.shape) # U, Sigma, Vt의 내적을 수행하며, 다시 원본 행렬 복원 a_ = np.dot(np.dot(U_, Sigma_), Vt_) print(np.round(a_, 3)) (4, 2) (2, 2) (2, 4) [[-0.212 -0.285 -0.574 -0.44 ] [-0.33 1.184 1.615 0.367] [-0.542 0.899 1.041 -0.073] [-0.212 -0.285 -0.574 -0.44 ]] 이번에는 Truncated SVD를 이용해 행렬을 분해해 보겠습니다. Truncated SVD는 Σ행렬에 있는 대각원소, 즉 특이값 중 상위 일부 데이터만 추출해 분해하는 방식입니다. 이렇게 분해하면 인위적으로 더 작은 차원의 U, Σ V로 분해하기 때문에 원본 행렬을 정확하게 다시 원복할 수는 없습니다. 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수 있습니다. 당연한 얘기지만, 원래 차원의 차수에 가깝게 잘라낼수록 원본 행렬에 더 가깝게 복원할 수 있습니다. Truncated SVD를 사이파이 모듈을 이용해 간단히 테스트해 보겠습니다. Truncated SVD는 넘파이가 아닌 사이파이에서만 지원됩니다. 사이파이는 SVD뿐만 아니라 Truncated SVD도 지원합니다. 일반적으로 사이파이의 SVD는 scipy.linalg.svd를 이용하면 되지만, Truncated DVD는 희소 행렬로만 지원돼서 scipy.sparse.linalg.svds를 이용해야 합니다. 임의의 원본 행렬 6 X 6을 Normal SVD로 분해해 분해된 행렬의 차원과 Sigma 행렬 내의 특이값을 확인한 뒤 다시 Truncated SVD로 분해해 분해된 행렬의 차원, SIgma 행렬 내의 특이값, 그리고 Truncated SVD로 분해된 행렬의 내적을 계산하여 다시 복원된 데이터와 원본 데이터를 비교해 보겠습니다. import numpy as np from scipy.sparse.linalg import svds from scipy.linalg import svd # 원본 행렬을 출력하고 SVD를 적용할 경우 U, Sigma, Vt의 차원 확인 np.random.seed(121) matrix = np.random.random((6, 6)) print('원본 행렬:\\n', matrix) U, Sigma, Vt = svd(matrix, full_matrices=False) print('\\n분해 행렬 차원:', U.shape, Sigma.shape, Vt.shape) print('\\nSigma값 행렬:', Sigma) # Truncated SVD로 Sigma 행렬의 특이값을 4개로 하여 Truncated SVD 수행 num_components = 4 U_tr, Sigma_tr, Vt_tr = svds(matrix, k=num_components) print('\\nTruncated SVD 분해 행렬 차원:', U_tr.shape, Sigma_tr.shape, Vt_tr.shape) print('\\nTruncated SVD Sigma값 행렬:', Sigma_tr) matrix_tr = np.dot(np.dot(U_tr, np.diag(Sigma_tr)), Vt_tr) # output of TruncatedSVD print('\\nTruncated SVD로 분해 후 복원 행렬:\\n', matrix_tr) 원본 행렬: [[0.11133083 0.21076757 0.23296249 0.15194456 0.83017814 0.40791941] [0.5557906 0.74552394 0.24849976 0.9686594 0.95268418 0.48984885] [0.01829731 0.85760612 0.40493829 0.62247394 0.29537149 0.92958852] [0.4056155 0.56730065 0.24575605 0.22573721 0.03827786 0.58098021] [0.82925331 0.77326256 0.94693849 0.73632338 0.67328275 0.74517176] [0.51161442 0.46920965 0.6439515 0.82081228 0.14548493 0.01806415]] 분해 행렬 차원: (6, 6) (6,) (6, 6) Sigma값 행렬: [3.2535007 0.88116505 0.83865238 0.55463089 0.35834824 0.0349925 ] Truncated SVD 분해 행렬 차원: (6, 4) (4,) (4, 6) Truncated SVD Sigma값 행렬: [0.55463089 0.83865238 0.88116505 3.2535007 ] Truncated SVD로 분해 후 복원 행렬: [[0.19222941 0.21792946 0.15951023 0.14084013 0.81641405 0.42533093] [0.44874275 0.72204422 0.34594106 0.99148577 0.96866325 0.4754868 ] [0.12656662 0.88860729 0.30625735 0.59517439 0.28036734 0.93961948] [0.23989012 0.51026588 0.39697353 0.27308905 0.05971563 0.57156395] [0.83806144 0.78847467 0.93868685 0.72673231 0.6740867 0.73812389] [0.59726589 0.47953891 0.56613544 0.80746028 0.13135039 0.03479656]] 6 X 6 행렬을 SVD 분해하면 U, Sigma, Vt가 각각 (6,6) (6,) (6,6) 차원이지만, Truncated SVD의 n_components를 4로 설정해 U, Sigma, Vt를 (6,4) (4,) (4,6)로 각각 분해했습니다. Truncated SVD로 분해된 행렬로 다시 복원할 경우 완벽하게 복원되지 않고 근사적으로 복원됨을 알 수 있습니다. 사이킷런 TruncatedSVD 클래스를 이용한 변환사이킷런의 TruncatedSVD 클래스는 사이파이의 svds와 같이 Truncated SVD 연산을 수행해 원본 행렬을 분해한 U, Sigma, Vt 행렬을 반환하지는 않습니다. 사이킷런의 TruncatedSVD 클래스는 PCA 클래스와 유사하게 fit()와 transform()을 호출해 원본 데이터를 몇 개의 주요 컴포넌트(즉, Truncated SVD의 K 컴포넌트 수)로 차원을 축소해 변환합니다. 원본 데이터를 Truncated SVD 방식으로 분해됨 U*Sigma 행렬에 선형 변환해 생성합니다. 새로운 주피터 노트북을 생성하고, 다음 코드를 입력해 붓꽃 데이터 세트를 TruncatedSVD를 이용해 변환해 보겠습니다. from sklearn.decomposition import TruncatedSVD, PCA from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data # 2개의 주요 컴포넌트로 TruncatedSVD 변환 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_ftrs) iris_tsvd = tsvd.transform(iris_ftrs) # 산점도 2차원으로 TruncatedSVD 변환된 데이터 표현, 품종은 색깔로 구분 plt.scatter(x=iris_tsvd[:, 0], y= iris_tsvd[:, 1], c= iris.target) plt.xlabel('TruncatedSVD Component 1') plt.ylabel('TruncatedSVD Component 2') Text(0, 0.5, &#39;TruncatedSVD Component 2&#39;) 왼쪽에 있는 그림이 TruncatedSVD로 변환된 붓꽃 데이터 세트입니다. 오른쪽은 비교를 위해서 PCA로 변환된 붓꽃 데이터 세트를 가져다 놓았습니다. TruncatedSVD 변환 역시 PCA와 유사하게 변환 후에 품종별로 어느 정도 클러스터링이 가능할 정도로 각 변환 속성으로 뛰어난 고유성을 가지고 있음을 알 수 있습니다. 사이킷런의 TruncatedSVD와 PCA 클래스 구현을 조금 더 자세히 들여다보면 두 개 클래스 모두 SVD를 이용해 행렬을 분해합니다. 붓꽃 데이터를 스케일링으로 변환한 뒤에 TruncatedSVD와 PCA 클래스 변환을 해보면 두 개가 거의 동일함을 알 수 있습니다. from sklearn.preprocessing import StandardScaler # 붓꽃 데이터를 StandardScaler로 변환 scaler = StandardScaler() iris_scaled = scaler.fit_transform(iris_ftrs) # 스케일링된 데이터를 기반으로 TruncatedSVD 변환 수행 tsvd = TruncatedSVD(n_components=2) tsvd.fit(iris_scaled) iris_tsvd = tsvd.transform(iris_scaled) # 스케일링된 데이터를 기반으로 PCA 변환 수행 pca = PCA(n_components=2) pca.fit(iris_scaled) iris_pca = pca.transform(iris_scaled) # TruncatedSVD 변환 데이터를 왼쪽에, PCA 변환 데이터를 오른쪽에 표현 fig, (ax1, ax2) = plt.subplots(figsize=(9, 4), ncols=2) ax1.scatter(x=iris_tsvd[:, 0], y= iris_tsvd[:, 1], c= iris.target) ax2.scatter(x=iris_pca[:, 0], y= iris_pca[:, 1], c= iris.target) ax1.set_title('Truncated SVD Transformed') ax2.set_title('PCA Transformed') Text(0.5, 1.0, &#39;PCA Transformed&#39;) 두 개의 변환 행렬 값과 원복 속성별 컴포넌트 비율값을 실제로 서로 비교해 보면 거의 같음을 알 수 있습니다. print((iris_pca - iris_tsvd).mean()) print((pca.components_ - tsvd.components_).mean()) 2.339760329927998e-15 4.85722573273506e-17 모두 0에 가까운 값이므로 2개의 변환이 서로 동일함을 알 수 있습니다. 즉, 데이터 세트가 스케일링으로 데이터 중심이 동일해지면 사이킷런의 SVD와 PCA는 동일한 변환을 수행합니다. 이는 PCA가 SVD 알고리즘으로 구현됐음을 의미합니다. 하지만 PCA는 밀집 행렬에 대한 변환만 가능하며 SVD는 희소 행렬에 대한 변환도 가능합니다. SVD는 PCA와 유사하게 컴퓨터 비전 영역에서 이미지 압축을 통한 패턴 인식과 신호 처리 분야에 사용됩니다. 또한 텍스트의 토픽 모델링 기법인 LSA의 기반 알고리즘입니다. 05. NMF(Non-Negative Matrix Factorization)NMF 개요NMF는 Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사(Low-Rank Approximation) 방식의 변형입니다. NMF는 원본 행렬 내의 모든 원소 값이 모두 양수(0 이상)라는 게 보장되면 다음과 같이 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭합니다. 4 X 6 원본 행렬 V는 4 X 2 행렬 W와 2 X 6 행렬 H로 근사해 분해될 수 있습니다. 행렬 분해는 일반적으로 SVD와 같은 행렬 분해 기법을 통칭하는 것입니다. 이처럼 행렬 분해를 하게 되면 W 행렬과 H 행렬은 일반적으로 길고 가는 행렬 W(즉, 원본 행렬의 행 크기보다 작고 열 크기와 같은 행렬)로 분해됩니다. 이렇게 분해된 행렬은 잠재 요소를 특성으로 가지게 됩니다. 분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응하며, 분해 행렬 H는 이 잠재 요소가 원본 열(즉, 원본 속성)로 어떻게 구성됐는지를 나타내는 행렬입니다. NMF는 SVD와 유사하게 차원 축소를 통한 잠재 요소 도출로 이미지 변환 및 압축, 텍스트의 토픽 도출 등의 영역에서 사용되고 있습니다. 사이킷런에서 NMF는 NMF 클래스를 이용해 지원됩니다. 붓꽃 데이터를 NMF를 이용해 2개의 컴포넌트로 변환하고 이를 시각화해 보겠습니다. from sklearn.decomposition import NMF from sklearn.datasets import load_iris import matplotlib.pyplot as plt %matplotlib inline iris = load_iris() iris_ftrs = iris.data nmf = NMF(n_components=2) nmf.fit(iris_ftrs) iris_nmf = nmf.transform(iris_ftrs) plt.scatter(x=iris_nmf[:, 0], y= iris_nmf[:, 1], c= iris.target) plt.xlabel('NMF Component 1') plt.ylabel('NMF Component 2') Text(0, 0.5, &#39;NMF Component 2&#39;) NMF도 SVD와 유사하게 이미지 압축을 통한 패턴 인식, 텍스트의 토픽 모델링 기법, 문서 유사도 및 클러스터링에 잘 사용됩니다. 또한 영화 추천과 같은 추천 영역에 활발하게 적용 됩니다. 사용자의 상품(예: 영화) 평가 데이터 세트인 사용자 - 평가 순위 데이터 세트를 행렬 분해 기법을 통해 분해하면서 사용자가 평가하지 않은 상품에 대한 잠재적인 요소를 추출해 이를 통해 평가 순위를 예측하고, 높은 순위로 예측되는 상품을 추천해주는 방식입니다(이를 잠재 요소 기반의 추천 방식이라고 합니다). 06. 정리지금까지 대표적인 차원 축소 알고리즘인 PCA, LDA, SVD, NMF에 대해서 알아봤습니다. 많은 피처로 이뤄진 데이터 세트를 PCA같은 차원 축소를 통해 더욱 직관적으로 이해할 수 있습니다. 무엇보다도 차원 축소는 단순히 피처의 개수를 줄이는 개념보다는 이를 통해 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 큰 의미가 있습니다. 이 때문에 많은 차원을 가지는 이미지나 텍스트에서 PCA, SVD 등의 차원 축소 알고리즘이 활발하게 사용됩니다. PCA는 입력 데이터의 변동성이 가장 큰 축을 구하고, 다시 이 축에 직각인 축을 반복적으로 축소하려는 차원 개수만큼 구한 뒤 입력 데이터를 이 축들에 투영해 차원을 축소하는 방식입니다. 이를 위해 입력 데이터의 공분산 행렬을 기반으로 고유 백터를 생성하고 이렇게 구한 고유 백터에 입력 데이터를 선형 변환하는 방식입니다. LDA는 PCA와 매우 유사한 방식이며, PCA가 입력 데이터 변동성의 가장 큰 축을 찾는 데 반해 LDA는 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾는 방식으로 차원을 축소합니다. SVD와 NMF는 매우 많은 피처 데이터를 가진 고차원 행렬을 두 개의 저차원 행렬로 분리하는 행렬 분해 기법입니다. 특히 이러한 행렬 분해를 수행하면서 원본 행렬에서 잠재된 요소를 추출하기 때문에 토픽 모델링이나 추천 시스템에서 활발하게 사용됩니다.","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"}],"tags":[{"name":"주성분분석","slug":"주성분분석","permalink":"https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"},{"name":"차원축소","slug":"차원축소","permalink":"https://ne-choi.github.io/tags/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C/"},{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"}],"author":"ne_choi"},{"title":"ch05. 회귀","slug":"Study/Python/ML/ch05_회귀","date":"2020-12-02T15:00:00.000Z","updated":"2021-01-20T03:58:12.188Z","comments":true,"path":"/2020/12/03/Study/Python/ML/ch05_회귀/","link":"","permalink":"https://ne-choi.github.io/2020/12/03/Study/Python/ML/ch05_%ED%9A%8C%EA%B7%80/","excerpt":"","text":"해당 자료는 파이썬 머신러닝 완벽가이드 공부를 위한 필사본입니다. Chapter 05. 회귀01. 회귀 소개 회귀는 현대 통계학을 이루는 큰 축 회귀 분석은 유전적 특성을 연구하던 영국의 통계학자 갈톤이 수행한 연구에서 유래했다는 것이 일반론 &lt; 회귀에 대한 예시&gt; “부모의 키가 크더라도 자식의 키가 대를 이어 무한정 커지지 않으며, 부모의 키가 작더라도 대를 이어 자식의 키가 무한정 작아지지 않는다” 즉, 회귀 분석은 이처럼 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법이다. [출처 : 인프런] ㅡ&gt; X는 피처값(속성) ㅡ&gt; Y는 결정값 회귀는 회귀 계수의 선형/비선형 여부,독립변수의 개수, 종속변수의 개수에 따라 여러 가지 유형으로 나눌 수 있다. 회귀에서 가장 중요한 것은 바로 회귀 계수이다 이 회귀 계수가 선형이냐 아니냐에 따라 선형 회귀와 비선형 회귀로 나눌 수 있다. 그리고 독립변수의 개수가 한 개인지 여러 개인지에 따라 단일 회귀, 다중 회귀로 나뉘게 된다. 독립변수 개수 회귀계수의 결합 1개:단일 회귀 선형:선형 회귀 여러 개:다중 회귀 비선형:비선형 회귀 &lt;회귀 유형 구분&gt; 지도학습은 두 가지 유형으로 나뉘는데, 바로 분류와 회귀이다. 이 두 가지 기법의 가장 큰 차이는 분류는 예측값이 카테고리와 같은 이산형 클래스 값이고 회귀는 연속형 숫자 값이라는 것입니다. 여러 가지 회귀 중에서 선형 회귀가 가장 많이 사용된다. 선형 회귀는 실제값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식이다. 선형 회귀 모델은 규제 방법에 따라 다시 별도의유형으로 나뉠 수 있다. 규제는 일반적인 선형 회귀의 과적합 문제를 해결하기 위해서 회귀 계수에 패널티값을 적용하는 것을 말한다. - 대표적인 선형 회귀모델 일반 선형 회귀: 예측값과 실제값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며 규제(Regularization) 릿지(Ridge): 릿지 회귀는 선형 회귀에 L2 규제를 추가한 회귀 모델, 릿지 회귀는 L2 규제를 적용하는데 L2 규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해서 회귀 계수값을 더 작게 만드는 규제 모델 라쏘(Lasso): 라쏘 회귀는 선형 회귀에 L1 규제를 적용한 방식, L2 규제가 회귀 계수 값의 크기를 줄이는데 반해 L1규제는 예측 영향력이 작은 피처의 회귀 계수를 0으로 만들어 회귀 예측시 피처가 선택되지 않게 함 → 이러한 특성 때문에 L1 규제는 피처 선택 가능으로도 불림 엘라스틱넷(ElasticNet): L2,L1 규제를 함께 결합한 모델, 주로 피처가 많은 데이터 세트에서 적용되며 L1규제로 피처의 개수를 줄임과 동시에 L2 규제로 계수 값의 크기를 조정 로지스틱 회귀(Logistic Regression): 로지스틱 회귀는 회귀라는 이름이 붙어 있지만 사실은 분류에 사용되는 선형 모델, 로지스틱 회귀는 매우 강력한 분류 알고리즘이며 일반적으로 이진 분류뿐만 아니라 희소 영역의 분류, 예를 들어 텍스트 분류와 같은 영역에서 뛰어난 예측성능을 보임 02. 단순선형 회귀를 통한 회귀 이해단순선형회귀는 독립변수도 하나 종속변수도 하나인 선형 회귀이다. - 예시 주택가격이 주택의 크기로만 결정된다고 할때 일반적으로 주택의 크기가 크면 가격이 높아지는 경향이 있기 때문에 주택가격은 크기에 대해 선형(직선형태)의 관계로 표현할 수 있다. [출처: 인프라] - 오류합 계산 방법 절대값을 취하여 더하는 방식 오류값의 제곱을 구해서 더하는 방식(RSS) 일반적으로 미분 등의 계산을 편리하게 하기 위해서 RSS방식으로 오류합을 구한다즉, Error^2 = RSS RSS는 이제 변수가 w0,w1인 식으로 표현 할 수 있으며 RSS를 최소로 하는 w0,w1 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항이다. RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 w 변수(회귀계수)가 중심 변수임을 인지하는 것이 매우 중요(학습 데이터로 입력되는 독립변수와 종속변수는 RSS에서 모두 상수로 간주한다.) [출처 : 인프라] 03. 비용 최소화하기 - 경사 하강법(Gradient Descent)W 파라미터의 개수가 적다면 고차원 방정식으로 비용 함수가 최소가 되는 W 변숫값을 도출할 수 있겠지만 W 파라미터가 많으면 고차원 방정식을 동원하더라도 해결하기가 어렵다. 경사 하강법은 이러한 고차원 방정식에 대한 문제를 해결해 주며서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식이다. [출처 : 인프라] 경사 하강법은 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제값의 차이가 작아지는 방향성을 가지고 W 파라미터를 지속해서 보정해 나간다. 최초 오류 값이 100이었다면 두 번째 오류 값은 100보다 작은 90, 세 번째는 80과 같은 방식으로 지속해서 오류를 감소시키는 방향으로 W 값을 계속 업데이트해 나간다. 그리고 오류 값이 더 이상 작아지지 않으면 그 오류 값을 최소 비용으로 판단하고 그때의 W 값을 최적 파라미터로 반환한다. 경사 하강법의 핵심: “어떻게 하면 오류가 작아지는 방향으로 W 값을 보정할 수 있을까?” [출처 : 인프런] [출처 : 인프런] - 경사 하강법 수행 프로세스 - 실제값을 Y=4X+6 시뮬레이션하는 데이터 값 생성 import numpy as np import matplotlib.pyplot as plt %matplotlib inline np.random.seed(0) # y = 4X + 6 식을 근사(w1=4, w0=6). random 값은 Noise를 위해 만듬 X = 2 * np.random.rand(100,1) # X는 100개의 랜덤값을 만든다. # np.random.randn는 노이즈값 이걸 사용하지 않으면 계속 1차 함수로 만들어냄 결국 퍼져보이기 위하여 사용 y = 6 +4 * X+ np.random.randn(100,1) # X, y 데이터 셋 산점도로 시각화 plt.scatter(X, y) &lt;matplotlib.collections.PathCollection at 0x1d01316fdc0&gt; X.shape, y.shape # 100개의 데이터를 다 가지고 있다는 것을 알 수 있다. ((100, 1), (100, 1)) - w0과 w1의 값을 최소화 할 수 있도록 업데이트 수행하는 함수 생성. 예측 배열 y_pred는 np.dot(X, w1.T) + w0 임 100개의 데이터 X(1,2,…,100)이 있다면 예측값은 w0 + X(1)w1 + X(2)w1 +..+ X(100)*w1이며, 이는 입력 배열 X와 w1 배열의 내적임. 새로운 w1과 w0를 update함 # w1 과 w0 를 업데이트 할 w1_update, w0_update를 반환. def get_weight_updates(w1, w0, X, y, learning_rate=0.01): N = len(y) # 먼저 w1_update, w0_update를 각각 w1, w0의 shape와 동일한 크기를 가진 0 값으로 초기화 w1_update = np.zeros_like(w1) w0_update = np.zeros_like(w0) # 예측 배열 계산하고 예측과 실제 값의 차이 계산 y_pred = np.dot(X, w1.T) + w0 diff = y-y_pred # w0_update를 dot 행렬 연산으로 구하기 위해 모두 1값을 가진 행렬 생성 w0_factors = np.ones((N,1)) # w1과 w0을 업데이트할 w1_update와 w0_update 계산 w1_update = -(2/N)*learning_rate*(np.dot(X.T, diff)) w0_update = -(2/N)*learning_rate*(np.dot(w0_factors.T, diff)) return w1_update, w0_update w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) y_pred = np.dot(X, w1.T) + w0 diff = y-y_pred print(diff.shape) w0_factors = np.ones((100,1)) w1_update = -(2/100)*0.01*(np.dot(X.T, diff)) w0_update = -(2/100)*0.01*(np.dot(w0_factors.T, diff)) print(w1_update.shape, w0_update.shape) w1, w0 (100, 1) (1, 1) (1, 1) (array([[0.]]), array([[0.]])) 반복적으로 경사 하강법을 이용하여 get_weigth_updates()를 호출하여 w1과 w0를 업데이트 하는 함수 생성 # 입력 인자 iters로 주어진 횟수만큼 반복적으로 w1과 w0를 업데이트 적용함. def gradient_descent_steps(X, y, iters=10000): # w0와 w1을 모두 0으로 초기화. w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) # 인자로 주어진 iters 만큼 반복적으로 get_weight_updates() 호출하여 w1, w0 업데이트 수행. for ind in range(iters): w1_update, w0_update = get_weight_updates(w1, w0, X, y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 - 예측 오차 비용을 계산을 수행하는 함수 생성 및 경사 하강법 수행 def get_cost(y, y_pred): N = len(y) cost = np.sum(np.square(y - y_pred))/N # 계측 오류값 구하기 return cost w1, w0 = gradient_descent_steps(X, y, iters=1000) print(\"w1:&#123;0:.3f&#125; w0:&#123;1:.3f&#125;\".format(w1[0,0], w0[0,0])) y_pred = w1[0,0] * X + w0 print('Gradient Descent Total Cost:&#123;0:.4f&#125;'.format(get_cost(y, y_pred))) w1:4.022 w0:6.162 Gradient Descent Total Cost:0.9935 plt.scatter(X, y) plt.plot(X,y_pred) [&lt;matplotlib.lines.Line2D at 0x1d0132424f0&gt;] - 미니 배치 확률적 경사 하강법을 이용한 최적 비용함수 도출 def stochastic_gradient_descent_steps(X, y, batch_size=10, iters=1000): w0 = np.zeros((1,1)) w1 = np.zeros((1,1)) prev_cost = 100000 iter_index =0 for ind in range(iters): np.random.seed(ind) # 전체 X, y 데이터에서 랜덤하게 batch_size만큼 데이터 추출하여 sample_X, sample_y로 저장 stochastic_random_index = np.random.permutation(X.shape[0]) # 임의로 추출 sample_X = X[stochastic_random_index[0:batch_size]] # 랜덤으로 인덱스를 추출 sample_y = y[stochastic_random_index[0:batch_size]] # 랜덤하게 batch_size만큼 추출된 데이터 기반으로 w1_update, w0_update 계산 후 업데이트 w1_update, w0_update = get_weight_updates(w1, w0, sample_X, sample_y, learning_rate=0.01) w1 = w1 - w1_update w0 = w0 - w0_update return w1, w0 np.random.permutation(X.shape[0]) # 랜덤 샘플링을 임의로 가져옴 array([66, 71, 54, 88, 82, 12, 36, 46, 14, 67, 10, 3, 62, 29, 97, 69, 70, 93, 31, 73, 60, 96, 28, 27, 21, 19, 33, 78, 32, 94, 1, 41, 40, 76, 37, 87, 24, 23, 50, 2, 47, 20, 77, 17, 56, 64, 68, 25, 15, 22, 16, 98, 63, 92, 86, 38, 6, 57, 95, 44, 9, 42, 81, 99, 35, 84, 59, 48, 75, 65, 85, 90, 55, 43, 58, 89, 30, 80, 34, 18, 51, 49, 52, 74, 26, 45, 39, 4, 11, 53, 91, 79, 8, 0, 5, 13, 61, 72, 7, 83]) w1, w0 = stochastic_gradient_descent_steps(X, y, iters=1000) print(\"w1:\",round(w1[0,0],3),\"w0:\",round(w0[0,0],3)) y_pred = w1[0,0] * X + w0 print('Stochastic Gradient Descent Total Cost:&#123;0:.4f&#125;'.format(get_cost(y, y_pred))) w1: 4.028 w0: 6.156 Stochastic Gradient Descent Total Cost:0.9937 04. 사이킷런 LinearRegression 클래스class sklearn.linear_model.LinearRegression(fit_intercept&#x3D;True,normalize&#x3D;False, copy_X&#x3D;True, n_jobs&#x3D;1) # fit &#x3D; 학습하다, intercept&#x3D;True(절편을 사용한다), normalize &#x3D; 데이트를 정렬할 것인가 LnearRegression 클래스는 예측값과 실제 값의 RSS를 최소화해 OLS 추정 방식으로 구현한 클래스이다. LinearRegression 클래스는 fit()메서드로 X,Y 배열을 입력 받으면 회귀 계수인 W를 coef_ 속성에 저장한다. [출처 :인프런] (1) 회귀평가 지표 - 사이킷런 회귀 평가 API 사이킷런은 아쉽게도 RMSE를 제공하지 않는다. RMSE를 구하기 위해서는 MSE에 제곱근을 씌워서 계산하는 함수를 직접 만들어야 한다. 다음은 각 평가 방법에 대한 사이킷런의 API 및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoring 파라미터의 적용 값이다 (2) LinearRegression을 이용한 보스턴 주택 가격 예측import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns from scipy import stats from sklearn.datasets import load_boston %matplotlib inline # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF['PRICE'] = boston.target print('Boston 데이타셋 크기 :',bostonDF.shape) bostonDF.head() Boston 데이타셋 크기 : (506, 14) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT PRICE 0 0.00632 18.0 2.31 0.0 0.538 6.575 65.2 4.0900 1.0 296.0 15.3 396.90 4.98 24.0 1 0.02731 0.0 7.07 0.0 0.469 6.421 78.9 4.9671 2.0 242.0 17.8 396.90 9.14 21.6 2 0.02729 0.0 7.07 0.0 0.469 7.185 61.1 4.9671 2.0 242.0 17.8 392.83 4.03 34.7 3 0.03237 0.0 2.18 0.0 0.458 6.998 45.8 6.0622 3.0 222.0 18.7 394.63 2.94 33.4 4 0.06905 0.0 2.18 0.0 0.458 7.147 54.2 6.0622 3.0 222.0 18.7 396.90 5.33 36.2 CRIM: 지역별 범죄 발생률 ZN: 25,000평방피트를 초과하는 거주 지역의 비율 NDUS: 비상업 지역 넓이 비율 CHAS: 찰스강에 대한 더미 변수(강의 경계에 위치한 경우는 1, 아니면 0) NOX: 일산화질소 농도 (자동차에서 나오는 질소산화물로 탄화수소 만나면 오존층 파괴, 기관지에 안 좋음) RM: 거주할 수 있는 방 개수 AGE: 1940년 이전에 건축된 소유 주택의 비율 DIS: 5개 주요 고용센터까지의 가중 거리 RAD: 고속도로 접근 용이도 TAX: 10,000달러당 재산세율 PTRATIO: 지역의 교사와 학생 수 비율 B: 지역의 흑인 거주 비율 LSTAT: 하위 계층의 비율 MEDV: 본인 소유의 주택 가격(중앙값) - 각 컬럼별로 주택가격에 미치는 영향도 조사 # 2개의 행과 4개의 열을 가진 subplots를 이용. axs는 4x2개의 ax를 가짐. fig, axs = plt.subplots(figsize=(16,8) , ncols=4 , nrows=2) lm_features = ['RM','ZN','INDUS','NOX','AGE','PTRATIO','LSTAT','RAD'] for i , feature in enumerate(lm_features): row = int(i/4) col = i%4 # 시본의 regplot을 이용해 산점도와 선형 회귀 직선을 함께 표현 sns.regplot(x=feature , y='PRICE',data=bostonDF , ax=axs[row][col]) 결과 해석 다른 칼럼보다 RM과 LSTAT의 PRICE 영향도가 가장 두드러지게 나타남 RM(방 개수)은 양방향의 선형성이 가장 크다→ 방의 크기가 클수록 가격이 증가함 LSTAT는 음방향의 선형성이 가장 큼→ LSTAT이 적을수록 PRICE가 증가함 - LinearRegression 클래스로 보스턴 주택 가격의 회귀 모델 만들기 from sklearn.model_selection import train_test_split from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error , r2_score y_target = bostonDF['PRICE'] X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False) X_train , X_test , y_train , y_test = train_test_split(X_data , y_target ,test_size=0.3, random_state=156) # 선형회귀 OLS로 학습/예측/평가 수행. lr = LinearRegression() lr.fit(X_train ,y_train ) y_preds = lr.predict(X_test) mse = mean_squared_error(y_test, y_preds) rmse = np.sqrt(mse) print('MSE : &#123;0:.3f&#125; , RMSE : &#123;1:.3F&#125;'.format(mse , rmse)) print('Variance score : &#123;0:.3f&#125;'.format(r2_score(y_test, y_preds))) MSE : 17.297 , RMSE : 4.159 Variance score : 0.757 # 절편과 회귀 계수 값 보기 print('절편 값:',lr.intercept_) print('회귀 계수값:', np.round(lr.coef_, 1)) 절편 값: 40.995595172164585 회귀 계수값: [ -0.1 0.1 0. 3. -19.8 3.4 0. -1.7 0.4 -0. -0.9 0. -0.6] # coef_ 속성은 회귀 계수 값만 가지고 있어, 이를 피처별 회귀 계수 값으로 재 매핑 # 회귀 계수를 큰 값 순으로 정렬하기 위해 Series로 생성. index 컬럼명에 유의 coeff = pd.Series(data=np.round(lr.coef_, 1), index=X_data.columns ) coeff.sort_values(ascending=False) RM 3.4 CHAS 3.0 RAD 0.4 ZN 0.1 B 0.0 TAX -0.0 AGE 0.0 INDUS 0.0 CRIM -0.1 LSTAT -0.6 PTRATIO -0.9 DIS -1.7 NOX -19.8 dtype: float64 결과 해석 RM이 양의 값으로 회귀 계수가 가장 큼 NOX 피처의 회귀 계수 - 값이 너무 커보임→ 최적화를 수행하면서 피처 codfficients의 변화 살필 예정 - 5개의 폴드 세트에서 cross_val_score()로 교차 검증하기: MSE, RMSE 측정사이킷런은 cross_val_score()를 이용하는데, RMSE를 제공하지 않으므로 MSE 수치 결과를 RMSE로 변환해야 한다. cross_val_score()의 인자로 scoring=’neg_mean_squared_error’를 지칭하면 반환되는 수치 값은 음수이다. 사이킷런은 높은 지표 값일수록 좋은 모델로 평가히는데 반해, 회귀는 MSE 값이 낮을수록 좋은 회귀 모델로 평가한다. from sklearn.model_selection import cross_val_score y_target = bostonDF['PRICE'] X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False) lr = LinearRegression() # cross_val_score( )로 5 Fold 셋으로 MSE 를 구한 뒤 이를 기반으로 다시 RMSE 구함. neg_mse_scores = cross_val_score(lr, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) # cross_val_score(scoring=\"neg_mean_squared_error\")로 반환된 값은 모두 음수 print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2)) print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores, 2)) print(' 5 folds 의 평균 RMSE : &#123;0:.3f&#125; '.format(avg_rmse)) 5 folds 의 개별 Negative MSE scores: [-12.46 -26.05 -33.07 -80.76 -33.31] 5 folds 의 개별 RMSE scores : [3.53 5.1 5.75 8.99 5.77] 5 folds 의 평균 RMSE : 5.829 결과 해석 5개 폴드 세트 교차 검증 수행 결과, 평균 RMSE는 약 5.836이 나옴 corss_val_score(scoring=”neg_mean_squared_error”)로 반환된 값은 모두 음수로 확인됨 05. 다항 회귀, 과(대)적합/과소적합(1) 다항 회귀 이해 현재까지 설명한 회귀는 y = $w_{0} + w_{1}*x_{1} + w_{2}*x_{2} + , … , + w_{n}*x_{n}$과 같이 독립변수(feature)와 종속변수(target) 관계가 일차 방정식 형태로 표현된 회귀 세상의 모든 관계를 직선으로만 표현할 수 없기 때문에 다항 회귀 개념이 필요 다항(Polynomial) 회귀: 회귀가 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것 y = $w_{0} + w_{1}*x_{1} + w_{2}*x_{2} + w_{3}*x_{1}*x_{2} + w_{4}*x_{1}^{2} + w_{5}*x_{2}^{2}$ 다항 회귀는 선형 회귀(비선형 회귀가 아님) cf) 회귀에서 선형/비선형을 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것으로, 독립변수의 선형/비선형 여부와는 무관함 사이킷런은 다항 회귀를 위한 클래스를 명시적으로 제공하지 않아, 비선형 함수를 선형 모델에 적용시키는 방법으로 구현 PolynomialFeatures 클래스로 피처를 다항식 피처로 변환함 from sklearn.preprocessing import PolynomialFeatures import numpy as np # 다항식으로 변환한 단항식 생성, [[0,1],[2,3]]의 2X2 행렬 생성 X = np.arange(4).reshape(2,2) print('일차 단항식 계수 feature:\\n',X ) # degree = 2 인 2차 다항식으로 변환하기 위해 PolynomialFeatures를 이용하여 변환 poly = PolynomialFeatures(degree=2) poly.fit(X) poly_ftr = poly.transform(X) print('변환된 2차 다항식 계수 feature:\\n', poly_ftr) 일차 단항식 계수 feature: [[0 1] [2 3]] 변환된 2차 다항식 계수 feature: [[1. 0. 1. 0. 0. 1.] [1. 2. 3. 4. 6. 9.]] - 3차 다항 회귀 함수를 임의로 설정하고 회귀 계수 예측하기 def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 print(X[:, 0]) print(X[:, 1]) return y X = np.arange(0,4).reshape(2,2) print('일차 단항식 계수 feature: \\n' ,X) y = polynomial_func(X) print('삼차 다항식 결정값: \\n', y) 일차 단항식 계수 feature: [[0 1] [2 3]] [0 2] [1 3] 삼차 다항식 결정값: [ 5 125] - 일차 단항식 계수를 삼차 다항식 계수로 변환하고, 선형 회귀에 적용 # 3 차 다항식 변환 poly_ftr = PolynomialFeatures(degree=3).fit_transform(X) print('3차 다항식 계수 feature: \\n',poly_ftr) # Linear Regression에 3차 다항식 계수 feature와 3차 다항식 결정값으로 학습 후 회귀 계수 확인 model = LinearRegression() model.fit(poly_ftr,y) print('Polynomial 회귀 계수\\n' , np.round(model.coef_, 2)) print('Polynomial 회귀 Shape :', model.coef_.shape) 3차 다항식 계수 feature: [[ 1. 0. 1. 0. 0. 1. 0. 0. 0. 1.] [ 1. 2. 3. 4. 6. 9. 8. 12. 18. 27.]] Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] Polynomial 회귀 Shape : (10,) 결과 해석 일차 단항식 계수 피처는 2개였지만, 3차 다항식 Polynomial 변환 이후에는 다항식 계수 피처가 10개로 늘어남 늘어난 피처 데이터 세트에 LinearRegression을 통해 3차 다항 회귀 형태의 다항 회귀를 적용하면 회귀 계수가 10개로 늘어남 10개의 회귀 계수가 도출됐으며 원래 다항식 계수 값과는 차이가 있지만, 다항 회귀로 근사함을 알 수 있음 - 사이킷런의 Pipeline 객체를 이용해 한 번에 다항 회귀 구현하기 from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.pipeline import Pipeline import numpy as np def polynomial_func(X): y = 1 + 2*X[:,0] + 3*X[:,0]**2 + 4*X[:,1]**3 return y # Pipeline 객체로 Streamline 하게 Polynomial Feature변환과 Linear Regression을 연결 model = Pipeline([('poly', PolynomialFeatures(degree=3)), ('linear', LinearRegression())]) X = np.arange(4).reshape(2,2) y = polynomial_func(X) model = model.fit(X, y) print('Polynomial 회귀 계수\\n', np.round(model.named_steps['linear'].coef_, 2)) Polynomial 회귀 계수 [0. 0.18 0.18 0.36 0.54 0.72 0.72 1.08 1.62 2.34] (2) 다항 회귀를 이용한 과소적합 및 과적합 이해 다항 회귀는 피처의 직선적 관계가 아닌, 복잡한 다항 관계를 모델링할 수 있음 다항식 차수가 높아질수록 매우 복잡한 피처 관계까지 모델링이 가능함 단, 다항 회귀의 차수(degree)를 높일수록 학습 데이터에 너무 맞춘 학습이 이루어져서 테스트 데이터 환경에서 예측 정확도가 떨어짐→ 차수가 높아질수록 과적합 문제가 크게 발생 - 다항 회귀의 과소적합과 과적합 문제를 잘 보여주는 예시- 원본 소스코드 설명 피처 X와 target y가 잡음(Noise)이 포함된 다항식의 코사인 그래프 관계를 가지게 만듦 이에 기반해 다항 회귀의 차수를 변화시키며 그에 따른 회귀 예측 곡선과 예측 정확도를 비교하는 예제 학습 데이터: 30개의 임의 데이터 X, X의 코사인 값에서 약간의 잡음 변동 값을 더한 target import numpy as np import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.model_selection import cross_val_score %matplotlib inline # random 값으로 구성된 X값에 대해 Cosine 변환값을 반환. def true_fun(X): # cosine 그래프 형식 return np.cos(1.5 * np.pi * X) # X는 0 부터 1까지 30개의 random 값을 순서대로 sampling 한 데이타 입니다. np.random.seed(0) n_samples = 30 X = np.sort(np.random.rand(n_samples)) # y 값은 cosine 기반의 true_fun() 에서 약간의 Noise 변동값을 더한 값입니다. y = true_fun(X) + np.random.randn(n_samples) * 0.1 # cosine에 noise 포함 - 예측 결과를 비교할 다항식 차수를 각각 1, 4, 15로 변경하며 예측 결과 비교하기 다항식 차수별로 학습 수행 후, cross_val_score()로 MSE 값을 구해 차수별 예측 성능 평가 0부터 1까지 균일하게 구성된 100개의 테스트용 데이터 세트로 차수별 회귀 예측 곡선 그리기 plt.figure(figsize=(14, 5)) degrees = [1, 4, 15] # 다항 회귀의 차수(degree)를 1, 4, 15로 각각 변화시키면서 비교합니다. for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) plt.setp(ax, xticks=(), yticks=()) # 개별 degree별로 Polynomial 변환합니다. polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(\"polynomial_features\", polynomial_features), # \"\"에 대한 객체가 반환 (\"linear_regression\", linear_regression)]) pipeline.fit(X.reshape(-1, 1), y) # 교차 검증으로 다항 회귀를 평가합니다. scores = cross_val_score(pipeline, X.reshape(-1,1), y,scoring=\"neg_mean_squared_error\", cv=10) coefficients = pipeline.named_steps['linear_regression'].coef_ # coef_: 회귀 계수 나오게 하는 명령어 print('\\nDegree &#123;0&#125; 회귀 계수는 &#123;1&#125; 입니다.'.format(degrees[i], np.round(coefficients),2)) print('Degree &#123;0&#125; MSE 는 &#123;1:.2f&#125; 입니다.'.format(degrees[i] , -1*np.mean(scores))) # 0 부터 1까지 테스트 데이터 세트를 100개로 나눠 예측을 수행합니다. # 테스트 데이터 세트에 회귀 예측을 수행하고 예측 곡선과 실제 곡선을 그려서 비교합니다. X_test = np.linspace(0, 1, 100) # 예측값 곡선 plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\") # 실제 값 곡선 plt.plot(X_test, true_fun(X_test), '--', label=\"True function\") plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\") plt.xlabel(\"x\"); plt.ylabel(\"y\"); plt.xlim((0, 1)); plt.ylim((-2, 2)); plt.legend(loc=\"best\") plt.title(\"Degree &#123;&#125;\\nMSE = &#123;:.2e&#125;(+/- &#123;:.2e&#125;)\".format(degrees[i], -scores.mean(), scores.std())) plt.show() Degree 1 회귀 계수는 [-2.] 입니다. Degree 1 MSE 는 0.41 입니다. Degree 4 회귀 계수는 [ 0. -18. 24. -7.] 입니다. Degree 4 MSE 는 0.04 입니다. Degree 15 회귀 계수는 [-2.98300000e+03 1.03899000e+05 -1.87416100e+06 2.03716240e+07 -1.44873341e+08 7.09315840e+08 -2.47066022e+09 6.24561781e+09 -1.15676672e+10 1.56895187e+10 -1.54006300e+10 1.06457475e+10 -4.91378589e+09 1.35919961e+09 -1.70381209e+08] 입니다. Degree 15 MSE 는 182594790.08 입니다. 실선: 다항 회귀 예측 곡선 점선: 실제 데이터 세트 X, Y의 코사인 곡선 학습 데이터: 0부터 1까지 30개의 임의 X값과 그에 따른 코사인 Y값에 잡음을 변동 값으로 추가해 구성 MSE(Mean Squared Error) 평가: 학습 데이터를 10개의 교차 검증 세트로 나누어 측정한 후 평균한 것 결과 해석 Degree 1 예측 곡선 단순한 직선으로 단순 선형 회귀와 동일 실제 데이터 세트인 코사인 데이터 세트를 직선으로 예측하기에는 너무 단순함 예측 곡선이 학습 데이터 패턴을 반영하지 못하는 과소적합 모델 MSE값: 약 0.407 Degree 4 예측 곡선 실제 데이터 세트와 유사한 모습 변동하는 잡음까지는 예측하지 못했지만, 학습 데이터 세트를 비교적 잘 반영해 코사인 곡선 기반으로 테스트 데이터를 잘 예측한 곡선을 가진 모델 MSE값: 0.043 (가장 뛰어난 예측 성능 = well-fit) Degree 15 예측 곡선 MSE값이 182815432이 될 정도로 이상한 오류 값 발생(과적합 강조를 위해 만든 예측 곡선) 데이터 세트의 변동 잡음까지 지나치게 반영한 결과, 예측 곡선이 학습 데이터 세트만 정확히 예측하고 테스트 값의 실제 곡선과는 완전히 다른 형태의 예측 곡선이 만들어짐 학습 데이터에 너무 충실하게 맞춘 심한 과적합 모델 결론 좋은 예측 모델은 학습 데이터 패턴을 잘 반영하면서도 복잡하지 않은, 균형 잡힌(Balanced) 모델을 의미 (3) 편향-분산 트레이드 오프 머신러닝이 극복해야 할 이슈 고편향(High Bias)성 Degree 1 모델처럼 매우 단순화된 모델로서 지나치게 한 방향성으로 치우친 경향을 보임 고분산(High Variance)성 Degree 15 모델처럼 학습 데이터 하나하나 특성을 반영하여 매우 복잡하고 지나치게 높은 변동성을 가짐 - 이미지 출처 일반적으로 편향과 분산은 한 쪽이 높으면, 한 쪽이 낮아지는 경향이 있음→ 편향이 높으면 분산이 낮아지고(과소적합), 분산이 높으면 편향이 낮아짐(과적합) 편향과 분산 관계에 따른 전체 오류 값(Total Error) 변화- 이미지 출처 편향이 너무 높으면 전체 오류가 높음 편향을 낮출수록 분산이 높아지고 전체 오류도 낮아짐 골디락스: 편향을 낮추고 분산을 높이며, 전체 오류가 가장 낮아지는 지점 골디락스 지점을 통과하며 분산을 지속적으로 높이면 전체 오류 값이 오히려 증가하며 예측 성능이 다시 저하됨 정리 과소적합: 높은 편향/낮은 분산에서 일어나기 쉬움 과적합: 낮은 편향/높은 분산에서 일어나기 쉬움→ 편향과 분산이 트레이드 오프를 이루며 오류 cost 값이 최대로 낮아지는 모델을 구축하는 것이 중요 06. 규제 선형 모델: 릿지, 라쏘, 엘라스틱넷(1) 규제 선형 모델의 개요 이전까지의 선형 모델 비용 함수는 RSS를 최소화하는(실제값과 예측값의 차이를 최소화하는) 것만 고려 학습 데이터에 지나치게 맞추게 되고, 회귀 계수가 쉽게 커지는 문제가 발생 변동성이 오히려 심해져 테스트 데이터 세트에서 예측 성능이 저하되게 쉬움→ 비용 함수는 학습 데이터의 잔차 오류값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 균형을 이루어야 함 회귀 계수의 크기를 제어해 과적합을 개선하려면, 비용(Cost) 함수의 목표가 아래와 같이 Min(RSS(W)+alpha&#x2217; | | W | | 2 2 ) 를 최소화하는 것으로 변경될 수 있음 비용 함수 목표 = M i n ( R S S ( W ) + a l p h a &#x2217; | | W | | 2 2 ) alpha: 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터 alpha가 0(또는 매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W)+0)가 됨 alpha가 무한대(또는 매우 큰 값)이라면 비용 함수 식은 RSS(W)에 비해 alpha&#x2217; | | W | | 2 2 값이 너무 커지므로 W 값을 0 또는 매우 작게 만들어야 Cost가 최소화 되는 비용 함수 목표를 달성할 수 있음 → alpha 값을 크게 하면 비용 함수는 회귀 계수 W의 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W의 값이 커져도 어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있음 alpha를 0에서부터 지속적으로 값을 증가시키면 회귀 계수 값의 크기를 감소시킬 수 있음 규제(Regularization): 비용 함수에 alpha 값으로 패널티를 부여해 회귀 계수 값의 크기를 감소해 과적합을 개선하는 방식 규제 방식 L2 규제: alpha&#x2217; | | W | | 2 2 와 같이 W 제곱에 패널티를 부여하는 방식 ← 릿지(Ridge) 회귀 L1 규제: alpha&#x2217; | | W | | 1 와 같이 W의 절대값에 패널티를 부여, 영향력이 크지 않은 회귀 계수 값을 0으로 변환 ← 라쏘(Lasso) 회귀 (2) 릿지 회귀 사이킷런은 Ridge 클래스로 릿지 회귀를 구현 주요 생성 파라미터: alpha, 릿지 회귀의 alpha L2 규제 계수에 해당 - 보스턴 주택 가격을 Ridge 클래스로 예측하고, cross_val_score()로 평가하기 # 앞의 LinearRegression예제에서 분할한 feature 데이터 셋인 X_data과 Target 데이터 셋인 Y_target 데이터셋을 그대로 이용 from sklearn.linear_model import Ridge from sklearn.model_selection import cross_val_score # boston 데이타셋 로드 boston = load_boston() # boston 데이타셋 DataFrame 변환 bostonDF = pd.DataFrame(boston.data , columns = boston.feature_names) # boston dataset의 target array는 주택 가격임. 이를 PRICE 컬럼으로 DataFrame에 추가함. bostonDF['PRICE'] = boston.target print('Boston 데이타셋 크기 :',bostonDF.shape) y_target = bostonDF['PRICE'] X_data = bostonDF.drop(['PRICE'],axis=1,inplace=False) ridge = Ridge(alpha = 10) # 일반적으로는 alpha를 1로 설정함 neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(' 5 folds 의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 3)) print(' 5 folds 의 개별 RMSE scores : ', np.round(rmse_scores,3)) print(' 5 folds 의 평균 RMSE : &#123;0:.3f&#125; '.format(avg_rmse)) Boston 데이타셋 크기 : (506, 14) 5 folds 의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517] 5 folds 의 개별 RMSE scores : [3.38 4.929 5.305 8.637 5.34 ] 5 folds 의 평균 RMSE : 5.518 결과 해석 릿지의 5개 폴드 세트 평균 RMSE: 5.524 앞 예제(규제 없는 LinearRegression) 평균인 5.836보다 뛰어난 예측 성능을 보임 - 릿지의 alpha 값을 0, 0.1, 1, 10, 100으로 변화시키며 RMSE와 회귀 계수 값 변화 살펴보기 # Ridge에 사용될 alpha 파라미터의 값들을 정의 alphas = [0 , 0.1 , 1 , 10 , 100] # alphas list 값을 iteration하면서 alpha에 따른 평균 rmse 구함. for alpha in alphas : ridge = Ridge(alpha = alpha) #cross_val_score를 이용하여 5 fold의 평균 RMSE 계산 neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print('alpha &#123;0&#125; 일 때 5 folds 의 평균 RMSE : &#123;1:.3f&#125; '.format(alpha,avg_rmse)) alpha 0 일 때 5 folds 의 평균 RMSE : 5.829 alpha 0.1 일 때 5 folds 의 평균 RMSE : 5.788 alpha 1 일 때 5 folds 의 평균 RMSE : 5.653 alpha 10 일 때 5 folds 의 평균 RMSE : 5.518 alpha 100 일 때 5 folds 의 평균 RMSE : 5.330 결과 해석 alpha가 100일 때, 평균 RMSE가 5.332로 가장 좋음 - alpha 값 변화에 따른 피처의 회귀 계수 값을 가로 막대 그래프로 시각화 # 각 alpha에 따른 회귀 계수 값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성 fig , axs = plt.subplots(figsize=(18,6) , nrows=1 , ncols=5) # 각 alpha에 따른 회귀 계수 값을 데이터로 저장하기 위한 DataFrame 생성 coeff_df = pd.DataFrame() # alphas 리스트 값을 차례로 입력해 회귀 계수 값 시각화 및 데이터 저장. pos는 axis의 위치 지정 for pos , alpha in enumerate(alphas) : ridge = Ridge(alpha = alpha) ridge.fit(X_data , y_target) # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=ridge.coef_ , index=X_data.columns ) colname='alpha:'+str(alpha) coeff_df[colname] = coeff # 막대 그래프로 각 alpha 값에서의 회귀 계수를 시각화. 회귀 계수값이 높은 순으로 표현 coeff = coeff.sort_values(ascending=False) axs[pos].set_title(colname) axs[pos].set_xlim(-3,6) sns.barplot(x=coeff.values , y=coeff.index, ax=axs[pos]) # for 문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀 계수를 DataFrame으로 표시 plt.show() 결과 해석 alpha 값을 계속 증가시킬수록 회귀 계수 값은 지속적으로 작아짐 특히, NOX 피처의 경우 alpha 값을 계속 증가시킴에 따라 회귀 계수가 크게 작아지고 있음 - DataFrame에 저장된 alpha 값 변화에 따른 릿지 회귀 계수 값 구하기 ridge_alphas = [0 , 0.1 , 1 , 10 , 100] sort_column = 'alpha:'+str(ridge_alphas[0]) coeff_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; alpha:0 alpha:0.1 alpha:1 alpha:10 alpha:100 RM 3.809865 3.818233 3.854000 3.702272 2.334536 CHAS 2.686734 2.670019 2.552393 1.952021 0.638335 RAD 0.306049 0.303515 0.290142 0.279596 0.315358 ZN 0.046420 0.046572 0.047443 0.049579 0.054496 INDUS 0.020559 0.015999 -0.008805 -0.042962 -0.052826 B 0.009312 0.009368 0.009673 0.010037 0.009393 AGE 0.000692 -0.000269 -0.005415 -0.010707 0.001212 TAX -0.012335 -0.012421 -0.012912 -0.013993 -0.015856 CRIM -0.108011 -0.107474 -0.104595 -0.101435 -0.102202 LSTAT -0.524758 -0.525966 -0.533343 -0.559366 -0.660764 PTRATIO -0.952747 -0.940759 -0.876074 -0.797945 -0.829218 DIS -1.475567 -1.459626 -1.372654 -1.248808 -1.153390 NOX -17.766611 -16.684645 -10.777015 -2.371619 -0.262847 결과 해석 alpha 값이 증가하며 회귀 계소가 지속적으로 작아짐 단, 릿지 회귀는 회귀 계수를 0으로 만들지 않음 (3) 라쏘 회귀 라쏘 회귀: W의 절댓값에 패널티를 부여하는 L1 규제를 선형 회귀에 적용한 것 L1 규제는 alpha&#x2217; | | W | | 1 를 의미하며, 라쏘 회귀 비용함수 목표는 RSS(W) + a l p h a &#x2217; | | W | | 1 식을 최소화하는 W를 찾는 것 L2 규제가 회귀 계쑤 크기를 감소시키는 데 반해, L1 규제는 불필요한 회귀 계수를 급격히 감소시켜 0으로 만들고 제거함 L1 규제는 적절한 피처만 회귀에 포함시키는 피처 선택의 득성을 가짐 사이킷런은 Lasso 클래스로 라쏘 회귀를 구현 주요 파라미터: alpha, 라쏘 회귀의 alpha L1 규제 계수에 해당 - Lasso 클래스로 라쏘의 alpha 값을 변화시키며 RMSE와 각 피처의 회귀 계수 출력하기 from sklearn.linear_model import Lasso, ElasticNet # alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀 계수값들을 DataFrame으로 반환 def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True, return_coeff=True): coeff_df = pd.DataFrame() if verbose : print('####### ', model_name , '#######') for param in params: if model_name =='Ridge': model = Ridge(alpha=param) elif model_name =='Lasso': model = Lasso(alpha=param) elif model_name =='ElasticNet': model = ElasticNet(alpha=param, l1_ratio=0.7) neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring=\"neg_mean_squared_error\", cv = 5) avg_rmse = np.mean(np.sqrt(-1 * neg_mse_scores)) print('alpha &#123;0&#125;일 때 5 폴드 세트의 평균 RMSE: &#123;1:.3f&#125; '.format(param, avg_rmse)) # cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출 model.fit(X_data_n , y_target_n) if return_coeff: # alpha에 따른 피처별 회귀 계수를 Series로 변환하고 이를 DataFrame의 컬럼으로 추가. coeff = pd.Series(data=model.coef_ , index=X_data_n.columns ) colname='alpha:'+str(param) coeff_df[colname] = coeff return coeff_df # end of get_linear_regre_eval # 라쏘에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출 lasso_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_lasso_df =get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target) ####### Lasso ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.612 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.615 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.669 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.776 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.189 결과 해석 alpha가 0.07일 때, 가장 좋은 평균 RMSE를 보여줌 - alpha 값에 따른 피처별 회귀 계수 # 반환된 coeff_lasso_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력 sort_column = 'alpha:'+str(lasso_alphas[0]) coeff_lasso_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 RM 3.789725 3.703202 2.498212 0.949811 0.000000 CHAS 1.434343 0.955190 0.000000 0.000000 0.000000 RAD 0.270936 0.274707 0.277451 0.264206 0.061864 ZN 0.049059 0.049211 0.049544 0.049165 0.037231 B 0.010248 0.010249 0.009469 0.008247 0.006510 NOX -0.000000 -0.000000 -0.000000 -0.000000 0.000000 AGE -0.011706 -0.010037 0.003604 0.020910 0.042495 TAX -0.014290 -0.014570 -0.015442 -0.015212 -0.008602 INDUS -0.042120 -0.036619 -0.005253 -0.000000 -0.000000 CRIM -0.098193 -0.097894 -0.083289 -0.063437 -0.000000 LSTAT -0.560431 -0.568769 -0.656290 -0.761115 -0.807679 PTRATIO -0.765107 -0.770654 -0.758752 -0.722966 -0.265072 DIS -1.176583 -1.160538 -0.936605 -0.668790 -0.000000 결과 해석 alpha의 크기가 증가함에 따라 일부 피처 회귀 계수는 아예 0으로 바뀜 NOX 속성은 alpha가 0.07일 때부터 회귀 계수가 0이며, alpha를 증가시키며 INDUS, CHAS와 같은 속성 회귀 계수가 0으로 바뀜 회귀 계수가 0인 피처는 회귀 식에서 제외되며 피처 선택의 효과를 얻을 수 있음 (4) 엘라스틱넷 회귀 엘라스틱넷(Elastic Net) 회귀: L2 규제와 L1 규제를 결합한 회귀 엘라스틱넷 회귀 비용함수 목표: RSS(W)+alpha2&#x2217; | | W | | 2 2 + a l p h a 1 &#x2217; | | W | | 1 식을 최소화하는 W를 찾는 것 엘라스틱넷은 라쏘 회귀가 상관관계가 높은 피처들의 경우에, 중요 피처만을 선택하고 다른 피처 회귀 계수는 0으로 만드는 성향이 강함 alpha 값에 따라 회귀 계쑤 값이 급격히 변동할 수 있는데, 엘라스틱넷 회귀는 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것 엘라스틱넷 회귀의 단점은 L1과 L2 규제가 결합된 규제로 인해 수행 시간이 상대적으로 오래 걸림 사이킷런은 Elastic Net 클래스로 엘라스틱넷 회귀를 구현 주요 파라미터: aplha, l1_ration Elastic Net 클래스의 aplha는 Ridge와 Lasso 클래스의 alpha 값과는 다름 엘라스틱넷 규제는 a * L1 + b * L2로 정의될 수 있으며, 이 때 a는 L1 규제의 alpha값, b는 L2 규제의 alpha 값 따라서 ElasticNet 클래스의 alpha 파라미터 값은 a + b 값 ElasticNet 클래스의 l1_ratio 파라미터 값은 a / (a + b) l1_ratio가 0이면 a가 0이므로 L2 규제와 동일하고, l1_ratio가 1이면 b가 0이므로 L1 규제와 동일 - Elastic Net 클래스로 엘라스틱넷 alpha 값을 변화시키며 RMSE와 각 피처의 회귀 계수 출력하기 l1_ratio를 0.7로 고정한 이유: 단순히 alpha 값의 변화만 살피기 위해 # 엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출 # l1_ratio는 0.7로 고정 elastic_alphas = [ 0.07, 0.1, 0.5, 1, 3] coeff_elastic_df =get_linear_reg_eval('ElasticNet', params=elastic_alphas, X_data_n=X_data, y_target_n=y_target) ####### ElasticNet ####### alpha 0.07일 때 5 폴드 세트의 평균 RMSE: 5.542 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.526 alpha 0.5일 때 5 폴드 세트의 평균 RMSE: 5.467 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.597 alpha 3일 때 5 폴드 세트의 평균 RMSE: 6.068 # 반환된 coeff_elastic_df를 첫번째 컬럼순으로 내림차순 정렬하여 회귀계수 DataFrame출력 sort_column = 'alpha:'+str(elastic_alphas[0]) coeff_elastic_df.sort_values(by=sort_column, ascending=False) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; alpha:0.07 alpha:0.1 alpha:0.5 alpha:1 alpha:3 RM 3.574162 3.414154 1.918419 0.938789 0.000000 CHAS 1.330724 0.979706 0.000000 0.000000 0.000000 RAD 0.278880 0.283443 0.300761 0.289299 0.146846 ZN 0.050107 0.050617 0.052878 0.052136 0.038268 B 0.010122 0.010067 0.009114 0.008320 0.007020 AGE -0.010116 -0.008276 0.007760 0.020348 0.043446 TAX -0.014522 -0.014814 -0.016046 -0.016218 -0.011417 INDUS -0.044855 -0.042719 -0.023252 -0.000000 -0.000000 CRIM -0.099468 -0.099213 -0.089070 -0.073577 -0.019058 NOX -0.175072 -0.000000 -0.000000 -0.000000 -0.000000 LSTAT -0.574822 -0.587702 -0.693861 -0.760457 -0.800368 PTRATIO -0.779498 -0.784725 -0.790969 -0.738672 -0.423065 DIS -1.189438 -1.173647 -0.975902 -0.725174 -0.031208 결과 해석 alpha 0.5일 때, RMSE가 5.468로 가장 좋은 예측 성능을 보임 alpha 값에 따른 피처들의 회귀 계수 값이 라쏘보다는 0 되는 값이 적음 (5) 선형 회귀 모델을 위한 데이터 변환 선형 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타겟 간에 선형의 관계가 있다 가정하고, 이러한 최적의 선형함수를 찾아내 결과를 예측 선형 회귀 모델은 피처값과 타겟값의 분포가 정규 분포(즉 평균을 중심으로 종 모양으로 데이터 값이 분포된 형태) 형태를 매우 선호함 타겟값의 경우 정규 분포 형태가 아니라 특정값의 분포가 치우친 왜곡된 형태의 분포도일 경우 예측 성능에 부정적인 영향을 미칠 가능성이 높음 피처값 역시 왜곡된 분포도로 인해 예측 성능에 부정적인 영향을 미칠 수 있음 일반적으로 선형 회귀 모델을 적용하기전에 데이터에 대한 스케일링/정규화 작업을 수행함 단, 스케일링/정규화 작업을 선행한다고 해서 무조건 예측 성능이 향상되는 것은 아니며 중요한 피처들이나 타겟값의 분포도가 심하게 왜곡됐을 경우에 이러한 변환 작업을 수행함 피처 데이터 셋과 타겟 데이터 셋에 이러한 스케일링/정규화 작업을 수행하는 방법이 다름 - 사이킷런을 이용해 피처 데이터 세트에 적용하는 방법 세 가지 StandardScaler 클래스를 이용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 셋으로 변환하거나 MinMaxScaler 클래스를 이용해 최소값이 0이고 최대값이 1인 값으로 정규화를 수행 스케일링/정규화를 수행한 데이터 셋에 다시 다항 특성을 적용하여 변환하는 방법이다. 보통 1번 방법을 통해 예측 성능에 향상이 없을 경우 이와 같은 방법을 적용 원래 값에 log 함수를 적용하면 보다 정규 분포에 가까운 형태로 값이 분포(= 로그 변환)된다. 실제로 선형 회귀에서는 앞서 소개한 1,2번 방법보다 로그 변환이 훨씬 많이 사용되는 변환 방법(1번 방법: 예측 성능 향상을 크게 기대하기 어려운 경우가 많음, 2번 방법: 피처 개수가 매우 많을 경우에는 다항 변환으로 생성되는 피처의 개수가 기하급수로 늘어나서 과적합의 이슈가 발생할 수 있음) 타겟값의 경우 일반적으로 로그 변환을 적용 결정값을 정규 분포나 다른 정규값으로 변환하면 변환된 값을 다시 원본 타겟값으로 원복하기 어려울 수 있음 왜곡된 분포도 형태의 타겟값을 로그 변환하여 예측 성능 향상이 된 경우가 많은 사례에서 검증되었기 때문에 타겟값의 경우는 로그 변환을 적용 - 보스턴 주택가격 피처 데이터 세트에 표준 정규 분포 변환, 최댓값/최솟값 정규화, 로그 변환을 적용한 후 RMSE로 각 경우별 예측 성능 측정하기 사용 함수: get_scaled_data() method 인자로 변환 방법을 결정하며, 표준 정규 분포 변환(Standard), 최댓값/최솟값 정규와(MinMax), 로그 변환(Log) 중에 하나를 선택 p_degree: 다항식 특성을 추가할 때, 다항식 차수가 입력됨 (2를 넘기지 않음) np.log1p(): log() 함수만 적용하면 언더 플로우가 발생하기 쉬워 1 + log() 함수를 적용 from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures # method는 표준 정규 분포 변환(Standard), 최대값/최소값 정규화(MinMax), 로그변환(Log) 결정 # p_degree는 다향식 특성을 추가할 때 적용. p_degree는 2이상 부여하지 않음. def get_scaled_data(method='None', p_degree=None, input_data=None): if method == 'Standard': scaled_data = StandardScaler().fit_transform(input_data) elif method == 'MinMax': scaled_data = MinMaxScaler().fit_transform(input_data) elif method == 'Log': scaled_data = np.log1p(input_data) else: scaled_data = input_data if p_degree != None: scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data) return scaled_data # Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환방법에 따른 RMSE 추출. alphas = [0.1, 1, 10, 100] #변환 방법은 모두 6개, 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성 # 최대/최소 정규화, 최대/최소 정규화+다항식 특성, 로그변환 scale_methods=[(None, None), ('Standard', None), ('Standard', 2), ('MinMax', None), ('MinMax', 2), ('Log', None)] for scale_method in scale_methods: X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data) print(X_data_scaled.shape, X_data.shape) print('\\n## 변환 유형:&#123;0&#125;, Polynomial Degree:&#123;1&#125;'.format(scale_method[0], scale_method[1])) get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False, return_coeff=False) (506, 13) (506, 13) ## 변환 유형:None, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.788 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.653 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.518 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.330 (506, 13) (506, 13) ## 변환 유형:Standard, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.826 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.803 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.637 alpha 100일 때 5 폴드 세트의 평균 RMSE: 5.421 (506, 104) (506, 13) ## 변환 유형:Standard, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 8.827 alpha 1일 때 5 폴드 세트의 평균 RMSE: 6.871 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.485 alpha 100일 때 5 폴드 세트의 평균 RMSE: 4.634 (506, 13) (506, 13) ## 변환 유형:MinMax, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.764 alpha 1일 때 5 폴드 세트의 평균 RMSE: 5.465 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.754 alpha 100일 때 5 폴드 세트의 평균 RMSE: 7.635 (506, 104) (506, 13) ## 변환 유형:MinMax, Polynomial Degree:2 alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 5.298 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.323 alpha 10일 때 5 폴드 세트의 평균 RMSE: 5.185 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.538 (506, 13) (506, 13) ## 변환 유형:Log, Polynomial Degree:None alpha 0.1일 때 5 폴드 세트의 평균 RMSE: 4.770 alpha 1일 때 5 폴드 세트의 평균 RMSE: 4.676 alpha 10일 때 5 폴드 세트의 평균 RMSE: 4.836 alpha 100일 때 5 폴드 세트의 평균 RMSE: 6.241 결과 해석 표준 정규 분포와 최솟값/최댓값 정규화로 피처 데이터 세트를 변경해도 성능상의 개선은 없음 표준 정규 분포로 일차 변환 후 2차 다항식 변환 시, alpha = 100에서 4.631로 성능 개선 최솟값/최댓값 정규화로 일차 변환 후 2차 다항식 변환 시, aplha = 1에서 4.320으로 성능 개선 단, 다항식 변환은 피처 개수가 많을 경우 적용하기 힘들며, 데이터 건수가 많아지면 시간이 많이 소모되어 적용하기에 한계가 있음 반면, 로그 변환은 alpha가 0.1, 1, 10인 경우 모두 성능이 좋게 향상됨 일반적으로 선형 회귀를 적용하려는 데이터 세트에, 데이터 값 분포가 심하게 왜곡되어 있을 경우에, 로그 변환을 적용하는 편이 더 좋은 결과를 기대할 수 있음 07. 로지스틱 회귀 로지스틱 회귀: 선형 회귀 방식을 분류에 적용한 알고리즘 → ‘분류’에 사용 선형 회귀 계열이나, 선형 회귀와 다른 점은 학습을 통해 선형 함수의 회귀 최적선을 찾지 않고 시그모이드(Sigmoid) 함수 최적선을 찾고 시그모이드 함수 반환 값을 확률로 간주하여 확률에 따라 분류를 결정하는 것 시그모이드 함수 y = $\\frac{1}{1+e-x}$ (-x는 제곱) 시그모이드 함수는 x 값이 +, -로 아무리 커지거나 작아져도 y 값은 0과 1 사이 값만 반환 x 값이 커지면 1에 근사하며 x 값이 작아지면 0에 근사 x가 0일 때는 0.5 회귀 분제를 분류 문제에 적용하기 종양의 크기에 따라 악성 종양인지(Yes = 1), 아닌지(No = 0)를 회귀를 이용하여 1과 0 값으로 예측하는 것 종양 크기에 따라 악성될 확률이 높다고 하면 아래 왼쪽 그림과 같이 분포하며 선형 회귀 선을 그릴 수 있으나, 해당 회귀 라인은 0과 1을 제대로 분류하지 못함 오른쪽 그림처럼 시그모이드 함수를 이용하면 조금 더 정확하게 0과 1을 분류할 수 있음 - 로지스틱 회귀로 암 여부 판단하기: 위스콘신 유방암 데이터 세트 이용 import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from sklearn.datasets import load_breast_cancer from sklearn.linear_model import LogisticRegression cancer = load_breast_cancer() from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # StandardScaler( )로 평균이 0, 분산 1로 데이터 분포도 변환 scaler = StandardScaler() data_scaled = scaler.fit_transform(cancer.data) X_train , X_test, y_train , y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0) - 로지스틱 회귀로 학습 및 예측하고, 정확도와 ROC-AUC 값 구하기 from sklearn.metrics import accuracy_score, roc_auc_score # 로지스틱 회귀를 이용하여 학습 및 예측 수행. lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) lr_preds = lr_clf.predict(X_test) # accuracy와 roc_auc 측정 print('accuracy: &#123;:0.3f&#125;'.format(accuracy_score(y_test, lr_preds))) print('roc_auc: &#123;:0.3f&#125;'.format(roc_auc_score(y_test , lr_preds))) accuracy: 0.977 roc_auc: 0.972 사이킷런 LogisticRegression 클래스의 주요 하이퍼 파라미터로 penalty와 C가 존재 penalty는 규제의 유형을 설정하며 ‘l2’로 설정 시 L2 규제를, ‘l1’으로 설정 시 L1 규제를 뜻함 C는 규제 강도를 조절하는 alpha 값의 역수로 C = $\\frac{1}{alpha}$ C 값이 작을수록 규제 강도가 큼을 의미 - 위스콘신 데이터 세트에서 해당 하이퍼 파라미터를 최적화하기 from sklearn.model_selection import GridSearchCV params=&#123;'penalty':['l2', 'l1'], 'C':[0.01, 0.1, 1, 1, 5, 10]&#125; grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3 ) grid_clf.fit(data_scaled, cancer.target) print('최적 하이퍼 파라미터:&#123;0&#125;, 최적 평균 정확도:&#123;1:.3f&#125;'.format(grid_clf.best_params_, grid_clf.best_score_)) C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; 최적 하이퍼 파라미터:&#123;&#39;C&#39;: 1, &#39;penalty&#39;: &#39;l2&#39;&#125;, 최적 평균 정확도:0.975 C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: Traceback (most recent call last): File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py&quot;, line 531, in _fit_and_score estimator.fit(X_train, y_train, **fit_params) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 1304, in fit solver = _check_solver(self.solver, self.penalty, self.dual) File &quot;C:\\Users\\naeun\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py&quot;, line 442, in _check_solver raise ValueError(&quot;Solver %s supports only &#39;l2&#39; or &#39;none&#39; penalties, &quot; ValueError: Solver lbfgs supports only &#39;l2&#39; or &#39;none&#39; penalties, got l1 penalty. warnings.warn(&quot;Estimator fit failed. The score on this train-test&quot; 로지스틱 회귀는 가볍고 빠르며, 이진 분류 예측 성능까지 뛰어남 이진 분류의 기본 모델로 사용하는 경우가 많음 로지스틱 회귀는 희소한 데이터 세트 분류에서도 뛰어난 성능을 보임 텍스트 분류에서도 자주 사용 08. 회귀 트리 회귀 함수를 기반으로 하지 않고, 결정 트리와 같이 트리를 기반으로 하는 회귀 방식 소개 트리 기반이 회귀: 회귀 트리를 이용하는 것 회귀를 위한 트리를 생성하고 이를 기반으로 회귀를 예측하는 것 4장 분류에서 언급한 분류 트리와 비슷하나, 리프 노트에서 예측 결정 값을 만드는 과정에 차이가 있음 분류 트리는 특정 클래스 레이블을 결정하나, 회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측값을 계산 예시(p.335-336) 피처가 단 하나인 X 피처 데이터 세트와 결정값 Y가 2차원 평면에 있다고 가정 데이터 세트의 X 피처를 결정 트리 기반으로 분할하면 X값의 균일도를 반영한 지니 계수에 따라 분할됨 루트 노드를 Split 0 기준으로 분할하고, 분할된 규칙 노드에서 다시 Split 1과 Split 2 규칙 노드로 분할할 수 있음 Split 2는 다시 재귀적으로 Split 3 규칙 노드로 트리 규칙으로 변환될 수 있음 리프 노드 생성 기준에 부합하는 트리 분할이 완료됐다면, 리프 노드에 소속된 데이터 값의 평균값을 구해 최종적으로 리프 노드에 결정 값으로 할당함 사이킷런 트리 기반 회귀와 분류의 Estimator 클래스 알고리즘 회귀 Estimator 클래스 분류 Estimator 클래스 Decision Tree DecisionTreeRegressor DecisionTreeClassifier Gradient Boosting GradientBoostingRegressor GradientBoostingClassifier XGBoost XGBRegressor XGBClassifier LightGBM LGBMRegressor LGBMClassifier - 사이킷런 랜덤 포레스트 회귀 트리인 RandomForestRegressor로 보스턴 주택 가격 예측 수행하기 from sklearn.datasets import load_boston from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestRegressor import pandas as pd import numpy as np # 보스턴 데이터 세트 로드 boston = load_boston() bostonDF = pd.DataFrame(boston.data, columns = boston.feature_names) bostonDF['PRICE'] = boston.target y_target = bostonDF['PRICE'] X_data = bostonDF.drop(['PRICE'], axis=1,inplace=False) rf = RandomForestRegressor(random_state=0, n_estimators=1000) neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print(' 5 교차 검증의 개별 Negative MSE scores: ', np.round(neg_mse_scores, 2)) print(' 5 교차 검증의 개별 RMSE scores : ', np.round(rmse_scores, 2)) print(' 5 교차 검증의 평균 RMSE : &#123;0:.3f&#125; '.format(avg_rmse)) 5 교차 검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88] 5 교차 검증의 개별 RMSE scores : [2.81 3.63 4.54 6.8 4.34] 5 교차 검증의 평균 RMSE : 4.423 - 결정 트리, GBM, XGBoost, LightGBM의 Regressor을 모두 이용해 보스턴 주택 가격 예측 수행 사용 함수: get_model_cv_prediction() 입력 모델과 데이터 세트를 입력 받아, 교차 검증으로 평균 RMSE를 계산하는 함수 def get_model_cv_prediction(model, X_data, y_target): neg_mse_scores = cross_val_score(model, X_data, y_target, scoring=\"neg_mean_squared_error\", cv = 5) rmse_scores = np.sqrt(-1 * neg_mse_scores) avg_rmse = np.mean(rmse_scores) print('##### ',model.__class__.__name__ , ' #####') print(' 5 교차 검증의 평균 RMSE : &#123;0:.3f&#125; '.format(avg_rmse)) - 다양한 유형의 회귀 트리를 생성하고, 보스턴 주택 가격 예측하기 from sklearn.tree import DecisionTreeRegressor from sklearn.ensemble import GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor dt_reg = DecisionTreeRegressor(random_state=0, max_depth=4) rf_reg = RandomForestRegressor(random_state=0, n_estimators=1000) gb_reg = GradientBoostingRegressor(random_state=0, n_estimators=1000) xgb_reg = XGBRegressor(n_estimators=1000) lgb_reg = LGBMRegressor(n_estimators=1000) # 트리 기반의 회귀 모델을 반복하면서 평가 수행 models = [dt_reg, rf_reg, gb_reg, xgb_reg, lgb_reg] for model in models: get_model_cv_prediction(model, X_data, y_target) ##### DecisionTreeRegressor ##### 5 교차 검증의 평균 RMSE : 5.978 ##### RandomForestRegressor ##### 5 교차 검증의 평균 RMSE : 4.423 ##### GradientBoostingRegressor ##### 5 교차 검증의 평균 RMSE : 4.269 ##### XGBRegressor ##### 5 교차 검증의 평균 RMSE : 4.251 ##### LGBMRegressor ##### 5 교차 검증의 평균 RMSE : 4.646 - feature_importances_를 이용해 보스턴 주택 가격 모델의 피처별 중요도 시각화하기 회귀 트리 Regressor 클래스는 선형 회귀와 다른 처리 방식으로, 회귀 계수를 제공하는 coef_ 속성이 없으나, feature_importances_를 이용해 피처별 중요도를 알 수 있음 import seaborn as sns %matplotlib inline rf_reg = RandomForestRegressor(n_estimators=1000) # 앞 예제에서 만들어진 X_data, y_target 데이터 셋을 적용하여 학습합니다. rf_reg.fit(X_data, y_target) feature_series = pd.Series(data=rf_reg.feature_importances_, index=X_data.columns ) feature_series = feature_series.sort_values(ascending=False) sns.barplot(x= feature_series, y=feature_series.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d019a2c850&gt; - 회귀 트리 Regressor가 예측값을 판단하는 방법을 선형 회귀와 비교하여 시각화하기 보스턴 데이터 세트를 100개만 샘플링하고 RM과 PRICE 칼럼만 추출 2차원 평면상에서 X축에 독립변수인 RM, Y축에 종속변수인 PRICE만 가지고 더 직관적으로 예측값을 시각화하기 위한 것 import matplotlib.pyplot as plt %matplotlib inline bostonDF_sample = bostonDF[['RM','PRICE']] bostonDF_sample = bostonDF_sample.sample(n=100,random_state=0) print(bostonDF_sample.shape) plt.figure() plt.scatter(bostonDF_sample.RM , bostonDF_sample.PRICE,c=\"darkorange\") (100, 2) &lt;matplotlib.collections.PathCollection at 0x1d01ef6b520&gt; - LinearRegression과 DecisionTreeRegressor를 max_depth 2, 7로 학습하기 import numpy as np from sklearn.linear_model import LinearRegression # 선형 회귀와 결정 트리 기반의 Regressor 생성. DecisionTreeRegressor의 max_depth는 각각 2, 7 lr_reg = LinearRegression() rf_reg2 = DecisionTreeRegressor(max_depth=2) rf_reg7 = DecisionTreeRegressor(max_depth=7) # 실제 예측을 적용할 테스트용 데이터 셋을 4.5 ~ 8.5 까지 100개 데이터 셋 생성. X_test = np.arange(4.5, 8.5, 0.04).reshape(-1, 1) # 보스턴 주택가격 데이터에서 시각화를 위해 피처는 RM만, 그리고 결정 데이터인 PRICE 추출 X_feature = bostonDF_sample['RM'].values.reshape(-1,1) y_target = bostonDF_sample['PRICE'].values.reshape(-1,1) # 학습과 예측 수행. lr_reg.fit(X_feature, y_target) rf_reg2.fit(X_feature, y_target) rf_reg7.fit(X_feature, y_target) pred_lr = lr_reg.predict(X_test) pred_rf2 = rf_reg2.predict(X_test) pred_rf7 = rf_reg7.predict(X_test) fig , (ax1, ax2, ax3) = plt.subplots(figsize=(14,4), ncols=3) # X축값을 4.5 ~ 8.5로 변환하며 입력했을 때, 선형 회귀와 결정 트리 회귀 예측 선 시각화 # 선형 회귀로 학습된 모델 회귀 예측선 ax1.set_title('Linear Regression') ax1.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\") ax1.plot(X_test, pred_lr,label=\"linear\", linewidth=2 ) # DecisionTreeRegressor의 max_depth를 2로 했을 때 회귀 예측선 ax2.set_title('Decision Tree Regression: \\n max_depth=2') ax2.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\") ax2.plot(X_test, pred_rf2, label=\"max_depth:3\", linewidth=2 ) # DecisionTreeRegressor의 max_depth를 7로 했을 때 회귀 예측선 ax3.set_title('Decision Tree Regression: \\n max_depth=7') ax3.scatter(bostonDF_sample.RM, bostonDF_sample.PRICE, c=\"darkorange\") ax3.plot(X_test, pred_rf7, label=\"max_depth:7\", linewidth=2) [&lt;matplotlib.lines.Line2D at 0x1d01f029910&gt;] 정리 선형 회귀: 예측 회귀선을 직선으로 표현 회귀 트리: 분할되는 데이터 지점에 따라 브랜치를 만들며 계단 형태로 회귀선을 만듦 DecisionTreeRegressor의 max_depth = 7인 경우, 학습 데이터 세트의 이상치(outlier) 데이터도 학습하면서 복잡한 계단 형태의 회귀선을 만들어 과적합 되기 쉬운 모델이 됨 09. 회귀 실습- 자전거 대여 수요 예측 데이터 설명 기간: 2011년 1월 - 2012년 12월 날짜/시간, 기온, 습도, 풍속 등 정보 1시간 간격으로 자전거 대여 횟수 기록 데이터의 주요 칼럼 (결정값: count) datetime: hourly date + timestamp season: 1 = 봄, 2 = 여름, 3 = 가을, 4 = 겨울 holiday: 1= 토/일요일의 주말 제외한 국경일 등의 휴일, 0 = 휴일 아닌 날 workingday: 1 = 토/일요일의 주말 및 휴일이 아닌 주중, 0 = 주말 및 휴일 weather: 1 = 맑음, 약간 구름 낀 흐림, 2 = 안개, 안개 + 흐림, 3 = 가벼운 눈, 가벼운 비 + 천둥, 4 = 심한 눈/비, 천둥/번개 temp: 온도(섭씨) atemp: 체감온도(섭씨) humidity: 상대습도 windspeed: 풍속 casual: 사전 등록되지 않은 사용자 대여 횟수 registered: 사전 등록된 사용자 대여 횟수 count: 대여 횟수 (casual + registered) (1) 데이터 클렌징 및 가공 bike_train.csv 데이터 세트로 모델을 학습한 후, 대여 횟수(count) 예측 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(\"ignore\", category=RuntimeWarning) bike_df = pd.read_csv('./data/bike_train.csv') print(bike_df.shape) bike_df.head(3) (10886, 12) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; datetime season holiday workingday weather temp atemp humidity windspeed casual registered count 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 # 데이터 타입 살펴보기 bike_df.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 10886 entries, 0 to 10885 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 datetime 10886 non-null object 1 season 10886 non-null int64 2 holiday 10886 non-null int64 3 workingday 10886 non-null int64 4 weather 10886 non-null int64 5 temp 10886 non-null float64 6 atemp 10886 non-null float64 7 humidity 10886 non-null int64 8 windspeed 10886 non-null float64 9 casual 10886 non-null int64 10 registered 10886 non-null int64 11 count 10886 non-null int64 dtypes: float64(3), int64(8), object(1) memory usage: 1020.7+ KB 데이터 타입 확인 Null 데이터 없음 datetime 칼럼만 object형, 년-월-일 시:분:초 형식 가공 필요 # 문자열을 datetime 타입으로 변경. bike_df['datetime'] = bike_df.datetime.apply(pd.to_datetime) # datetime 타입에서 년, 월, 일, 시간 추출 bike_df['year'] = bike_df.datetime.apply(lambda x : x.year) bike_df['month'] = bike_df.datetime.apply(lambda x : x.month) bike_df['day'] = bike_df.datetime.apply(lambda x : x.day) bike_df['hour'] = bike_df.datetime.apply(lambda x: x.hour) bike_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; datetime season holiday workingday weather temp atemp humidity windspeed casual registered count year month day hour 0 2011-01-01 00:00:00 1 0 0 1 9.84 14.395 81 0.0 3 13 16 2011 1 1 0 1 2011-01-01 01:00:00 1 0 0 1 9.02 13.635 80 0.0 8 32 40 2011 1 1 1 2 2011-01-01 02:00:00 1 0 0 1 9.02 13.635 80 0.0 5 27 32 2011 1 1 2 # datetime 삭제 # casule + registered = count이므로 casule, registered 값도 삭제 drop_columns = ['datetime','casual','registered'] bike_df.drop(drop_columns, axis=1,inplace=True) - 다양한 회귀 모델을 데이터 세트에 적용해 예측 성능 측정하기 캐글에서 요구한 성능 평가 방법은 RMSLE(Root Mean Square Log Error)로 오류 값 로그에 대한 RMSE 단, 사이킷런은 RMSLE를 제공하지 않아 RMSLE를 수행하는 성능 형가 함수를 만들어야 함 from sklearn.metrics import mean_squared_error, mean_absolute_error # log 값 변환 시 NaN등의 이슈로 log() 가 아닌 log1p() 를 이용하여 RMSLE 계산 def rmsle(y, pred): log_y = np.log1p(y) log_pred = np.log1p(pred) squared_error = (log_y - log_pred) ** 2 rmsle = np.sqrt(np.mean(squared_error)) return rmsle # 사이킷런의 mean_square_error() 를 이용하여 RMSE 계산 def rmse(y,pred): return np.sqrt(mean_squared_error(y,pred)) # MSE, RMSE, RMSLE 를 모두 계산 def evaluate_regr(y,pred): rmsle_val = rmsle(y,pred) rmse_val = rmse(y,pred) # MAE 는 scikit learn의 mean_absolute_error() 로 계산 mae_val = mean_absolute_error(y,pred) print('RMSLE: &#123;0:.3f&#125;, RMSE: &#123;1:.3F&#125;, MAE: &#123;2:.3F&#125;'.format(rmsle_val, rmse_val, mae_val)) (2) 로그 변환, 피처 인코딩, 모델 학습/예측/평가 회귀 모델을 이용해 자전거 대여 횟수 예측하기 먼저, 결괏값이 정규 분포로 되어 있는지 확인해야 함 카테고리형 회귀 모델은 원-핫 인코딩으로 피처를 인코딩해야 함 - 사이킷런의 LinearRegression 객체로 회귀 예측하기 from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.linear_model import LinearRegression , Ridge , Lasso y_target = bike_df['count'] X_features = bike_df.drop(['count'],axis=1,inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) evaluate_regr(y_test ,pred) RMSLE: 1.165, RMSE: 140.900, MAE: 105.924 결과 해석 실제 Target 데이터 값인 대여 횟수(Count)를 감안하면 예측 오류로서는 비교적 큰 값 - 실제값과 예측값이 어느 정도 차이 나는지 DataFrame 칼럼으로 만들어서 오류 값이 가장 큰 순으로 5개만 확인하기 def get_top_error_data(y_test, pred, n_tops = 5): # DataFrame에 컬럼들로 실제 대여횟수(count)와 예측 값을 서로 비교 할 수 있도록 생성. result_df = pd.DataFrame(y_test.values, columns=['real_count']) result_df['predicted_count']= np.round(pred) result_df['diff'] = np.abs(result_df['real_count'] - result_df['predicted_count']) # 예측값과 실제값이 가장 큰 데이터 순으로 출력. print(result_df.sort_values('diff', ascending=False)[:n_tops]) get_top_error_data(y_test,pred,n_tops=5) real_count predicted_count diff 1618 890 322.0 568.0 3151 798 241.0 557.0 966 884 327.0 557.0 412 745 194.0 551.0 2817 856 310.0 546.0 결과 해석 가장 큰 상위 5 오류값은 546 - 568로 실제값을 감안하면 오류가 꽤 큼 회귀에서 큰 예측 오류가 발생할 경우, Target 값의 분포가 왜곡된 형태를 이루는지를 확인해야 함 Target 값 분포는 정규 분포 형태가 가장 좋으며, 왜곡된 경우에는 회귀 예측 성능이 저하되는 경우가 쉽게 발생 - 판다스 DataFrame의 hist()를 이용해 자전거 대여 모델의 Target 값인 count 칼럼이 정규 분포를 이루는지 확인하기 y_target.hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d0203fafd0&gt; 결과 해석 count 칼럼 값이 정규 분포가 아닌, 0 - 200 사이에 왜곡된 것을 알 수 있음 왜곡된 값을 정규 분포 형태로 바꾸는 방법: 로그를 적용해 변환하는 것 Numpy의 log1p()이용 변경된 Target 값을 기반으로 학습하고, 예측한 값은 expm1() 함수를 이용해 원래의 scale 값으로 원상 복구 - lop1p()를 적용한 ‘count’값이 분포 확인하기 y_log_transform = np.log1p(y_target) y_log_transform.hist() &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d01ef87c10&gt; 정규 분포 형태는 아니지만, 왜곡 정도가 많이 향상됨 - 위 데이터로 다시 학습하고 평가하기 # 타겟 컬럼인 count 값을 log1p 로 Log 변환 y_target_log = np.log1p(y_target) # 로그 변환된 y_target_log를 반영하여 학습/테스트 데이터 셋 분할 X_train, X_test, y_train, y_test = train_test_split(X_features, y_target_log, test_size=0.3, random_state=0) lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) pred = lr_reg.predict(X_test) # 테스트 데이터 셋의 Target 값은 Log 변환되었으므로 다시 expm1를 이용하여 원래 scale로 변환 y_test_exp = np.expm1(y_test) # 예측 값 역시 Log 변환된 타겟 기반으로 학습되어 예측되었으므로 다시 exmpl으로 scale변환 pred_exp = np.expm1(pred) evaluate_regr(y_test_exp ,pred_exp) RMSLE: 1.017, RMSE: 162.594, MAE: 109.286 RMSLE 오류는 줄어들었으나, RMSE는 오히려 더 늘어남 - 각 피처의 회귀 계수 값을 시각화해 확인하기 coef = pd.Series(lr_reg.coef_, index=X_features.columns) coef_sort = coef.sort_values(ascending=False) sns.barplot(x=coef_sort.values, y=coef_sort.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d020439100&gt; 결과 해석 Year 피처 회귀 계수 값이 독보적으로 큼 Year는 2011, 2012 두 개의 값으로, year에 따라 자전거 대여 횟수가 크게 영향을 받는다고 할 수 없음 Category 피처지만 숫자형 값으로 되어 있고 2011, 2012가 매우 큰 숫자라 영향을 주게 됨 원-핫 인코딩을 적용해 변환하여야 함 - 여러 칼럼 원-핫 인코딩하고 선형 회귀 모델(LinearRegression, Ridge, Lasso) 모두 학습해 예측 성능 확인하기 사용 함수: get_model_predict() 모델과 학습/테스트 데이터 세트를 입력하면 성능 평가 수치를 반환하는 함수 # 'year', month', 'day', hour'등의 피처들을 One Hot Encoding X_features_ohe = pd.get_dummies(X_features, columns=['year', 'month','day', 'hour', 'holiday', 'workingday','season','weather']) # 원-핫 인코딩이 적용된 feature 데이터 세트 기반으로 학습/예측 데이터 분할. X_train, X_test, y_train, y_test = train_test_split(X_features_ohe, y_target_log, test_size=0.3, random_state=0) # 모델과 학습/테스트 데이터 셋을 입력하면 성능 평가 수치를 반환 def get_model_predict(model, X_train, X_test, y_train, y_test, is_expm1=False): model.fit(X_train, y_train) pred = model.predict(X_test) if is_expm1 : y_test = np.expm1(y_test) pred = np.expm1(pred) print('###',model.__class__.__name__,'###') evaluate_regr(y_test, pred) # end of function get_model_predict # model 별로 평가 수행 lr_reg = LinearRegression() ridge_reg = Ridge(alpha=10) lasso_reg = Lasso(alpha=0.01) for model in [lr_reg, ridge_reg, lasso_reg]: get_model_predict(model,X_train, X_test, y_train, y_test,is_expm1=True) ### LinearRegression ### RMSLE: 0.590, RMSE: 97.688, MAE: 63.382 ### Ridge ### RMSLE: 0.590, RMSE: 98.529, MAE: 63.893 ### Lasso ### RMSLE: 0.635, RMSE: 113.219, MAE: 72.803 결과 해석 원-핫 인코딩 적용 후, 선형 회귀 예측 성능이 많이 향상됨 - 원-핫 인코딩으로 피처가 늘어났으므로, 회귀 계수 상위 25개 피처를 추출해 시각화하기 coef = pd.Series(lr_reg.coef_ , index=X_features_ohe.columns) coef_sort = coef.sort_values(ascending=False)[:20] sns.barplot(x=coef_sort.values , y=coef_sort.index) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d0204cb2e0&gt; 결과 해석 선형 회귀 모델 시 month_9, month_8, month_7 등의 월 관련 피처와 workingday 관련 피처, hour 관련 피처의 회귀 계수가 높은 것을 알 수 있음 월, 주말/주중, 시간대 등 상식선에서 자전거 타는 데 필요한 피처의 회귀 계수가 높아짐→ 선형 회귀 수행 시에는 피처를 어떻게 인코딩하는가가 성능에 큰 영향을 미칠 수 있음 - 회귀 트리로 회귀 예측 수행하기 from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor from xgboost import XGBRegressor from lightgbm import LGBMRegressor # 랜덤 포레스트, GBM, XGBoost, LightGBM model 별로 평가 수행 rf_reg = RandomForestRegressor(n_estimators=500) gbm_reg = GradientBoostingRegressor(n_estimators=500) xgb_reg = XGBRegressor(n_estimators=500) lgbm_reg = LGBMRegressor(n_estimators=500) for model in [rf_reg, gbm_reg, xgb_reg, lgbm_reg]: # XGBoost의 경우 DataFrame이 입력 될 경우 버전에 따라 오류 발생 가능. ndarray로 변환. get_model_predict(model,X_train.values, X_test.values, y_train.values, y_test.values,is_expm1=True) ### RandomForestRegressor ### RMSLE: 0.355, RMSE: 50.447, MAE: 31.270 ### GradientBoostingRegressor ### RMSLE: 0.330, RMSE: 53.336, MAE: 32.746 ### XGBRegressor ### RMSLE: 0.342, RMSE: 51.732, MAE: 31.251 ### LGBMRegressor ### RMSLE: 0.319, RMSE: 47.215, MAE: 29.029 결과 해석 앞의 선형 회귀 모델보다 회귀 예측 성능이 개선됨 단, 회귀 트리가 선형 트리보다 나은 성능을 가진다는 의미가 아님 데이터 세트 유형에 따라 결과는 얼마든지 달라질 수 있음 10. 회귀 실습- 캐글 주택 가격: 고급 회귀 기법 데이터 설명 변수: 79개 미국 아이오와주의 에임스(Ames) 지방 주택 가격 정보- 피처별 설명 확인하기 성능 평가 RMSLE(Root Mean Squared Log Error) 기반 가격이 비싼 주택일수록 예측 결과 오류가 전체 오류에 미치는 비중이 높으므로, 이를 상쇄하기 위해 오류 값을 로그 변환한 RMSLE를 이용 (1) 데이터 사전 처리(Preprocessing)import warnings warnings.filterwarnings('ignore') import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline house_df_org = pd.read_csv('./data/house_price.csv') house_df = house_df_org.copy() house_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice 0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 2 2008 WD Normal 208500 1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 5 2007 WD Normal 181500 2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 9 2008 WD Normal 223500 3 rows × 81 columns # 데이터 세트 전체 크기와 칼럼 타입, Null이 있는 칼럼과 건수를 내림차순으로 출력 print('데이터 세트의 Shape:', house_df.shape) print('\\n전체 feature 들의 type \\n',house_df.dtypes.value_counts()) isnull_series = house_df.isnull().sum() print('\\nNull 컬럼과 그 건수:\\n ', isnull_series[isnull_series > 0].sort_values(ascending=False)) 데이터 세트의 Shape: (1460, 81) 전체 feature 들의 type object 43 int64 35 float64 3 dtype: int64 Null 컬럼과 그 건수: PoolQC 1453 MiscFeature 1406 Alley 1369 Fence 1179 FireplaceQu 690 LotFrontage 259 GarageYrBlt 81 GarageType 81 GarageFinish 81 GarageQual 81 GarageCond 81 BsmtFinType2 38 BsmtExposure 38 BsmtFinType1 37 BsmtCond 37 BsmtQual 37 MasVnrArea 8 MasVnrType 8 Electrical 1 dtype: int64 데이터 타입 확인 테이터 세트는 1460개의 레코드와 81개의 피처로 구성 피처 타입은 숫자형과 문자형 모두 존재 Target을 제외한 80개 피처 중, 43개가 문자형이고 37개가 숫자형 1480개 데이터 중, PoolQC, MiseFeature, Alley, Fence는 1000개가 넘는 Null 값을 가짐 Null 값이 너무 많은 피처는 drop - 회귀 모델 적용 전, 타깃 값 분포가 정규 분포인지 확인하기 아래 그래프에서 볼 수 있듯, 데이터 값 분포가 왼쪽으로 치우친 형태로 정규 분포에서 벗어나 있음 plt.title('Original Sale Price Histogram') sns.distplot(house_df['SalePrice']) &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d017a30310&gt; - 로그 변환(Log Transformation)을 적용하여, 정규 분포가 아닌 결괏값을 정규 분포 형태로 변환하기 Numpy의 log1p()로 로그 변환한 결괏값 기반으로 학습 예측 시에는 결괏값을 expm1()로 환원 plt.title('Log Transformed Sale Price Histogram') log_SalePrice = np.log1p(house_df['SalePrice']) sns.distplot(log_SalePrice, color = 'g') &lt;matplotlib.axes._subplots.AxesSubplot at 0x1d017b1a790&gt; SalePrice를 로그 변환해 정규 분포 형태로 결괏값이 분포함을 확인할 수 있음 - 다음 작업 SalePrice를 로그 변환하고 DataFrame에 반영 Null 값이 많은 피처인 PoolQC, MiseFeature, Alley, Fence, FireplaceQu 삭제 단순 식별자인 Id 삭제 LotFrontage Null 값은 259개로 비교적 많으나, 평균값으로 대체 나머지 피처 Null 값은 많지 않으므로 숫자형의 경우 평균값으로 대체 # SalePrice 로그 변환 original_SalePrice = house_df['SalePrice'] house_df['SalePrice'] = np.log1p(house_df['SalePrice']) # Null 이 너무 많은 컬럼들과 불필요한 컬럼 삭제 house_df.drop(['Id','PoolQC' , 'MiscFeature', 'Alley', 'Fence','FireplaceQu'], axis=1 , inplace=True) # Drop 하지 않는 숫자형 Null컬럼들은 평균값으로 대체 house_df.fillna(house_df.mean(),inplace=True) # Null 값이 있는 피처명과 타입을 추출 null_column_count = house_df.isnull().sum()[house_df.isnull().sum() > 0] # house_df.isnull().sum(): 안 바뀐 Null 값 있는지 확인해보기 print('## Null 피처의 Type :\\n', house_df.dtypes[null_column_count.index]) ## Null 피처의 Type : MasVnrType object BsmtQual object BsmtCond object BsmtExposure object BsmtFinType1 object BsmtFinType2 object Electrical object GarageType object GarageFinish object GarageQual object GarageCond object dtype: object - 문자형 피처는 원-핫 인코딩으로 변환하기 사용 함수: get_dummies() 자동으로 문자열 피처를 원-핫 인코딩으로 변환하면서 Null 값을 ‘None’ 칼럼으로 대체해주어 Null 값을 대체하는 별도의 로직이 필요 없음 원-핫 인코딩을 적용하면 칼럼이 증가하기 때문에, 변환 후 늘어난 칼럼 값까지 확인하기 print('get_dummies() 수행 전 데이터 Shape:', house_df.shape) house_df_ohe = pd.get_dummies(house_df) print('get_dummies() 수행 후 데이터 Shape:', house_df_ohe.shape) null_column_count = house_df_ohe.isnull().sum()[house_df_ohe.isnull().sum() > 0] print('## Null 피처의 Type :\\n', house_df_ohe.dtypes[null_column_count.index]) get_dummies() 수행 전 데이터 Shape: (1460, 75) get_dummies() 수행 후 데이터 Shape: (1460, 271) ## Null 피처의 Type : Series([], dtype: object) 결과 해석 원-핫 인코딩 후 피처가 75개에서 272개로 증가 Null 값을 가진 피처는 없음 (2) 선형 회귀 모델 학습/예측/평가RMSE 평가 함수 생성 def get_rmse(model): pred = model.predict(X_test) mse = mean_squared_error(y_test , pred) rmse = np.sqrt(mse) print('&#123;0&#125; 로그 변환된 RMSE: &#123;1&#125;'.format(model.__class__.__name__,np.round(rmse, 3))) return rmse def get_rmses(models): # 개별 모델 값을 구하는 수식 rmses = [ ] for model in models: rmse = get_rmse(model) rmses.append(rmse) return rmses LinearRegression, Ridge, Lasso 학습, 예측, 평가 from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error y_target = house_df_ohe['SalePrice'] # ''에 대한 타겟과 피처를 추출 X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # LinearRegression, Ridge, Lasso 학습, 예측, 평가 lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge() ridge_reg.fit(X_train, y_train) lasso_reg = Lasso() lasso_reg.fit(X_train, y_train) models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.128 Lasso 로그 변환된 RMSE: 0.176 [0.1318957657915403, 0.1275084633405302, 0.17628250556471403] 회귀 계수값과 컬럼명 시각화를 위해 상위 10개, 하위 10개(-값으로 가장 큰 10개) 회귀 계수값과 컬럼명을 가지는 Series생성 함수. def get_top_bottom_coef(model): # coef_ 속성을 기반으로 Series 객체를 생성. index는 컬럼명. coef = pd.Series(model.coef_, index=X_features.columns) # + 상위 10개 , - 하위 10개 coefficient 추출하여 반환. coef_high = coef.sort_values(ascending=False).head(10) coef_low = coef.sort_values(ascending=False).tail(10) return coef_high, coef_low 인자로 입력되는 여러개의 회귀 모델들에 대한 회귀계수값과 컬럼명 시각화 def visualize_coefficient(models): # 3개 회귀 모델의 시각화를 위해 3개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=3) # 열별로 묶음 fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 회귀 계수 시각화. for i_num, model in enumerate(models): # 상위 10개, 하위 10개 회귀 계수를 구하고, 이를 판다스 concat으로 결합. coef_high, coef_low = get_top_bottom_coef(model) coef_concat = pd.concat( [coef_high , coef_low] ) # 순차적으로 ax subplot에 barchar로 표현. 한 화면에 표현하기 위해 tick label 위치와 font 크기 조정. axs[i_num].set_title(model.__class__.__name__+' Coeffiecents', size=25) axs[i_num].tick_params(axis=\"y\",direction=\"in\", pad=-120) for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=coef_concat.values, y=coef_concat.index , ax=axs[i_num]) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 회귀 계수 시각화. models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) 5 폴드 교차검증으로 모델별로 RMSE와 평균 RMSE출력 from sklearn.model_selection import cross_val_score def get_avg_rmse_cv(models): for model in models: # 분할하지 않고 전체 데이터로 cross_val_score( ) 수행. 모델별 CV RMSE값과 평균 RMSE 출력 rmse_list = np.sqrt(-cross_val_score(model, X_features, y_target, scoring=\"neg_mean_squared_error\", cv = 5)) rmse_avg = np.mean(rmse_list) print('\\n&#123;0&#125; CV RMSE 값 리스트: &#123;1&#125;'.format( model.__class__.__name__, np.round(rmse_list, 3))) print('&#123;0&#125; CV 평균 RMSE 값: &#123;1&#125;'.format( model.__class__.__name__, np.round(rmse_avg, 3))) # 앞 예제에서 학습한 lr_reg, ridge_reg, lasso_reg 모델의 CV RMSE값 출력 models = [lr_reg, ridge_reg, lasso_reg] get_avg_rmse_cv(models) LinearRegression CV RMSE 값 리스트: [0.135 0.165 0.168 0.111 0.198] LinearRegression CV 평균 RMSE 값: 0.155 Ridge CV RMSE 값 리스트: [0.117 0.154 0.142 0.117 0.189] Ridge CV 평균 RMSE 값: 0.144 Lasso CV RMSE 값 리스트: [0.161 0.204 0.177 0.181 0.265] Lasso CV 평균 RMSE 값: 0.198 각 모델들의 alpha값을 변경하면서 하이퍼 파라미터 튜닝 후 다시 학습/예측/평가 from sklearn.model_selection import GridSearchCV def get_best_params(model, params): grid_model = GridSearchCV(model, param_grid=params, scoring='neg_mean_squared_error', cv=5) grid_model.fit(X_features, y_target) rmse = np.sqrt(-1* grid_model.best_score_) print('&#123;0&#125; 5 CV 시 최적 평균 RMSE 값: &#123;1&#125;, 최적 alpha:&#123;2&#125;'.format(model.__class__.__name__, np.round(rmse, 4), grid_model.best_params_)) return grid_model.best_estimator_ ridge_params = &#123; 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] &#125; lasso_params = &#123; 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] &#125; best_rige = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1418, 최적 alpha:&#123;&#39;alpha&#39;: 12&#125; Lasso 5 CV 시 최적 평균 RMSE 값: 0.142, 최적 alpha:&#123;&#39;alpha&#39;: 0.001&#125; # 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=12) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.132 Ridge 로그 변환된 RMSE: 0.124 Lasso 로그 변환된 RMSE: 0.12 숫자 피처들에 대한 데이터 분포 왜곡도 확인 후 높은 왜곡도를 가지는 피처 추출 from scipy.stats import skew # skew: 왜곡 # object가 아닌 숫자형 피쳐의 컬럼 index 객체 추출. features_index = house_df.dtypes[house_df.dtypes != 'object'].index # house_df에 컬럼 index를 [ ]로 입력하면 해당하는 컬럼 데이터 셋 반환. apply lambda로 skew()호출 skew_features = house_df[features_index].apply(lambda x : skew(x)) # skew 정도가 1 이상인 컬럼들만 추출. skew_features_top = skew_features[skew_features > 1] print(skew_features_top.sort_values(ascending=False)) MiscVal 24.451640 PoolArea 14.813135 LotArea 12.195142 3SsnPorch 10.293752 LowQualFinSF 9.002080 KitchenAbvGr 4.483784 BsmtFinSF2 4.250888 ScreenPorch 4.117977 BsmtHalfBath 4.099186 EnclosedPorch 3.086696 MasVnrArea 2.673661 LotFrontage 2.382499 OpenPorchSF 2.361912 BsmtFinSF1 1.683771 WoodDeckSF 1.539792 TotalBsmtSF 1.522688 MSSubClass 1.406210 1stFlrSF 1.375342 GrLivArea 1.365156 dtype: float64 왜곡도가 1인 피처들은 로그 변환 적용하고 다시 하이퍼 파라미터 튜닝 후 재 학습/예측/평가 house_df[skew_features_top.index] = np.log1p(house_df[skew_features_top.index]) # Skew가 높은 피처들을 로그 변환 했으므로 다시 원-핫 인코딩 적용 및 피처/타겟 데이터 셋 생성, house_df_ohe = pd.get_dummies(house_df) y_target = house_df_ohe['SalePrice'] X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) # 피처들을 로그 변환 후 다시 최적 하이퍼 파라미터와 RMSE 출력 ridge_params = &#123; 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] &#125; lasso_params = &#123; 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] &#125; best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1275, 최적 alpha:&#123;&#39;alpha&#39;: 10&#125; Lasso 5 CV 시 최적 평균 RMSE 값: 0.1252, 최적 alpha:&#123;&#39;alpha&#39;: 0.001&#125; # 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=10) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.128 Ridge 로그 변환된 RMSE: 0.122 Lasso 로그 변환된 RMSE: 0.119 이상치 데이터 검출을 위해 주요 피처인 GrLivArea값에 대한 산포도 확인 plt.scatter(x = house_df_org['GrLivArea'], y = house_df_org['SalePrice']) plt.ylabel('SalePrice', fontsize=15) plt.xlabel('GrLivArea', fontsize=15) plt.show() 이상치 데이터 삭제 후 재 학습/예측/평가 # GrLivArea와 SalePrice 모두 로그 변환되었으므로 이를 반영한 조건 생성. cond1 = house_df_ohe['GrLivArea'] > np.log1p(4000) cond2 = house_df_ohe['SalePrice'] &lt; np.log1p(500000) outlier_index = house_df_ohe[cond1 &amp; cond2].index print('아웃라이어 레코드 index :', outlier_index.values) print('아웃라이어 삭제 전 house_df_ohe shape:', house_df_ohe.shape) # DataFrame의 index를 이용하여 아웃라이어 레코드 삭제. house_df_ohe.drop(outlier_index, axis=0, inplace=True) # axis=0: 레코드 레벨 삭제 print('아웃라이어 삭제 후 house_df_ohe shape:', house_df_ohe.shape) 아웃라이어 레코드 index : [ 523 1298] 아웃라이어 삭제 전 house_df_ohe shape: (1460, 271) 아웃라이어 삭제 후 house_df_ohe shape: (1458, 271) y_target = house_df_ohe['SalePrice'] X_features = house_df_ohe.drop('SalePrice',axis=1, inplace=False) X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.2, random_state=156) ridge_params = &#123; 'alpha':[0.05, 0.1, 1, 5, 8, 10, 12, 15, 20] &#125; lasso_params = &#123; 'alpha':[0.001, 0.005, 0.008, 0.05, 0.03, 0.1, 0.5, 1,5, 10] &#125; best_ridge = get_best_params(ridge_reg, ridge_params) best_lasso = get_best_params(lasso_reg, lasso_params) Ridge 5 CV 시 최적 평균 RMSE 값: 0.1125, 최적 alpha:&#123;&#39;alpha&#39;: 8&#125; Lasso 5 CV 시 최적 평균 RMSE 값: 0.1122, 최적 alpha:&#123;&#39;alpha&#39;: 0.001&#125; # 앞의 최적화 alpha값으로 학습데이터로 학습, 테스트 데이터로 예측 및 평가 수행. lr_reg = LinearRegression() lr_reg.fit(X_train, y_train) ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 모든 모델의 RMSE 출력 models = [lr_reg, ridge_reg, lasso_reg] get_rmses(models) # 모든 모델의 회귀 계수 시각화 models = [lr_reg, ridge_reg, lasso_reg] visualize_coefficient(models) LinearRegression 로그 변환된 RMSE: 0.129 Ridge 로그 변환된 RMSE: 0.103 Lasso 로그 변환된 RMSE: 0.1 (3) 회귀 트리 학습/예측/평가XGBoost와 LightGBM 학습/예측/평가 from xgboost import XGBRegressor xgb_params = &#123;'n_estimators':[1000]&#125; xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) # subsample=0.8: 80%만 샘플링 best_xgb = get_best_params(xgb_reg, xgb_params) XGBRegressor 5 CV 시 최적 평균 RMSE 값: 0.1178, 최적 alpha:&#123;&#39;n_estimators&#39;: 1000&#125; from lightgbm import LGBMRegressor lgbm_params = &#123;'n_estimators':[1000]&#125; lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) best_lgbm = get_best_params(lgbm_reg, lgbm_params) LGBMRegressor 5 CV 시 최적 평균 RMSE 값: 0.1163, 최적 alpha:&#123;&#39;n_estimators&#39;: 1000&#125; 트리 회귀 모델의 피처 중요도 시각화 # 모델의 중요도 상위 20개의 피처명과 그때의 중요도값을 Series로 반환. def get_top_features(model): ftr_importances_values = model.feature_importances_ ftr_importances = pd.Series(ftr_importances_values, index=X_features.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] return ftr_top20 def visualize_ftr_importances(models): # 2개 회귀 모델의 시각화를 위해 2개의 컬럼을 가지는 subplot 생성 fig, axs = plt.subplots(figsize=(24,10),nrows=1, ncols=2) fig.tight_layout() # 입력인자로 받은 list객체인 models에서 차례로 model을 추출하여 피처 중요도 시각화. for i_num, model in enumerate(models): # 중요도 상위 20개의 피처명과 그때의 중요도값 추출 ftr_top20 = get_top_features(model) axs[i_num].set_title(model.__class__.__name__+' Feature Importances', size=25) #font 크기 조정. for label in (axs[i_num].get_xticklabels() + axs[i_num].get_yticklabels()): label.set_fontsize(22) sns.barplot(x=ftr_top20.values, y=ftr_top20.index , ax=axs[i_num]) # 앞 예제에서 get_best_params( )가 반환한 GridSearchCV로 최적화된 모델의 피처 중요도 시각화 models = [best_xgb, best_lgbm] visualize_ftr_importances(models) (4) 회귀 모델들의 예측 결과 혼합을 통한 최종 예측def get_rmse_pred(preds): for key in preds.keys(): pred_value = preds[key] mse = mean_squared_error(y_test , pred_value) rmse = np.sqrt(mse) print('&#123;0&#125; 모델의 RMSE: &#123;1&#125;'.format(key, rmse)) # 개별 모델의 학습 ridge_reg = Ridge(alpha=8) ridge_reg.fit(X_train, y_train) lasso_reg = Lasso(alpha=0.001) lasso_reg.fit(X_train, y_train) # 개별 모델 예측 ridge_pred = ridge_reg.predict(X_test) lasso_pred = lasso_reg.predict(X_test) # 개별 모델 예측값 혼합으로 최종 예측값 도출 pred = 0.4 * ridge_pred + 0.6 * lasso_pred preds = &#123;'최종 혼합': pred, 'Ridge': ridge_pred, 'Lasso': lasso_pred&#125; #최종 혼합 모델, 개별모델의 RMSE 값 출력 get_rmse_pred(preds) 최종 혼합 모델의 RMSE: 0.10007930884470519 Ridge 모델의 RMSE: 0.10345177546603257 Lasso 모델의 RMSE: 0.10024170460890033 xgb_reg = XGBRegressor(n_estimators=1000, learning_rate=0.05, colsample_bytree=0.5, subsample=0.8) lgbm_reg = LGBMRegressor(n_estimators=1000, learning_rate=0.05, num_leaves=4, subsample=0.6, colsample_bytree=0.4, reg_lambda=10, n_jobs=-1) xgb_reg.fit(X_train, y_train) lgbm_reg.fit(X_train, y_train) xgb_pred = xgb_reg.predict(X_test) lgbm_pred = lgbm_reg.predict(X_test) pred = 0.5 * xgb_pred + 0.5 * lgbm_pred preds = &#123;'최종 혼합': pred, 'XGBM': xgb_pred, 'LGBM': lgbm_pred&#125; get_rmse_pred(preds) 최종 혼합 모델의 RMSE: 0.1017007808403327 XGBM 모델의 RMSE: 0.10738299364833828 LGBM 모델의 RMSE: 0.10382510019327311 (5) 스태킹 모델을 통한 회귀 예측from sklearn.model_selection import KFold from sklearn.metrics import mean_absolute_error # 개별 기반 모델에서 최종 메타 모델이 사용할 학습 및 테스트용 데이터를 생성하기 위한 함수. def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds ): # 지정된 n_folds값으로 KFold 생성. kf = KFold(n_splits=n_folds, shuffle=False, random_state=0) #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화 train_fold_pred = np.zeros((X_train_n.shape[0] ,1 )) test_pred = np.zeros((X_test_n.shape[0],n_folds)) print(model.__class__.__name__ , ' model 시작 ') for folder_counter , (train_index, valid_index) in enumerate(kf.split(X_train_n)): #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 셋 추출 print('\\t 폴드 세트: ',folder_counter,' 시작 ') X_tr = X_train_n[train_index] y_tr = y_train_n[train_index] X_te = X_train_n[valid_index] #폴드 세트 내부에서 다시 만들어진 학습 데이터로 기반 모델의 학습 수행. model.fit(X_tr , y_tr) #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장. train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) #입력된 원본 테스트 데이터를 폴드 세트내 학습된 기반 모델에서 예측 후 데이터 저장. test_pred[:, folder_counter] = model.predict(X_test_n) # 폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성 test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1) #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터 return train_fold_pred , test_pred_mean 기반 모델은 리지, 라소, XGBoost, LightGBM 으로 만들고 최종 메타 모델은 라소로 생성하여 학습/예측/평가 # get_stacking_base_datasets( )은 넘파이 ndarray를 인자로 사용하므로 DataFrame을 넘파이로 변환. X_train_n = X_train.values X_test_n = X_test.values y_train_n = y_train.values # 각 개별 기반(Base)모델이 생성한 학습용/테스트용 데이터 반환. ridge_train, ridge_test = get_stacking_base_datasets(ridge_reg, X_train_n, y_train_n, X_test_n, 5) lasso_train, lasso_test = get_stacking_base_datasets(lasso_reg, X_train_n, y_train_n, X_test_n, 5) xgb_train, xgb_test = get_stacking_base_datasets(xgb_reg, X_train_n, y_train_n, X_test_n, 5) lgbm_train, lgbm_test = get_stacking_base_datasets(lgbm_reg, X_train_n, y_train_n, X_test_n, 5) Ridge model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 Lasso model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 XGBRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 LGBMRegressor model 시작 폴드 세트: 0 시작 폴드 세트: 1 시작 폴드 세트: 2 시작 폴드 세트: 3 시작 폴드 세트: 4 시작 # 개별 모델이 반환한 학습 및 테스트용 데이터 세트를 Stacking 형태로 결합. Stack_final_X_train = np.concatenate((ridge_train, lasso_train, xgb_train, lgbm_train), axis=1) Stack_final_X_test = np.concatenate((ridge_test, lasso_test, xgb_test, lgbm_test), axis=1) # 최종 메타 모델은 라쏘 모델을 적용. meta_model_lasso = Lasso(alpha=0.0005) #기반 모델의 예측값을 기반으로 새롭게 만들어진 학습 및 테스트용 데이터로 예측하고 RMSE 측정. meta_model_lasso.fit(Stack_final_X_train, y_train) final = meta_model_lasso.predict(Stack_final_X_test) mse = mean_squared_error(y_test , final) rmse = np.sqrt(mse) print('스태킹 회귀 모델의 최종 RMSE 값은:', rmse) 스태킹 회귀 모델의 최종 RMSE 값은: 0.09799154066897717 11. 정리 선형 회귀와 비용 함수 RSS 경사 하강법 다항회귀와 과소적합/과대적합 규제 -L2규제를 적용한 릿지, L1규제를 적용한 라쏘, L1과 L2규제가 결합된 엘라스틱넷 회귀 분류를 위한 로지스틱 회귀 CART 기반의 회귀 트리 왜곡도 개선을 위한 데이터 변환과 원-핫 인코딩 실습 예제를 통한 데이터 정제와 변환 그리고 선형회귀/회귀트리/혼합모델/스태킹 모델 학습/예측/평가비교","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"}],"tags":[{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"회귀","slug":"회귀","permalink":"https://ne-choi.github.io/tags/%ED%9A%8C%EA%B7%80/"},{"name":"선형회귀","slug":"선형회귀","permalink":"https://ne-choi.github.io/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"},{"name":"릿지라쏘엘라스틱넷","slug":"릿지라쏘엘라스틱넷","permalink":"https://ne-choi.github.io/tags/%EB%A6%BF%EC%A7%80%EB%9D%BC%EC%8F%98%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EB%84%B7/"},{"name":"로지스틱회귀","slug":"로지스틱회귀","permalink":"https://ne-choi.github.io/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80/"},{"name":"과적합과소적합","slug":"과적합과소적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%A0%81%ED%95%A9%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/"}],"author":"ne_choi"},{"title":"ch04. 분류","slug":"Study/Python/ML/ch04_분류","date":"2020-12-01T15:00:00.000Z","updated":"2021-01-20T03:58:12.305Z","comments":true,"path":"/2020/12/02/Study/Python/ML/ch04_분류/","link":"","permalink":"https://ne-choi.github.io/2020/12/02/Study/Python/ML/ch04_%EB%B6%84%EB%A5%98/","excerpt":"","text":"해당 자료는 파이썬 머신러닝 완벽가이드 공부를 위한 필사본입니다. Chapter 04. 분류00. 정리결정 트리 대부분의 앙상블 기법은 결정 트리 기반의 다수의 약한 학습기를 결합해 변동성을 줄여 예측 오류를 줄이고 성능을 개선하고 있습니다. 결정 트리 알고리즘은 정보의 균일도에 기반한 규칙 트리를 만들어서 예측을 수행합니다. 결정 트리는 어떻게 예측 결과가 도출되었는지 그 과정을 비교적 쉽게 알 수 있습니다. 결정 트리의 단점은 균일한 최종 예측 결과를 도출하기 위해 결정 트리가 깊어지고 복잡해 지면서 과적합이 쉽게 발생합니다. 배깅 배깅 방식은 학습 데이터를 중복을 허용하면서 다수의 세트로 샘플링하여 이를 다수의 약한 학습기가 학습한 뒤 최종 결과를 결합해 예측하는 방식입니다. 랜덤 포레스트는 수행시간이 빠르고 비교적 안정적인 예측 성능을 제공하는 훌륭한 머신러닝 알고리즘입니다. 부스팅 부스팅은 학습기들이 순차적으로 학습을 진행하면서 예측이 틀린 데이터에 대해서는 가중치를 부여해 다음번 학습기가 학습할 때에는 이전에 예측이 틀린 데이터에 대해서는 보다 높은 정확도로 예측할 수 있도록 해줍니다. GBM은 뛰어난 예측 성능을 가졌지만, 수행 시간이 너무 오래 걸린다는 단점이 있습니다. XGBoost는 많은 캐글 경연대회에서 우승을 위한 알고리즘으로 불리면서 명성을 쌓아 왔습니다. LightGBM은 XGBoost보다 빠른 학습 수행 시간에도 불구하고 XGBoost에 버금가는 예측 성능을 보유하고 있습니다. 01. 분류(Classification)의 개요지도학습은 레이블(Label) -&gt; 기대가 되는 값, 예측되는 값즉, 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식. 즉, 기존 데이터가 어떤 레이블에 속하는지 패턴을 알고리즘으로 인지한 뒤에 새롭게 관측된 데이터에 대한 레이블을 판별하는 것이다. 분류는 다양한 머신러닝 알고리즘으로 구현할 수 있습니다. 나이브 베이즈 : 베이즈 통계와 생성 모델에 기반 로지스틱 회귀 : 독립변수와 종속변수의 선형 관계성에 기반 결정 트리 : 데이터 균일도에 따른 규칙 기반 서포트 벡터 머신 : 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아줌 신경망 : 심층 연결 기반 앙상블 : 서로 같거나 다른 머신러닝 알고리즘을 결합 이번 장에서는 이 다양한 알고리즘 중에서 앙상블 방법을 집중적으로 다루게 됩니다.앙상블은 분류에서 가장 각광을 받는 방법 중 하나이다. 앙상블은 일반적으로 배깅과 부스팅 방식으로 나뉘게 됩니다. 배깅 랜덤 포레스트 : 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등 -&gt; 분석가가 애용하는 알고리즘 부스팅 그래디언트 부스팅 : 뛰어난 예측성능을 가지고 있지만 수행시간이 너무 오래 걸리는 단점으로 최적화 모델 튜닝이 어려웠다. but. XGBoost(eXtra Gradient Boost)와 LightGBM 등 기존 그래디언트 부스팅의 예측 성능을 한 단계 발전과 수행 시간 단축으로 정형 데이터 분류 영역에서 가장 활용도가 높은 알고리즘으로 자리 잡음. 2. 결정 트리1. 결정 트리 모델 특징 데이터에 있는 규칙을 학습을 통해 트리 기반의 분류 규칙을 만든다. 일반적으로 if/else 기반으로 나타내는데 스무고개 게임처럼 if/else를 반복하며 분류 장점 균일도를 기반으로 하기 때무에 쉽고 직관적이다. 룰이 매우 명확하여 규칙노드와 리프 노드가 만들어지는 기준을 파악할 수 있다.→ 정보의 균일도만 신경쓰면 되기 때문에 사적 가공이 많이 필요하지 않다. 단점 과적합으로 알고리즘 성능이 떨어질 수 있다.피처가 많고 균일도가 다양하게 존재할수록 트리의 깊이가 커지고 복잡해질 수 밖에 없다.(학습 데이터를 기반으로 정확도를 올리기 위해 계속 조건을 추가하기 때문에 깊이가 깊어지고 복잡한 모델이되어 새로운 상황에 대한 예측력이 떨어진다.) 결정 트리 구조 루트노드 : 트리 구조가 시작되는 곳 규칙노드 : 규칙조건이 되는 것 리프노드 : 결정된 클래스 값(더 이상 자식 노드가 없는 것) 서브트리 : 새로운 규칙 조건마다 생성 규칙의 기준은 순수도를 가장 높여줄 수 있는 쪽을 선택해 진행한다. – 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요 그림에서 균일한 데이터 세트의 순서는 1 → 2 → 3 이다. 항아리에 10개의 구슬이 들어 있고 그 중 절반가량이 빨간색이고 나머지 절반가량이 파란색인 경우 그 구슬들의 집합은 빨간색과 파란색이 섞여 있어 불순한 것으로 간주한다 (항아리 2). 반면에 항아리에 빨간색 또는 파란색 구슬만 있는 경우 그 구슬 집합은 완벽하게 순수한 것으로 간주한다. 결정노드는 정보 균일도가 높은 데이터를 먼저 선택하도록 규칙을 만든다. 즉 데이터를 나눌 수 있는 조건을 찾아 자식 노드를 만들며 내려가게 된다. 이때 정보의 균일도를 측정하는 대표적인 방법은 엔트로피를 이용한 정보 이득 지수와 지니계수가 있다. 엔트로피, 정보이득지수, 지니 계수 엔트로피 : 주어진 데이터 집합의 혼잡도 (&lt;-&gt;균일도) (값이 작을수록 데이터가 균일) 정보이득지수 : 1- 엔트로피 지수 (정보 이득이 높은 속성을 기준으로 분할) 지니계수: 0이 가장 평등하고 1로 갈수록 불평등하다. (지니 계수가 낮을수록 데이터 균일도가 높다. 지니계수가 낮은 속성을 기준으로 분할) 2. 결정 트리 파라미터 min_samples_split(분리될 노드에 최소 자료 수) 노드를 분할하기 위한 최소한의 샘플 데이터 수 과적합 제어 용도 디폴트 2 (작게 설정할수록 과적합 가능성 증가) min_samples_leaf(잎사귀 노드에 최소 자료 수) 말단 노드가 되기 위한 최소한의 샘플 데이터 수 과적합 제어 용도 비대칭적 데이터(하나의 피처가 과도하게 많은 경우) 에는 특정 클래스의 데이터가 극도로 작을 수 있으므로 작게 설정 max_features 최적의 분할을 위해 고려할 최대 피처 개수 디폴트 = none (데이터 세트의 모든 피처를 사용해 분할) max_features = sqrt/ auto : √(전체 피처 개수) log = log2(전체 피처 개수) 선정 Int형으로 지정하면 대상 피처의 개수, float형으로 지정하면 전체 피처 중 대상 피처의 퍼센트 max _depth 트리의 최대 깊이 디폴트 = None (완벽하게 클래스 결정값이 될 때까지 깊이를 계속 키우며 분할 하거나 노드가 가지는 데이터의 개수가 min_sample_split보다 작아질때까지 계속 깊이를증가 시킨다.) 깊이가 깊어지면 과적합하므로 주의 3. 결정 트리 모델 시각화결정트리는 Graphviz 패키지 이용하여 시각화 할 수 있다. from sklearn.tree import DecisionTreeClassifier from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings('ignore') # DecisionTree Classifier 생성 dt_clf = DecisionTreeClassifier(random_state=156) # 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 셋으로 분리 iris_data = load_iris() X_train , X_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11) # DecisionTreeClassifer 학습. dt_clf.fit(X_train , y_train) DecisionTreeClassifier(random_state=156) test_size = test데이터의 비율 random_state = 난수 값 고정 from sklearn.tree import export_graphviz # export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot 파일을 생성함. export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names , \\ feature_names = iris_data.feature_names, impurity=True, filled=True) import graphviz # 위에서 생성된 tree.dot 파일을 Graphviz 읽어서 Jupyter Notebook상에서 시각화 with open(\"tree.dot\") as f: dot_graph = f.read() graphviz.Source(dot_graph) Impurity가 True일 경우 각 노드의 불순물을 표시한다. filled는 True일 경우 분류를 위한 다수 클래스, 회귀 값의 극한 또는 다중 출력의 노드 순도를 나타내기 위해 노드를 색칠한다. from sklearn.tree import export_graphviz export_graphviz(dt_clf, out_file='tree1.dot', class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=False, filled=True) import graphviz with open('tree1.dot') as f: dot_graph = f.read() graphviz.Source(dot_graph) 두 그림을 비교해보면 impurity=False일때 gini 계수가 사라진 것을 알 수 있다. - Graphviz로 시각화된 결정 트리 지표 설명** 이 그림은 직관적으로 리프 노드와 브랜치 노드를 볼 수 있다. 자식노드가 없는 리프 노드에서 최종적으로 어떤 클래스인지 결정된다. 그 노드에 도달하기까지의 조건을 만족한다면 거기서 이 꽃이 어떤 종류의 붓꽃인지 예측하는 것이다. 1)리프노드가 되는 조건? 최종 데이터가 오직 “하나의” 클래스 값으로 구성 하이퍼 파라미터 조건 충족(뒤에서 자세히 설명) 브랜치 노드 안에는 맨 위와 같이 5개의 지표가 존재하고, 리프 노드에는 주황색 노드와 같이 4개의 지표가 존재한다. 맨 위의 노드 구성을 예시로 설명해보겠다. petal length(꽃잎 길이) ≤ 2.45 자식노드를 만들기 위한 규칙조건 (없으면 리프노드라는 증거!) 꽃잎 길이가 2.45 이하인 데이터와 초과인 데이터로 분류하겠다는 의미 gini = 0.666 지니계수 아래의 value 분포도를 통해 계산 높을수록 데이터 불균일 samples = 120 아직 아무런 조건으로도 나뉘어있지 않은 상황 세 품종 데이터 전체 갯수가 120개 value = [38, 41, 41] 품종 순서대로 Setosa 38개, Versicolor 41개, Virginica41개라는 의미 리스트 안의 값을 모두 더하면 samples의 개수와 같음 class = versicolor 하위 노드를 가질경우 value에서 가장 많은 값의 품종선택 여기서는 versicolor = virginica 41으로 같으므로 인덱스 작은것 선택 노드 색깔이 의미하는 것 붓꽃 데이터의 레이블 값을 의미, 색깔이 짙어질수록 지니계수가 낮아 데이터가 균일하고 해당 레이블에 속하는 샘플이 많다는 의미이다. 0 : Setosa(주황) 1 : Versicolor(초록) 2 : Virginica(보라) 위의 주황색 노드에서 전체 41개의 샘플이 모두 Setosa이므로 매우 균일한 상태라고 볼 수 있다. 2) 하이퍼 파라미터 변경에 따른 트리 변화너무 복잡한 트리가 되면 과적합이 발생하여 오히려 예측성능이 낮아질 수 있다. 이를 제어하는 파라미터를 알아보자. max_depth min_samples_split min_samples_leaf max_depth : 너무 깊어지지 않도록! 적절히 설정하는 것이 중요할 것이다. 너무 간단해도, 너무 복잡해도 성능이 좋지 않을 것이기 때문이다. min_samples_split : 현재 sample 갯수를 보고 자식을 만들지 말지 결정! 기본 설정값은 2이다. 현재 sample이 2개이고, 두 개가 다른 품종이라면 자식노드를 만들어 분할해야한다. 하지만 이 파라미터를 4로 변경하면 샘플이 다른 품종이 섞인 3개여도 분할을 멈추고 리프노드가 된다. 따라서 자연스레 트리 깊이도 줄어든다. min_samples_leaf : sample갯수가 이 값 이하가 되도록 부모 규칙 변경! 리프노드가 될 수 있는 조건은 샘플수의 디폴트가 1이다. 즉, 샘플이 하나 남아야 리프노드로 인정되는 것이다. 그래야 한 품종만 남는다. 하지만 그러면 트리의 리프노드는 너무 많아지고 더 복잡해진다. 따라서 이 파라미터로 리프노드의 엄격했던 기준을 완화시켜주도록 한다. 자식 샘플 갯수가 4여도 리프노드로 만들어줄게! 그러니까 규칙을 좀만 널널하게 해줘~ 이런 식이다. 3) 어떤 속성이 좋은 모델을 만들까?사이킷런에는 규칙을 정하는 데 있어 피처(속성)의 중요한 역할 지표를 DecisionTreeclassifierr 객체의 featureimportances 속성으로 제공한다 반환되는 ndarray값은 피처 순서대로 중요도가 할당되어있다. 막대그래프로 시각화하면 더욱 직관적으로 확인 가능하다. import seaborn as sns import numpy as np %matplotlib inline # feature importance 추출 print(\"Feature importances:/n&#123;0&#125;\". format(np.round(dt_clf.feature_importances_, 3))) # feature별 importance 매핑 for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_): print('&#123;0&#125;:&#123;1:3f&#125;'.format(name, value)) # feature importance를 column 별로 시각화하기 sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names) Feature importances:/n[0.025 0. 0.555 0.42 ] sepal length (cm):0.025005 sepal width (cm):0.000000 petal length (cm):0.554903 petal width (cm):0.420092 &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f25ee2f30f0&gt; 4. 결정 트리 과적합(Overfitting)청일점, 홍일점처럼 일부 이상치 데이터까지 분류하기 위해서 분할이 자주 일어나면 결정 기준 경계도 많아지게 된다. 이렇게 복잡한 모델은 학습 데이터셋의 특성과 약간만 다른형태의 데이터 셋이 들어오면 제대로 예측할 수 없다. from sklearn.datasets import make_classification import matplotlib.pyplot as plt %matplotlib inline plt.title(\"3 Class values with 2 Features Sample data creation\") # 2차원 시각화를 위해서 feature는 2개, 결정값 클래스는 3가지 유형의 classification 샘플 데이터 생성. X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1,random_state=0) # plot 형태로 2개의 feature로 2차원 좌표 시각화, 각 클래스값은 다른 색깔로 표시됨. plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, cmap='rainbow', edgecolor='k') &lt;matplotlib.collections.PathCollection at 0x7f25edd95f60&gt; 과적합 예시 import numpy as np # Classifier의 Decision Boundary를 시각화 하는 함수 def visualize_boundary(model, X, y): fig,ax = plt.subplots() # 학습 데이타 scatter plot으로 나타내기 ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k', clim=(y.min(), y.max()), zorder=3) ax.axis('tight') ax.axis('off') xlim_start , xlim_end = ax.get_xlim() ylim_start , ylim_end = ax.get_ylim() # 호출 파라미터로 들어온 training 데이타로 model 학습 . model.fit(X, y) # meshgrid 형태인 모든 좌표값으로 예측 수행. xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200)) Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) # contourf() 를 이용하여 class boundary 를 visualization 수행. n_classes = len(np.unique(y)) contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap='rainbow', clim=(y.min(), y.max()), zorder=1) from sklearn.tree import DecisionTreeClassifier # 특정한 트리 생성 제약없는 결정 트리의 Decsion Boundary 시각화. dt_clf = DecisionTreeClassifier().fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) # min_samples_leaf=6 으로 트리 생성 조건을 제약한 Decision Boundary 시각화 dt_clf = DecisionTreeClassifier( min_samples_leaf=6).fit(X_features, y_labels) visualize_boundary(dt_clf, X_features, y_labels) 이상치에 크게 반응하지 않으면서 좀 더 일반화된 분류 규칙에 따라 분류됐음을 알 수 있다. 5. 결정 트리 실습 - 사용자 행동 인식 데이터 세트**Ch04-02. 결정트리 실습 - 사용자 행동 인식 데이터 세트 주제 : 결정 트리를 이용해 사용자 행동 인식 데이터 셋에 대한 예측 분류 수행 데이터 셋 : UCI 머신러닝 리포지토리에서 제공, 해당 데이터는 30명에게 스마트폰 센서를 장착한 뒤 사람의 동작과 관련된 여러가지 피처를 수집한 데이터임. from google.colab import drive # 패키지 불러오기 from os.path import join ROOT = \"/content/drive\" # 드라이브 기본 경로 print(ROOT) # print content of ROOT (Optional) drive.mount(ROOT) # 드라이브 기본 경로 Mount MY_GOOGLE_DRIVE_PATH = 'My Drive/human_activity/' # 프로젝트 경로 PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH) # 프로젝트 경로 print(PROJECT_PATH) /content/drive Mounted at /content/drive /content/drive/My Drive/human_activity/ %cd \"&#123;PROJECT_PATH&#125;\" /content/drive/My Drive/human_activity import pandas as pd import matplotlib.pyplot as plt %matplotlib inline # features.txt 파일에는 피처 이름 index와 피처명이 공백으로 분리되어 있음. 이를 DataFrame으로 로드. feature_name_df = pd.read_csv('/content/drive/My Drive/human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name']) # 피처명 index를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출 feature_name = feature_name_df.iloc[:, 1].values.tolist() print('전체 피처명에서 10개만 추출:', feature_name[:10]) 전체 피처명에서 10개만 추출: [&#39;tBodyAcc-mean()-X&#39;, &#39;tBodyAcc-mean()-Y&#39;, &#39;tBodyAcc-mean()-Z&#39;, &#39;tBodyAcc-std()-X&#39;, &#39;tBodyAcc-std()-Y&#39;, &#39;tBodyAcc-std()-Z&#39;, &#39;tBodyAcc-mad()-X&#39;, &#39;tBodyAcc-mad()-Y&#39;, &#39;tBodyAcc-mad()-Z&#39;, &#39;tBodyAcc-max()-X&#39;] # 중복된 피처명 확인 feature_dup_df=feature_name_df.groupby('column_name').count() print(feature_dup_df[feature_dup_df['column_index']>1].count()) feature_dup_df[feature_dup_df['column_index']>1].head() column_index 42 dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; column_index column_name fBodyAcc-bandsEnergy()-1,16 3 fBodyAcc-bandsEnergy()-1,24 3 fBodyAcc-bandsEnergy()-1,8 3 fBodyAcc-bandsEnergy()-17,24 3 fBodyAcc-bandsEnergy()-17,32 3 # 중복된 피처명에 대해서는 원본 피처명에 _1 또는 _2를 추가로 부여해 새로운 피처명을 가지는 DataFrame반환 def get_new_feature_name_df(old_feature_name_df): feature_dup_df=pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt']) feature_dup_df=feature_dup_df.reset_index() new_feature_name_df=pd.merge(old_feature_name_df.reset_index(),feature_dup_df,how='outer') new_feature_name_df['column_name']=new_feature_name_df[['column_name','dup_cnt']].apply(lambda x:x[0]+'_'+str(x[1]) if x[1]>0 else x[0], axis=1) new_feature_name_df=new_feature_name_df.drop(['index'],axis=1) return new_feature_name_df train/test data load import pandas as pd def get_human_dataset(): # 각 데이터 파일은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당. feature_name_df=pd.read_csv('/content/drive/My Drive/human_activity/features.txt', sep='\\s+', header=None, names=['column_index','column_name']) # 중복된 피처명을 수정하는 get_new_feature_name_df()를 이용, 신규 피처명 DataFrame 생성 new_feature_name_df = get_new_feature_name_df(feature_name_df) # DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 변환 feature_name=new_feature_name_df.iloc[:, 1].values.tolist() # train 피처 데이터셋과 test 피처 데이터를 DataFrame으로 로딩. 칼럼명은 feature_name 적용 X_train=pd.read_csv('/content/drive/My Drive/human_activity/train/X_train.txt',sep='\\s+',names=feature_name) X_test=pd.read_csv('/content/drive/My Drive/human_activity/test/X_test.txt',sep='\\s+',names=feature_name) # train label과 test label 데이터를 DataFrame으로 로딩하고 칼럼명은 action으로 부여 y_train=pd.read_csv('/content/drive/My Drive/human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action']) y_test=pd.read_csv('/content/drive/My Drive/human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action']) # 로드된 학습/테스트용 DataFrame을 모두 반환 return X_train, X_test, y_train, y_test X_train, X_test, y_train, y_test = get_human_dataset() print('## 학습 피처 데이터셋 info()') print(X_train.info()) ## 학습 피처 데이터셋 info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 7352 entries, 0 to 7351 Columns: 561 entries, tBodyAcc-mean()-X to angle(Z,gravityMean) dtypes: float64(561) memory usage: 31.5 MB None print(y_train['action'].value_counts()) 6 1407 5 1374 4 1286 1 1226 2 1073 3 986 Name: action, dtype: int64 레이블 값은 1, 2, 3, 4, 5, 6의 6개 값이고 이는 움직임 위치와 관련된 속성이다. 분포도는 특정 값으로 왜곡되지 않고 비교적 고르게 분포되어있다. DecisionTreeClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score import time # 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() # 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정 dt_clf = DecisionTreeClassifier(random_state=156) dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test, pred) print('결정 트리 예측 정확도 : &#123;0:.4f&#125;'.format(accuracy)) # DecisionTreeClassifier의 하이퍼 파라미터 추출 print('DecisionTreeClassifie의 기본 하이퍼 파라미터:\\n', dt_clf.get_params()) print(\"결정 트리 수행 시간: &#123;0:.1f&#125; 초 \".format(time.time() - start_time)) 결정 트리 예측 정확도 : 0.8548 DecisionTreeClassifie의 기본 하이퍼 파라미터: &#123;&#39;ccp_alpha&#39;: 0.0, &#39;class_weight&#39;: None, &#39;criterion&#39;: &#39;gini&#39;, &#39;max_depth&#39;: None, &#39;max_features&#39;: None, &#39;max_leaf_nodes&#39;: None, &#39;min_impurity_decrease&#39;: 0.0, &#39;min_impurity_split&#39;: None, &#39;min_samples_leaf&#39;: 1, &#39;min_samples_split&#39;: 2, &#39;min_weight_fraction_leaf&#39;: 0.0, &#39;presort&#39;: &#39;deprecated&#39;, &#39;random_state&#39;: 156, &#39;splitter&#39;: &#39;best&#39;&#125; 결정 트리 수행 시간: 5.2 초 Tree Depth에 따른 예측 성능 변화아래 예제 테스트 시간이 2분이니 주의 요망 from sklearn.model_selection import GridSearchCV import time # 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() params = &#123; 'max_depth' : [6, 8, 10, 12, 16, 20, 24]&#125; # 5개의 cv세트로 7개의 max_depth 테스트 grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1) grid_cv.fit(X_train, y_train) print('GridSearchCV 최고 평균 정확도 수치 : &#123;0:.4f&#125;'.format(grid_cv.best_score_)) print('GridSearchCV 최적 하이퍼 파라미터: ',grid_cv.best_params_) print(\"GridSearchCV 수행 시간: &#123;0:.1f&#125; 초 \".format(time.time() - start_time)) Fitting 5 folds for each of 7 candidates, totalling 35 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 35 out of 35 | elapsed: 1.9min finished GridSearchCV 최고 평균 정확도 수치 : 0.8513 GridSearchCV 최적 하이퍼 파라미터: &#123;&#39;max_depth&#39;: 16&#125; GridSearchCV 수행 시간: 116.1 초 max_depth가 16일때 5개의 폴드 세트의 최고 평균 정확도가 약 85.13%로 도출되었다. 그렇다면 max_depth값에 따라 어떻게 예측 성능이 변화했는지 GridSearchCV객체의 cvresult 속성을 통해서 살펴보도록 할 것이다. # GridSearchCV객체의 cv_result_ 속성을 DataFrame으로 생성. cv_results_df = pd.DataFrame(grid_cv.cv_results_) # max_depth 파라미터 값과 그때의 테스트 셋의 정확도 수치 추출 cv_results_df[['param_max_depth', 'mean_test_score']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; param_max_depth mean_test_score 0 6 0.850791 1 8 0.851069 2 10 0.851209 3 12 0.844135 4 16 0.851344 5 20 0.850800 6 24 0.849440 import time # 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() max_depths = [ 6, 8 ,10, 12, 16 ,20, 24] # max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정 for depth in max_depths: dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156) dt_clf.fit(X_train , y_train) pred = dt_clf.predict(X_test) accuracy = accuracy_score(y_test , pred) print('max_depth = &#123;0&#125; 정확도: &#123;1:.4f&#125;'.format(depth , accuracy)) print(\"수행 시간: &#123;0:.1f&#125; 초 \".format(time.time() - start_time)) max_depth = 6 정확도: 0.8558 max_depth = 8 정확도: 0.8707 max_depth = 10 정확도: 0.8673 max_depth = 12 정확도: 0.8646 max_depth = 16 정확도: 0.8575 max_depth = 20 정확도: 0.8548 max_depth = 24 정확도: 0.8548 # max_depth와 min_samples_split을 같이 변경하면서 성능 튜닝 import time # 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() params = &#123; 'max_depth' : [8, 12, 16, 20], 'min_samples_split' : [16, 24] &#125; grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1) grid_cv.fit(X_train, y_train) print('GridSearchCV 최고 평균 정확도 수치 : &#123;0:.4f&#125;'.format(grid_cv.best_score_)) print('GridSearchCV 최적 하이퍼 파라미터: ',grid_cv.best_params_) print(\"GridSearchCV 수행 시간: &#123;0:.1f&#125; 초 \".format(time.time() - start_time)) Fitting 5 folds for each of 8 candidates, totalling 40 fits [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. [Parallel(n_jobs=1)]: Done 40 out of 40 | elapsed: 2.2min finished GridSearchCV 최고 평균 정확도 수치 : 0.8549 GridSearchCV 최적 하이퍼 파라미터: &#123;&#39;max_depth&#39;: 8, &#39;min_samples_split&#39;: 16&#125; best_df_clf = grid_cv.best_estimator_ pred1 = best_df_clf.predict(X_test) accuracy = accuracy_score(y_test , pred1) print('결정 트리 예측 정확도:&#123;0:.4f&#125;'.format(accuracy)) 결정 트리 예측 정확도:0.8717 import seaborn as sns ftr_importances_values = best_df_clf.feature_importances_ # Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환 ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns ) # 중요도값 순으로 Series를 정렬 ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title('Feature importances Top 20') sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() 03. 앙상블 학습1. 앙상블 학습 개요 Ensemble Learning : 여러 개의 분류기(Classifier)를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도축하는 기법 마치 집단 지성으로 어려운 문제를 쉽게 해결하는 것처럼 목표: 다양한 분류기의 예측 결과를 결합 -&gt; 단일 분류기보다 신뢰성이 높은 예측값 얻는 것 대부분의 정형 데이터 분류 시 뛰어난 성능 보임 (이미지, 영상, 음성 등의 비정형 데이터의 분류는 딥러닝이 더 뛰어난 성능) 대표적인 앙상블 알고리즘: Random Forest, Gradient Boosting 기존의 Gradient Boosting을 뛰어넘는 새로운 알고리즘 개발 XGBoost LightGBM (XGBoost보다 훨씬 빠른 수행 속도) Stacking (여러 가지 모델의 결과를 기반으로 메타 모델을 수립) Ensemble Learning 유형: 전통적으로 Voting, Bagging, Boosting 3가지 + Stacking을 포함한 다양한 앙상블 방법 Bagging, Boosting은 결정 트리 알고리즘 기반, Voting과 Stacking은 서로 다른 알고리즘을 기반(Ensemble의 한 개념) Voting과 Bagging : 여러 개의 분류기가 투표를 통해 최종 예측 결과를 결정하는 방식 Voting은 서로 다른 알고리즘을 가진 분류기 결합, Bagging은 각각의 분류기가 모두 같은 유형의 알고리즘 기반이지만, 데이터 샘플링을 서로 다르게 가져가면서 학습을 수행해 Voting을 수행하는 것이다. (대표적인 Bagging: Random Forest) Voting : 다른 ML 알고리즘이 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고 보팅을 통해 최종 예측 결과 선정 Bagging : 단일 ML 알고리즘이 Bootstrapping 방식으로 샘플링된 데이터 세트에 대해서 학습을 통해 개별적인 예측을 수행한 결과를 보팅을 통해 최종 예측 결과 선정 Bootstrapping 분할 방식: 개별 분류기에게 데이터를 샘플링해서 추출하는 방식, 여러 개의 데이터 세트를 중첩되게 분리하는 것 (Voting 방식과 다름) 교차 검증이 데이터 세트 간에 중첩 허용하지 않는 것과 다르게, Bagging은 중첩 허용 5 size만큼 Bootstrap 실행 (중복 허용, 복원 추출 개념) 각 분류기 k개만큼 데이터를 샘플링 -&gt; 개별적인 예측을 보팅(결과 값 평균)을 통해 최종 예측 결과 선정 OOB error(Out-of-Bag error): 학습데이터에서 미 추출된 데이터에 대해 각각의 분류기가 예측하고 Error율 계산해서 평균 냄 -&gt; 학습데이터 내에서 미 추출된 데이터를 검증 데이터로 써서 검증데이터에 대한 성능지표를 계산할 수 있는 장점 Bagging과 Tree의 차이점 Tree: 쉽고 직관적인 분류 기준을 가지고 있지만, Low Bias(정답과 예측값의 거리), High Variance(모델별 예측값 간의 거리) =&gt; overfitting 발생 Bagging: 위와 같은 문제를 해결하기 위해 모델이 예측한 값의 평균을 사용하여 bias를 유지하고 Variance를 감소, 학습데이터의 noise에 강건해짐, 모형해석의 어려움(단점) 결정 트리 알고리즘의 장점은 그대로 취하고 단점은 보완하면서 bias-variance trade-off의 효과를 극대화할 수 있음 Boosting : 여러 개의 분류기가 순차적으로 학습을 수행하되, 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서 올바르게 예측할 수 있도록 다음 분류기에게 가중치(weight)를 부여하면서 학습과 예측 진행 Stacking : 여러 가지 다른 모델의 예측 결괏값을 다시 학습 데이터로 만들어서 다른 모델(메타 모델)로 재학습시켜 결과를 예측 2. 보팅 유형: 하드 보팅(Hard Voting), 소프트 보팅(Soft Voting)** 하드 보팅 : 예측한 결괏값들 중 다수의 분류기가 결정한 예측값을 최종 보팅 결괏값으로 선정 (다수결 원칙과 비슷) 소프트 보팅 : 분류기들의 레이블 값 결정 확률을 모두 더하고 이를 평균해서 이들 중 확률이 가장 높은 레이블 값을 최종 보팅 결괏값으로 선정 (일반적인 보팅 방법) 일반적으로 하드 보팅보다는 소프트 보팅이 예측 성능이 좋아서 더 많이 사용됨. 3. 보팅 분류기(Voting Classifier) 사이킷런은 보팅 방식의 앙상블을 구현한 보팅 분류기 클래스를 제공하고 있다. 암 데이터로 위스콘신 데이터 세트를 생성 import pandas as pd from sklearn.ensemble import VotingClassifier from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score cancer = load_breast_cancer() data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names) data_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension radius error texture error perimeter error area error smoothness error compactness error concavity error concave points error symmetry error fractal dimension error worst radius worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension 0 17.99 10.38 122.8 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 1.0950 0.9053 8.589 153.40 0.006399 0.04904 0.05373 0.01587 0.03003 0.006193 25.38 17.33 184.6 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 1 20.57 17.77 132.9 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 0.7339 3.398 74.08 0.005225 0.01308 0.01860 0.01340 0.01389 0.003532 24.99 23.41 158.8 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 2 19.69 21.25 130.0 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 0.7456 0.7869 4.585 94.03 0.006150 0.04006 0.03832 0.02058 0.02250 0.004571 23.57 25.53 152.5 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 로지스틱 회귀와 KNN을 기반하여 소프트 보팅 방식으로 새롭게 보팅 분류기를 만들어 보았다. # 개별 모델은 로지스틱 회귀와 KNN 임. lr_clf = LogisticRegression() knn_clf = KNeighborsClassifier(n_neighbors=8) # 개별 모델을 소프트 보팅 기반의 앙상블 모델로 구현한 분류기 vo_clf = VotingClassifier( estimators=[('LR',lr_clf),('KNN',knn_clf)] , voting='soft' ) X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2 , random_state= 156) # VotingClassifier 학습/예측/평가. vo_clf.fit(X_train , y_train) pred = vo_clf.predict(X_test) print('Voting 분류기 정확도: &#123;0:.4f&#125;'.format(accuracy_score(y_test , pred))) # 개별 모델의 학습/예측/평가. classifiers = [lr_clf, knn_clf] for classifier in classifiers: classifier.fit(X_train , y_train) pred = classifier.predict(X_test) class_name= classifier.__class__.__name__ print('&#123;0&#125; 정확도: &#123;1:.4f&#125;'.format(class_name, accuracy_score(y_test , pred))) Voting 분류기 정확도: 0.9474 LogisticRegression 정확도: 0.9386 KNeighborsClassifier 정확도: 0.9386 04. 랜덤 포레스트1. 랜덤 포레스트의 개요 및 실습 Bagging의 대표적인 알고리즘 Random Forest; 비교적 빠른 수행 속도 + 높은 예측 성능 여러 개의 결정 트리 분류기가 전체 데이터에서 bagging 방식으로 각자의 데이터를 샘플링 -&gt; 개별적으로 학습 수행 후 최종적으로 모든 분류기가 voting을 통해 예측 결정 (X+Y)의 분산이 X와 Y 각각의 분산을 더한 것보다 더 크기 때문에 Bagging Model 자체의 분산이 커질 수 있음 공분산이 0이면, 두 변수는 서로 독립적인 관계 따라서 Random Forest는 기본 Bagging과 다르게 데이터뿐만 아니라, 변수도 random하게 뽑아서 다양한 모델 만듦 (각 분류기간의 공분산을 줄이는 게 목표) 모델의 분산을 줄여 일반적으로 Bagging보다 성능이 좋음 뽑을 변수의 수는 hyper parameter(일반적으로 √p 사용, p는 변수 개수) 개별적인 분류기의 기반 알고리즘은 결정트리이지만, 개별 트리가 학습하는 데이터세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트 (Bootstrapping 방식) Bagging: bootstrap aggregating의 줄임말 랜덤 포레스트의 Subset 데이터는 Bootstrapping 방식으로 데이터가 임의로 만들어짐 Subset 데이터의 건수는 전체 데이터의 건수와 동일하지만, 개별 데이터가 중첩되어 만들어짐 데이터가 중첩된 개별 데이터 세트에 결정 트리 분류기를 각각 적용 ⇒ 랜덤 포레스트! 정리배깅: 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘. 배깅의 대표적인 알고리즘은 랜덤포레스트 랜덤포레스트의 장점 : 1. 앙상블 알고리즘 중 비교적 빠른 수행 속도를 가지고 있음 다양한 영역에서 높은 예측 성능 결정 트리의 쉽고 직관적인 장점 그대로 가지고 있음 2. 랜덤 포레스트 하이퍼 파라미터 및 튜닝하이퍼 파라미터란, 일반적으로 머신러닝에서 어떠한 임의의 모델을 학습시킬때, 사람이 직접 튜닝 (설정) 해주어야하는 변수를 말한다. RandomForest의 단점: 하이퍼 파라미터가 너무 많다 시간이 많이 소모된다 예측 성능이 크게 향상되는 경우가 많지 않다 트리 기반 자체의 하이퍼 파라미터가 원래 많고, 배깅, 부스팅, 학습, 정규화를 위한 하이퍼 파라미터까지 추가되므로 많을 수 밖에 없다. 코드: n_estimators: 결정 트리의 개수 지정/ max_features는 결정트리에 max_features 파라미터와 같음(최적의 분할을 위해 고려할 최대 피처갯수). 기본이 sqrt(전체 피처갯수) / max_depth(트리의 최대 깊이 규정), min_samples_leaf(말단 노드가 되기 위한 최소한의 샘플 데이터 수): 과적합 개선 아래 예제는 5분 이상 소요 주의 요망 # GridSearchCV 로 교차검증 및 하이퍼 파라미터 튜닝 from sklearn.model_selection import GridSearchCV params = &#123; 'n_estimators':[100], 'max_depth' : [6, 8, 10, 12], 'min_samples_leaf' : [8, 12, 18 ], 'min_samples_split' : [8, 16, 20] &#125; # RandomForestClassifier 객체 생성 후 GridSearchCV 수행 rf_clf = RandomForestClassifier(random_state=0, n_jobs=-1) grid_cv = GridSearchCV(rf_clf , param_grid=params , cv=2, n_jobs=-1 ) grid_cv.fit(X_train , y_train) print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_) print('최고 예측 정확도: &#123;0:.4f&#125;'.format(grid_cv.best_score_)) # 튜닝된 하이퍼 파라미터로 재 학습 및 예측/평가 rf_clf1 = RandomForestClassifier(n_estimators=300, max_depth=10, min_samples_leaf=8, \\ min_samples_split=8, random_state=0) rf_clf1.fit(X_train , y_train) pred = rf_clf1.predict(X_test) print('예측 정확도: &#123;0:.4f&#125;'.format(accuracy_score(y_test , pred))) # 개별 feature들의 중요도 시각화 import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline ftr_importances_values = rf_clf1.feature_importances_ ftr_importances = pd.Series(ftr_importances_values,index=X_train.columns ) ftr_top20 = ftr_importances.sort_values(ascending=False)[:20] plt.figure(figsize=(8,6)) plt.title('Feature importances Top 20') sns.barplot(x=ftr_top20 , y = ftr_top20.index) plt.show() 05. GBM(Gradient Boosting Machine)1. GBM의 개요 및 실습부스팅 알고리즘: 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 하고, 오류를 개선해 나가면서 학습하는 방식 에이다 부스트, GBM 과의 차이: GBM은 가중치 업데이트를 경사하강법을 이용한다.경사하강법은 “반복 수행을 통해 오류를 최소화할 수 있도록 가중치의 업데이트 값을 도출하는 기법” 정도로만 이해하고 5장(회귀)에서 더 알아보도록 한다. 경사하강법 보통 GBM이 랜덤 포레스트 보다는 예측 성능이 뛰어나다.BUT, 수행시간 문제는 GBM이 극복해야할 중요한 과제. 아래 코드 수행시간 3분 이상 from sklearn.ensemble import GradientBoostingClassifier import time import warnings warnings.filterwarnings('ignore') X_train, X_test, y_train, y_test = get_human_dataset() # GBM 수행 시간 측정을 위함. 시작 시간 설정. start_time = time.time() gb_clf = GradientBoostingClassifier(random_state=0) gb_clf.fit(X_train , y_train) gb_pred = gb_clf.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print('GBM 정확도: &#123;0:.4f&#125;'.format(gb_accuracy)) print(\"GBM 수행 시간: &#123;0:.1f&#125; 초 \".format(time.time() - start_time)) 2. GBM 하이퍼 파라미터 및 튜닝n_estimators: 결정트리갯수/ max_depth:. Max_features: 위와 같이 loss: 경사 하강법에서 사용할 비용 함수 지정 learning_rate: weak learner가 순차적으로 오류값을 보정해 나가는데 적용하는 계수. 범위는 0과 1사이 , 기본값은 0.1, 너무 작은 값: 예측성능은 높아지지만 속도 느림. 너무 큰 값: 예측성능이 떨어지지만 속도는 빠름/ subsample: 학 습에 사용하는 데이터 샘플링 비율 (ex. 0.5 면 학습데이터 50%) 장점: 과적합에도 강한 뛰어난 예측 성능을 가진 알고리즘 단점: 수행시간이 오래걸림 아래 코드 수행시간 1시간 이상 from sklearn.model_selection import GridSearchCV params = &#123; 'n_estimators':[100, 500], 'learning_rate' : [ 0.05, 0.1] &#125; grid_cv = GridSearchCV(gb_clf , param_grid=params , cv=2 ,verbose=1) grid_cv.fit(X_train , y_train) print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_) print('최고 예측 정확도: &#123;0:.4f&#125;'.format(grid_cv.best_score_)) # GridSearchCV를 이용하여 최적으로 학습된 estimator로 predict 수행. gb_pred = grid_cv.best_estimator_.predict(X_test) gb_accuracy = accuracy_score(y_test, gb_pred) print('GBM 정확도: &#123;0:.4f&#125;'.format(gb_accuracy)) 06. XGBoost(eXtra Gradient Boost)1. XGBoost 개요트리 기반의 앙상블 학습 import xgboost print(xgboost.__version__) 1.2.1 import xgboost as xgb from xgboost import plot_importance import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split import warnings warnings.filterwarnings('ignore') dataset = load_breast_cancer() X_features= dataset.data y_label = dataset.target cancer_df = pd.DataFrame(data=X_features, columns=dataset.feature_names) cancer_df['target']= y_label cancer_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; mean radius mean texture mean perimeter mean area mean smoothness mean compactness mean concavity mean concave points mean symmetry mean fractal dimension ... worst texture worst perimeter worst area worst smoothness worst compactness worst concavity worst concave points worst symmetry worst fractal dimension target 0 17.99 10.38 122.8 1001.0 0.11840 0.27760 0.3001 0.14710 0.2419 0.07871 ... 17.33 184.6 2019.0 0.1622 0.6656 0.7119 0.2654 0.4601 0.11890 0 1 20.57 17.77 132.9 1326.0 0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 ... 23.41 158.8 1956.0 0.1238 0.1866 0.2416 0.1860 0.2750 0.08902 0 2 19.69 21.25 130.0 1203.0 0.10960 0.15990 0.1974 0.12790 0.2069 0.05999 ... 25.53 152.5 1709.0 0.1444 0.4245 0.4504 0.2430 0.3613 0.08758 0 3 rows × 31 columns print(dataset.target_names) print(cancer_df['target'].value_counts()) [&#39;malignant&#39; &#39;benign&#39;] 1 357 0 212 Name: target, dtype: int64 # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(X_features, y_label, test_size=0.2, random_state=156 ) print(X_train.shape , X_test.shape) (455, 30) (114, 30) dtrain = xgb.DMatrix(data=X_train , label=y_train) dtest = xgb.DMatrix(data=X_test , label=y_test) params = &#123; 'max_depth':3, 'eta': 0.1, 'objective':'binary:logistic', 'eval_metric':'logloss', 'early_stoppings':100 &#125; num_rounds = 400 # train 데이터 셋은 ‘train’ , evaluation(test) 데이터 셋은 ‘eval’ 로 명기합니다. wlist = [(dtrain,'train'),(dtest,'eval') ] # 하이퍼 파라미터와 early stopping 파라미터를 train( ) 함수의 파라미터로 전달 xgb_model = xgb.train(params = params , dtrain=dtrain , num_boost_round=num_rounds , evals=wlist ) [10:03:48] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: Parameters: &#123; early_stoppings &#125; might not be used. This may not be accurate due to some parameters are only used in language bindings but passed down to XGBoost core. Or some parameters are not used but slip through this verification. Please open an issue if you find above cases. [0] train-logloss:0.60969 eval-logloss:0.61352 [1] train-logloss:0.54080 eval-logloss:0.54784 [2] train-logloss:0.48375 eval-logloss:0.49425 [3] train-logloss:0.43446 eval-logloss:0.44799 [4] train-logloss:0.39055 eval-logloss:0.40911 [5] train-logloss:0.35414 eval-logloss:0.37498 [6] train-logloss:0.32122 eval-logloss:0.34571 [7] train-logloss:0.29259 eval-logloss:0.32053 [8] train-logloss:0.26747 eval-logloss:0.29721 [9] train-logloss:0.24515 eval-logloss:0.27799 [10] train-logloss:0.22569 eval-logloss:0.26030 [11] train-logloss:0.20794 eval-logloss:0.24604 [12] train-logloss:0.19218 eval-logloss:0.23156 [13] train-logloss:0.17792 eval-logloss:0.22005 [14] train-logloss:0.16522 eval-logloss:0.20857 [15] train-logloss:0.15362 eval-logloss:0.19999 [16] train-logloss:0.14333 eval-logloss:0.19012 [17] train-logloss:0.13398 eval-logloss:0.18182 [18] train-logloss:0.12560 eval-logloss:0.17473 [19] train-logloss:0.11729 eval-logloss:0.16766 [20] train-logloss:0.10969 eval-logloss:0.15820 [21] train-logloss:0.10297 eval-logloss:0.15473 [22] train-logloss:0.09707 eval-logloss:0.14895 [23] train-logloss:0.09143 eval-logloss:0.14331 [24] train-logloss:0.08633 eval-logloss:0.13634 [25] train-logloss:0.08131 eval-logloss:0.13278 [26] train-logloss:0.07686 eval-logloss:0.12791 [27] train-logloss:0.07284 eval-logloss:0.12526 [28] train-logloss:0.06925 eval-logloss:0.11998 [29] train-logloss:0.06555 eval-logloss:0.11641 [30] train-logloss:0.06241 eval-logloss:0.11450 [31] train-logloss:0.05959 eval-logloss:0.11257 [32] train-logloss:0.05710 eval-logloss:0.11154 [33] train-logloss:0.05441 eval-logloss:0.10868 [34] train-logloss:0.05204 eval-logloss:0.10668 [35] train-logloss:0.04975 eval-logloss:0.10421 [36] train-logloss:0.04775 eval-logloss:0.10296 [37] train-logloss:0.04585 eval-logloss:0.10058 [38] train-logloss:0.04401 eval-logloss:0.09868 [39] train-logloss:0.04226 eval-logloss:0.09644 [40] train-logloss:0.04065 eval-logloss:0.09587 [41] train-logloss:0.03913 eval-logloss:0.09424 [42] train-logloss:0.03738 eval-logloss:0.09471 [43] train-logloss:0.03611 eval-logloss:0.09427 [44] train-logloss:0.03494 eval-logloss:0.09389 [45] train-logloss:0.03365 eval-logloss:0.09418 [46] train-logloss:0.03253 eval-logloss:0.09402 [47] train-logloss:0.03148 eval-logloss:0.09236 [48] train-logloss:0.03039 eval-logloss:0.09301 [49] train-logloss:0.02947 eval-logloss:0.09127 [50] train-logloss:0.02855 eval-logloss:0.09005 [51] train-logloss:0.02753 eval-logloss:0.08961 [52] train-logloss:0.02655 eval-logloss:0.08958 [53] train-logloss:0.02568 eval-logloss:0.09070 [54] train-logloss:0.02500 eval-logloss:0.08958 [55] train-logloss:0.02430 eval-logloss:0.09036 [56] train-logloss:0.02357 eval-logloss:0.09159 [57] train-logloss:0.02296 eval-logloss:0.09153 [58] train-logloss:0.02249 eval-logloss:0.09199 [59] train-logloss:0.02185 eval-logloss:0.09195 [60] train-logloss:0.02132 eval-logloss:0.09194 [61] train-logloss:0.02079 eval-logloss:0.09146 [62] train-logloss:0.02022 eval-logloss:0.09031 [63] train-logloss:0.01970 eval-logloss:0.08941 [64] train-logloss:0.01918 eval-logloss:0.08972 [65] train-logloss:0.01872 eval-logloss:0.08974 [66] train-logloss:0.01833 eval-logloss:0.08962 [67] train-logloss:0.01787 eval-logloss:0.08873 [68] train-logloss:0.01760 eval-logloss:0.08862 [69] train-logloss:0.01724 eval-logloss:0.08974 [70] train-logloss:0.01688 eval-logloss:0.08998 [71] train-logloss:0.01664 eval-logloss:0.08978 [72] train-logloss:0.01629 eval-logloss:0.08958 [73] train-logloss:0.01598 eval-logloss:0.08953 [74] train-logloss:0.01566 eval-logloss:0.08875 [75] train-logloss:0.01539 eval-logloss:0.08860 [76] train-logloss:0.01515 eval-logloss:0.08812 [77] train-logloss:0.01488 eval-logloss:0.08840 [78] train-logloss:0.01464 eval-logloss:0.08874 [79] train-logloss:0.01449 eval-logloss:0.08815 [80] train-logloss:0.01418 eval-logloss:0.08758 [81] train-logloss:0.01401 eval-logloss:0.08741 [82] train-logloss:0.01377 eval-logloss:0.08849 [83] train-logloss:0.01357 eval-logloss:0.08858 [84] train-logloss:0.01341 eval-logloss:0.08807 [85] train-logloss:0.01325 eval-logloss:0.08764 [86] train-logloss:0.01311 eval-logloss:0.08742 [87] train-logloss:0.01293 eval-logloss:0.08761 [88] train-logloss:0.01271 eval-logloss:0.08707 [89] train-logloss:0.01254 eval-logloss:0.08727 [90] train-logloss:0.01235 eval-logloss:0.08716 [91] train-logloss:0.01223 eval-logloss:0.08696 [92] train-logloss:0.01206 eval-logloss:0.08717 [93] train-logloss:0.01193 eval-logloss:0.08707 [94] train-logloss:0.01182 eval-logloss:0.08659 [95] train-logloss:0.01165 eval-logloss:0.08612 [96] train-logloss:0.01148 eval-logloss:0.08714 [97] train-logloss:0.01136 eval-logloss:0.08677 [98] train-logloss:0.01124 eval-logloss:0.08669 [99] train-logloss:0.01113 eval-logloss:0.08655 [100] train-logloss:0.01100 eval-logloss:0.08650 [101] train-logloss:0.01085 eval-logloss:0.08641 [102] train-logloss:0.01076 eval-logloss:0.08629 [103] train-logloss:0.01064 eval-logloss:0.08626 [104] train-logloss:0.01050 eval-logloss:0.08683 [105] train-logloss:0.01039 eval-logloss:0.08677 [106] train-logloss:0.01030 eval-logloss:0.08732 [107] train-logloss:0.01020 eval-logloss:0.08730 [108] train-logloss:0.01007 eval-logloss:0.08728 [109] train-logloss:0.01000 eval-logloss:0.08730 [110] train-logloss:0.00991 eval-logloss:0.08729 [111] train-logloss:0.00980 eval-logloss:0.08800 [112] train-logloss:0.00971 eval-logloss:0.08794 [113] train-logloss:0.00963 eval-logloss:0.08784 [114] train-logloss:0.00956 eval-logloss:0.08807 [115] train-logloss:0.00948 eval-logloss:0.08765 [116] train-logloss:0.00942 eval-logloss:0.08730 [117] train-logloss:0.00931 eval-logloss:0.08780 [118] train-logloss:0.00923 eval-logloss:0.08775 [119] train-logloss:0.00915 eval-logloss:0.08768 [120] train-logloss:0.00912 eval-logloss:0.08763 [121] train-logloss:0.00902 eval-logloss:0.08757 [122] train-logloss:0.00897 eval-logloss:0.08755 [123] train-logloss:0.00890 eval-logloss:0.08716 [124] train-logloss:0.00884 eval-logloss:0.08767 [125] train-logloss:0.00880 eval-logloss:0.08774 [126] train-logloss:0.00871 eval-logloss:0.08828 [127] train-logloss:0.00864 eval-logloss:0.08831 [128] train-logloss:0.00861 eval-logloss:0.08827 [129] train-logloss:0.00856 eval-logloss:0.08789 [130] train-logloss:0.00846 eval-logloss:0.08886 [131] train-logloss:0.00842 eval-logloss:0.08868 [132] train-logloss:0.00839 eval-logloss:0.08874 [133] train-logloss:0.00830 eval-logloss:0.08922 [134] train-logloss:0.00827 eval-logloss:0.08918 [135] train-logloss:0.00822 eval-logloss:0.08882 [136] train-logloss:0.00816 eval-logloss:0.08851 [137] train-logloss:0.00808 eval-logloss:0.08848 [138] train-logloss:0.00805 eval-logloss:0.08839 [139] train-logloss:0.00797 eval-logloss:0.08915 [140] train-logloss:0.00795 eval-logloss:0.08911 [141] train-logloss:0.00790 eval-logloss:0.08876 [142] train-logloss:0.00787 eval-logloss:0.08868 [143] train-logloss:0.00785 eval-logloss:0.08839 [144] train-logloss:0.00778 eval-logloss:0.08927 [145] train-logloss:0.00775 eval-logloss:0.08924 [146] train-logloss:0.00773 eval-logloss:0.08914 [147] train-logloss:0.00769 eval-logloss:0.08891 [148] train-logloss:0.00762 eval-logloss:0.08942 [149] train-logloss:0.00760 eval-logloss:0.08939 [150] train-logloss:0.00758 eval-logloss:0.08911 [151] train-logloss:0.00752 eval-logloss:0.08873 [152] train-logloss:0.00750 eval-logloss:0.08872 [153] train-logloss:0.00746 eval-logloss:0.08848 [154] train-logloss:0.00741 eval-logloss:0.08847 [155] train-logloss:0.00739 eval-logloss:0.08854 [156] train-logloss:0.00737 eval-logloss:0.08852 [157] train-logloss:0.00734 eval-logloss:0.08855 [158] train-logloss:0.00732 eval-logloss:0.08828 [159] train-logloss:0.00730 eval-logloss:0.08830 [160] train-logloss:0.00728 eval-logloss:0.08828 [161] train-logloss:0.00726 eval-logloss:0.08801 [162] train-logloss:0.00724 eval-logloss:0.08776 [163] train-logloss:0.00722 eval-logloss:0.08778 [164] train-logloss:0.00720 eval-logloss:0.08778 [165] train-logloss:0.00718 eval-logloss:0.08752 [166] train-logloss:0.00716 eval-logloss:0.08754 [167] train-logloss:0.00714 eval-logloss:0.08764 [168] train-logloss:0.00712 eval-logloss:0.08739 [169] train-logloss:0.00710 eval-logloss:0.08738 [170] train-logloss:0.00708 eval-logloss:0.08730 [171] train-logloss:0.00707 eval-logloss:0.08737 [172] train-logloss:0.00705 eval-logloss:0.08740 [173] train-logloss:0.00703 eval-logloss:0.08739 [174] train-logloss:0.00701 eval-logloss:0.08713 [175] train-logloss:0.00699 eval-logloss:0.08716 [176] train-logloss:0.00697 eval-logloss:0.08696 [177] train-logloss:0.00696 eval-logloss:0.08705 [178] train-logloss:0.00694 eval-logloss:0.08697 [179] train-logloss:0.00692 eval-logloss:0.08697 [180] train-logloss:0.00690 eval-logloss:0.08704 [181] train-logloss:0.00688 eval-logloss:0.08680 [182] train-logloss:0.00687 eval-logloss:0.08683 [183] train-logloss:0.00685 eval-logloss:0.08658 [184] train-logloss:0.00683 eval-logloss:0.08659 [185] train-logloss:0.00681 eval-logloss:0.08661 [186] train-logloss:0.00680 eval-logloss:0.08637 [187] train-logloss:0.00678 eval-logloss:0.08637 [188] train-logloss:0.00676 eval-logloss:0.08630 [189] train-logloss:0.00675 eval-logloss:0.08610 [190] train-logloss:0.00673 eval-logloss:0.08602 [191] train-logloss:0.00671 eval-logloss:0.08605 [192] train-logloss:0.00670 eval-logloss:0.08615 [193] train-logloss:0.00668 eval-logloss:0.08592 [194] train-logloss:0.00667 eval-logloss:0.08592 [195] train-logloss:0.00665 eval-logloss:0.08598 [196] train-logloss:0.00663 eval-logloss:0.08601 [197] train-logloss:0.00662 eval-logloss:0.08592 [198] train-logloss:0.00660 eval-logloss:0.08585 [199] train-logloss:0.00659 eval-logloss:0.08587 [200] train-logloss:0.00657 eval-logloss:0.08589 [201] train-logloss:0.00656 eval-logloss:0.08595 [202] train-logloss:0.00654 eval-logloss:0.08573 [203] train-logloss:0.00653 eval-logloss:0.08573 [204] train-logloss:0.00651 eval-logloss:0.08575 [205] train-logloss:0.00650 eval-logloss:0.08582 [206] train-logloss:0.00648 eval-logloss:0.08584 [207] train-logloss:0.00647 eval-logloss:0.08578 [208] train-logloss:0.00645 eval-logloss:0.08569 [209] train-logloss:0.00644 eval-logloss:0.08571 [210] train-logloss:0.00643 eval-logloss:0.08581 [211] train-logloss:0.00641 eval-logloss:0.08559 [212] train-logloss:0.00640 eval-logloss:0.08580 [213] train-logloss:0.00639 eval-logloss:0.08581 [214] train-logloss:0.00637 eval-logloss:0.08574 [215] train-logloss:0.00636 eval-logloss:0.08566 [216] train-logloss:0.00634 eval-logloss:0.08584 [217] train-logloss:0.00633 eval-logloss:0.08563 [218] train-logloss:0.00632 eval-logloss:0.08573 [219] train-logloss:0.00631 eval-logloss:0.08578 [220] train-logloss:0.00629 eval-logloss:0.08579 [221] train-logloss:0.00628 eval-logloss:0.08582 [222] train-logloss:0.00627 eval-logloss:0.08576 [223] train-logloss:0.00626 eval-logloss:0.08567 [224] train-logloss:0.00624 eval-logloss:0.08586 [225] train-logloss:0.00623 eval-logloss:0.08587 [226] train-logloss:0.00622 eval-logloss:0.08593 [227] train-logloss:0.00621 eval-logloss:0.08595 [228] train-logloss:0.00619 eval-logloss:0.08587 [229] train-logloss:0.00618 eval-logloss:0.08606 [230] train-logloss:0.00617 eval-logloss:0.08600 [231] train-logloss:0.00616 eval-logloss:0.08592 [232] train-logloss:0.00615 eval-logloss:0.08610 [233] train-logloss:0.00613 eval-logloss:0.08611 [234] train-logloss:0.00612 eval-logloss:0.08617 [235] train-logloss:0.00611 eval-logloss:0.08626 [236] train-logloss:0.00610 eval-logloss:0.08629 [237] train-logloss:0.00609 eval-logloss:0.08622 [238] train-logloss:0.00608 eval-logloss:0.08639 [239] train-logloss:0.00607 eval-logloss:0.08634 [240] train-logloss:0.00606 eval-logloss:0.08618 [241] train-logloss:0.00605 eval-logloss:0.08619 [242] train-logloss:0.00604 eval-logloss:0.08625 [243] train-logloss:0.00602 eval-logloss:0.08626 [244] train-logloss:0.00601 eval-logloss:0.08629 [245] train-logloss:0.00600 eval-logloss:0.08622 [246] train-logloss:0.00599 eval-logloss:0.08640 [247] train-logloss:0.00598 eval-logloss:0.08635 [248] train-logloss:0.00597 eval-logloss:0.08628 [249] train-logloss:0.00596 eval-logloss:0.08645 [250] train-logloss:0.00595 eval-logloss:0.08629 [251] train-logloss:0.00594 eval-logloss:0.08631 [252] train-logloss:0.00593 eval-logloss:0.08636 [253] train-logloss:0.00592 eval-logloss:0.08639 [254] train-logloss:0.00591 eval-logloss:0.08649 [255] train-logloss:0.00590 eval-logloss:0.08644 [256] train-logloss:0.00589 eval-logloss:0.08629 [257] train-logloss:0.00588 eval-logloss:0.08646 [258] train-logloss:0.00587 eval-logloss:0.08639 [259] train-logloss:0.00586 eval-logloss:0.08644 [260] train-logloss:0.00585 eval-logloss:0.08646 [261] train-logloss:0.00585 eval-logloss:0.08649 [262] train-logloss:0.00584 eval-logloss:0.08644 [263] train-logloss:0.00583 eval-logloss:0.08647 [264] train-logloss:0.00582 eval-logloss:0.08632 [265] train-logloss:0.00581 eval-logloss:0.08649 [266] train-logloss:0.00580 eval-logloss:0.08654 [267] train-logloss:0.00579 eval-logloss:0.08647 [268] train-logloss:0.00578 eval-logloss:0.08650 [269] train-logloss:0.00577 eval-logloss:0.08652 [270] train-logloss:0.00576 eval-logloss:0.08669 [271] train-logloss:0.00575 eval-logloss:0.08674 [272] train-logloss:0.00575 eval-logloss:0.08683 [273] train-logloss:0.00574 eval-logloss:0.08668 [274] train-logloss:0.00573 eval-logloss:0.08664 [275] train-logloss:0.00572 eval-logloss:0.08650 [276] train-logloss:0.00571 eval-logloss:0.08636 [277] train-logloss:0.00570 eval-logloss:0.08652 [278] train-logloss:0.00570 eval-logloss:0.08657 [279] train-logloss:0.00569 eval-logloss:0.08659 [280] train-logloss:0.00568 eval-logloss:0.08668 [281] train-logloss:0.00567 eval-logloss:0.08664 [282] train-logloss:0.00566 eval-logloss:0.08650 [283] train-logloss:0.00566 eval-logloss:0.08636 [284] train-logloss:0.00565 eval-logloss:0.08640 [285] train-logloss:0.00564 eval-logloss:0.08643 [286] train-logloss:0.00563 eval-logloss:0.08646 [287] train-logloss:0.00562 eval-logloss:0.08650 [288] train-logloss:0.00562 eval-logloss:0.08637 [289] train-logloss:0.00561 eval-logloss:0.08646 [290] train-logloss:0.00560 eval-logloss:0.08645 [291] train-logloss:0.00559 eval-logloss:0.08632 [292] train-logloss:0.00558 eval-logloss:0.08628 [293] train-logloss:0.00558 eval-logloss:0.08615 [294] train-logloss:0.00557 eval-logloss:0.08620 [295] train-logloss:0.00556 eval-logloss:0.08622 [296] train-logloss:0.00556 eval-logloss:0.08631 [297] train-logloss:0.00555 eval-logloss:0.08618 [298] train-logloss:0.00554 eval-logloss:0.08626 [299] train-logloss:0.00553 eval-logloss:0.08613 [300] train-logloss:0.00553 eval-logloss:0.08618 [301] train-logloss:0.00552 eval-logloss:0.08605 [302] train-logloss:0.00551 eval-logloss:0.08602 [303] train-logloss:0.00551 eval-logloss:0.08610 [304] train-logloss:0.00550 eval-logloss:0.08598 [305] train-logloss:0.00549 eval-logloss:0.08606 [306] train-logloss:0.00548 eval-logloss:0.08597 [307] train-logloss:0.00548 eval-logloss:0.08600 [308] train-logloss:0.00547 eval-logloss:0.08600 [309] train-logloss:0.00546 eval-logloss:0.08588 [310] train-logloss:0.00546 eval-logloss:0.08592 [311] train-logloss:0.00545 eval-logloss:0.08595 [312] train-logloss:0.00544 eval-logloss:0.08603 [313] train-logloss:0.00544 eval-logloss:0.08611 [314] train-logloss:0.00543 eval-logloss:0.08599 [315] train-logloss:0.00542 eval-logloss:0.08590 [316] train-logloss:0.00542 eval-logloss:0.08595 [317] train-logloss:0.00541 eval-logloss:0.08598 [318] train-logloss:0.00540 eval-logloss:0.08600 [319] train-logloss:0.00540 eval-logloss:0.08588 [320] train-logloss:0.00539 eval-logloss:0.08597 [321] train-logloss:0.00539 eval-logloss:0.08605 [322] train-logloss:0.00538 eval-logloss:0.08609 [323] train-logloss:0.00537 eval-logloss:0.08598 [324] train-logloss:0.00537 eval-logloss:0.08598 [325] train-logloss:0.00536 eval-logloss:0.08590 [326] train-logloss:0.00535 eval-logloss:0.08578 [327] train-logloss:0.00535 eval-logloss:0.08586 [328] train-logloss:0.00534 eval-logloss:0.08594 [329] train-logloss:0.00534 eval-logloss:0.08582 [330] train-logloss:0.00533 eval-logloss:0.08587 [331] train-logloss:0.00532 eval-logloss:0.08589 [332] train-logloss:0.00532 eval-logloss:0.08592 [333] train-logloss:0.00531 eval-logloss:0.08584 [334] train-logloss:0.00531 eval-logloss:0.08574 [335] train-logloss:0.00530 eval-logloss:0.08582 [336] train-logloss:0.00529 eval-logloss:0.08589 [337] train-logloss:0.00529 eval-logloss:0.08594 [338] train-logloss:0.00528 eval-logloss:0.08583 [339] train-logloss:0.00528 eval-logloss:0.08591 [340] train-logloss:0.00527 eval-logloss:0.08583 [341] train-logloss:0.00526 eval-logloss:0.08573 [342] train-logloss:0.00526 eval-logloss:0.08568 [343] train-logloss:0.00525 eval-logloss:0.08572 [344] train-logloss:0.00525 eval-logloss:0.08580 [345] train-logloss:0.00524 eval-logloss:0.08582 [346] train-logloss:0.00524 eval-logloss:0.08571 [347] train-logloss:0.00523 eval-logloss:0.08579 [348] train-logloss:0.00523 eval-logloss:0.08583 [349] train-logloss:0.00522 eval-logloss:0.08573 [350] train-logloss:0.00522 eval-logloss:0.08566 [351] train-logloss:0.00521 eval-logloss:0.08573 [352] train-logloss:0.00521 eval-logloss:0.08581 [353] train-logloss:0.00520 eval-logloss:0.08571 [354] train-logloss:0.00519 eval-logloss:0.08566 [355] train-logloss:0.00519 eval-logloss:0.08570 [356] train-logloss:0.00518 eval-logloss:0.08563 [357] train-logloss:0.00518 eval-logloss:0.08553 [358] train-logloss:0.00517 eval-logloss:0.08560 [359] train-logloss:0.00517 eval-logloss:0.08568 [360] train-logloss:0.00516 eval-logloss:0.08558 [361] train-logloss:0.00516 eval-logloss:0.08560 [362] train-logloss:0.00515 eval-logloss:0.08564 [363] train-logloss:0.00515 eval-logloss:0.08571 [364] train-logloss:0.00514 eval-logloss:0.08579 [365] train-logloss:0.00514 eval-logloss:0.08569 [366] train-logloss:0.00513 eval-logloss:0.08573 [367] train-logloss:0.00513 eval-logloss:0.08568 [368] train-logloss:0.00512 eval-logloss:0.08559 [369] train-logloss:0.00512 eval-logloss:0.08552 [370] train-logloss:0.00511 eval-logloss:0.08559 [371] train-logloss:0.00511 eval-logloss:0.08550 [372] train-logloss:0.00511 eval-logloss:0.08556 [373] train-logloss:0.00510 eval-logloss:0.08561 [374] train-logloss:0.00510 eval-logloss:0.08563 [375] train-logloss:0.00509 eval-logloss:0.08553 [376] train-logloss:0.00509 eval-logloss:0.08561 [377] train-logloss:0.00508 eval-logloss:0.08567 [378] train-logloss:0.00508 eval-logloss:0.08571 [379] train-logloss:0.00507 eval-logloss:0.08562 [380] train-logloss:0.00507 eval-logloss:0.08558 [381] train-logloss:0.00506 eval-logloss:0.08562 [382] train-logloss:0.00506 eval-logloss:0.08564 [383] train-logloss:0.00505 eval-logloss:0.08555 [384] train-logloss:0.00505 eval-logloss:0.08562 [385] train-logloss:0.00505 eval-logloss:0.08562 [386] train-logloss:0.00504 eval-logloss:0.08555 [387] train-logloss:0.00504 eval-logloss:0.08546 [388] train-logloss:0.00503 eval-logloss:0.08550 [389] train-logloss:0.00503 eval-logloss:0.08546 [390] train-logloss:0.00502 eval-logloss:0.08532 [391] train-logloss:0.00502 eval-logloss:0.08539 [392] train-logloss:0.00502 eval-logloss:0.08530 [393] train-logloss:0.00501 eval-logloss:0.08537 [394] train-logloss:0.00501 eval-logloss:0.08530 [395] train-logloss:0.00500 eval-logloss:0.08537 [396] train-logloss:0.00500 eval-logloss:0.08528 [397] train-logloss:0.00500 eval-logloss:0.08532 [398] train-logloss:0.00499 eval-logloss:0.08528 [399] train-logloss:0.00499 eval-logloss:0.08520 pred_probs = xgb_model.predict(dtest) print('predict( ) 수행 결괏값을 10개만 표시, 예측 확률 값으로 표시됨') print(np.round(pred_probs[:10],3)) # 예측 확률이 0.5 보다 크면 1 , 그렇지 않으면 0 으로 예측값 결정하여 List 객체인 preds에 저장 preds = [ 1 if x > 0.5 else 0 for x in pred_probs ] print('예측값 10개만 표시:',preds[:10]) predict( ) 수행 결괏값을 10개만 표시, 예측 확률 값으로 표시됨 [0.95 0.003 0.9 0.086 0.993 1. 1. 0.999 0.998 0. ] 예측값 10개만 표시: [1, 0, 1, 0, 1, 1, 1, 1, 1, 0] get_clf_eval(y_test , preds, pred_probs) --------------------------------------------------------------------------- NameError Traceback (most recent call last) &lt;ipython-input-10-4ed56bef7ded&gt; in &lt;module&gt; ----&gt; 1 get_clf_eval(y_test , preds, pred_probs) NameError: name &#39;get_clf_eval&#39; is not defined import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) plot_importance(xgb_model, ax=ax) 2. 파이썬 래퍼 XGBoost 하이퍼 파라미터파라미터의 유형 일반 파라미터 : 일반적으로 실행 시 스레드의 개수나 냐ㅣ둣 모드 등의 선택을위한 파라미터로서 디폴트 파라미터 값을 바꾸는 경우는 거의 없다. 부스터 파라미터 : 트리 최적화, 부스팅, regularization 등의 관련 파라미터들을 지칭. 학습 태스크 파라미터 : 학습 수행 시의 객체 함수, 평가를 위한 지표등을 설정하는 파라미터. 주요 일반 파라미터 booster: gbtree(나무 기반 모델) or gblinear (회귀 기반 모델)선택, 디폴트는 gbtree silent : 출력 메시지를 나타내고 싶지 않을 경우 1로 설정 , 디폴트 = 0 nthread : cpu의 실행 스레드 개수를 조정, 디폴트는 cpu의 전체 스레드를 다 사용하는 것. 주요 부스터 파라미터 regularization(정규화) : 과적합 모델을 일반화 해주는 기법 L1 Regularization (lasso): 기존 함수에 계수의 절댓값에 가중치를 곱한 것을 더해주는 것 L2 Regularization (ridge) : 기존함수에 계수의 제곱의 가중치를 곱한 것을 더해주는 것 학습 태스크 파라미터 log loss : 로그 우도에 -1을 곱한 값우도, 가능도(liklelihood) : 어떤 값이 관측 되었을때, 이것이 어던 확률 분포에서 온건지에대한 확률 ROC: 분류모델의 성능을 보여주는 그래프 AUC: ROC 곡선의 아래 면적 (1일때 가장 좋음) 3. 과적합 제어과적합을 제어하는 방법 eta 값을 낮춘다. (0.01 ~ 0.1) → eta 값을 낮추면 num_round(n_estimator)를 반대로 높여주어야 한다. max_depth 값을 낮춘다. min_child_weight 값을 높인다. gamma 값을 높인다. subsample과 colsample_bytree를 낮춘다. XGBoost는 자체적으로 교차 검증, 성능평가, 피처 중요도 등의 시각화 기능을 가지고 있다. 또한 다른 여러가지 성능을 향상시키는 기능을 가지고 있다. Ex) Early Stopping 조기 중단 기능 GBM의 경우 n_estimators에 지정된 횟수만큼 학습을 끝까지 수행하지만, XGB의 경우 오류가 더 이상 개선되지 않으면 수행을 중지 Ex) n_estimators 를 200으로 설정하고, 조기 중단 파라미터 값을 50으로 설정하면, 1부터 200회까지 부스팅을 반복하다가 50회를 반복하는 동안 학습오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료합니다. 07. LightGBM1. LightGBMXGBoost는 GBM보다는 빠르지만 여전히 학습시간이 오래 걸리고, 대용량 데이터로 학습 성능을 기대하려면 높은 병렬도로 학습을 진행해야 한다. LightGBM의 장점 XGBoost보다 학습에 걸리는 시간이 훨씬 적다. 메모리 사용량도 상대적으로 적다. 기능상의 다양상도 XGBoost보다 약간 더 많다. ex. 카테고리형 피처의 자동 변환(원ㅡ핫 인코딩 사용하지않는)과 이에 따른 최적 분할이다. XGBoost와 마찬가지로 대용량 데이터에 대한 뛰어난 성능 및 병렬컴퓨팅 기능을 제공하고 최근에는 추가로GPU까지 지원한다. XGBoost의 장점은 계승하고 단점은 보완하는 방식으로 개발되었다. LightGBM의 단점 적은(10,000건 이하)의 데이터 셋에 적용할 경우 과적합 발생 쉽다. → 오버피팅에 더 강하지만 균형을 맞추기 위한 시간이 필요하다. LightGBM : 리프 중심 트리 분할(Leaf Wise) 방식 사용 → 트리의 균형을 맞추지 않고 최대 손실값(max delta loss)를 가지는 리프 노드를 지속적으로 분할하며 트리 깊이 확장하면서 트리의 깊이가 깊어지고 비대칭적 규칙 트리 생성한다. → 학습을 반복할 수록 균형트리분할방식보다 예측 오류 손실을 최소화할 수 있다. 패키지 설명 패키지명 : ‘lightgbm’ 초기에 lightgbm은 독자적인 모듈로 설계되었으나 편의를 위해 scikit-learn wrapper로 호환이 가능하게 추가로 설계되었다. 패키지 내에 파이썬 래퍼, 사이킷런 래퍼 모두 내장하고있다. 사이킷런 래퍼 LightGBM클래스는 분류를 위한 LGBMClassifier클래스와 회귀를 위한 LGBMRegressor클래스이다. fit( ), predict( ) 기반의 학습 및 예측과 사이킷런이 제공하는 다양한 기능 활용이 가능하다. → 사이킷런에 익숙하다면 별도의 파이썬 래퍼 클래스를 사용하지 않아도 된다. Light GBM은 leaf-wise 방식을 취하고 있기 때문에 수렴이 굉장히 빠르지만, 파라미터 조정에 실패할 경우 과적합을 초래할 수 있다. 2.LightGBM 설치아나콘다로 쉽게 설치 가능하다. 단, 윈도우에 설치할 경우에는 Visual Studio Build tool 2015 이상이 먼저 설치돼 있어야한다. 그 후에 OS 터미널에서 conda명령어를 수행한다. (윈도우10에서는 아나콘다 프롬프트→관리자 권한으로 실행→ conda명령어 수행) conda install -c conda-forge lightgbm 도중에 나오는 Proceed([y]/n)에서 y를 입력하고 엔터를 누른다. 3. LightGBM 하이퍼 파라미터XGBoost와 많은 부분이 유사하다. 주의할 점은 위의 트리분할 방식차이에 따라 이러한 트리 특성에 맞는 하이퍼 파라미터 설정이 필요하다는 점이다.(예를들어 max_depth가 매우 크게 가진다는 것) 주요파라미터 Learning Task 파라미터 objective : 최솟값을 가져야 할 손실함수 정의. 회귀, 다중클래스분류, 이진 분류인지에따라 지정 튜닝방안 num_leaves(트리의 최대 리프 개수)를 중심으로 min_data_in_leaf와 max_depth를 함께 조정하면서 복잡도를 줄이는 것이 기본 튜닝 방안 과적합을 방지하기 위해 num_leaves는 2^(max_depth)보다 작아야 한다. 예를 들어 max_depth가 7이기 때문에, 2^(max_depth)=128이 되는데, 이 때 num_leaves를 이보다 작은 70~80 정도로 설정하는 것이 낫다. learning_rate는 DOWN, n_estimators는 UP (물론 너무 키우면 과적합) learning_rate는 후반부에 건드리는 것이 좋은데, 초반부터 너무 작은 학습률을 지정하면 효율이 크게 떨어질 수 있기 때문이다. reg_lambda, reg_alpha와 같은 regularization적용 학습데이터에 사용할 피처개수나 데이터 샘플링 레코드 개수 줄이기 위해 colsample_bytree, subsample 적용 4. 파이썬 래퍼 LightGBM vs 사이킷런 래퍼 XGBoost vs 사이킷런 래퍼 LightGBM하이퍼 파라미터개요 XGBoost가 사이킷런 규칙에 따라 자신의 하이퍼 파라미터 변경함 LightGBM은 XGBoost와 기능이 많이 유사해서 사이킷런 래퍼의 파라미터를 XGBoost에 맞춰서 변경함 # LightGBM의 파이썬 패키지인 lightgbm에서 LGBMClassifier 임포트 from lightgbm import LGBMClassifier import pandas as pd import numpy as np from sklearn.datasets import load_breast_cancer from sklearn.model_selection import train_test_split dataset = load_breast_cancer() ftr = dataset.data target = dataset.target # 전체 데이터 중 80%는 학습용 데이터, 20%는 테스트용 데이터 추출 X_train, X_test, y_train, y_test=train_test_split(ftr, target, test_size=0.2, random_state=156 ) # 앞서 XGBoost와 동일하게 n_estimators는 400 설정. lgbm_wrapper = LGBMClassifier(n_estimators=400) # LightGBM도 XGBoost와 동일하게 조기 중단 수행 가능. evals = [(X_test, y_test)] lgbm_wrapper.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True) preds = lgbm_wrapper.predict(X_test) pred_proba = lgbm_wrapper.predict_proba(X_test)[:, 1] from sklearn.metrics import confusion_matrix, accuracy_score from sklearn.metrics import precision_score, recall_score from sklearn.metrics import f1_score, roc_auc_score # 수정된 get_clf_eval() 함수 def get_clf_eval(y_test, pred=None, pred_proba=None): confusion = confusion_matrix( y_test, pred) accuracy = accuracy_score(y_test , pred) precision = precision_score(y_test , pred) recall = recall_score(y_test , pred) f1 = f1_score(y_test,pred) # ROC-AUC 추가 roc_auc = roc_auc_score(y_test, pred_proba) print('오차 행렬') print(confusion) # ROC-AUC print 추가 print('정확도: &#123;0:.4f&#125;, 정밀도: &#123;1:.4f&#125;, 재현율: &#123;2:.4f&#125;,\\ F1: &#123;3:.4f&#125;, AUC:&#123;4:.4f&#125;'.format(accuracy, precision, recall, f1, roc_auc)) get_clf_eval(y_test, preds, pred_proba) from lightgbm import plot_importance import matplotlib.pyplot as plt %matplotlib inline fig, ax = plt.subplots(figsize=(10, 12)) # 사이킷런 래퍼 클래스를 입력해도 무방. plot_importance(lgbm_wrapper, ax=ax)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"}],"tags":[{"name":"랜덤포레스트","slug":"랜덤포레스트","permalink":"https://ne-choi.github.io/tags/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/"},{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"분류","slug":"분류","permalink":"https://ne-choi.github.io/tags/%EB%B6%84%EB%A5%98/"},{"name":"결정트리","slug":"결정트리","permalink":"https://ne-choi.github.io/tags/%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC/"},{"name":"앙상블학습","slug":"앙상블학습","permalink":"https://ne-choi.github.io/tags/%EC%95%99%EC%83%81%EB%B8%94%ED%95%99%EC%8A%B5/"}],"author":"ne_choi"},{"title":"ch03. 평가","slug":"Study/Python/ML/ch03_평가","date":"2020-11-29T15:00:00.000Z","updated":"2021-01-20T03:58:12.366Z","comments":true,"path":"/2020/11/30/Study/Python/ML/ch03_평가/","link":"","permalink":"https://ne-choi.github.io/2020/11/30/Study/Python/ML/ch03_%ED%8F%89%EA%B0%80/","excerpt":"","text":"해당 자료는 파이썬 머신러닝 완벽가이드 공부를 위한 필사본입니다. Chapter 03. 평가 머신러닝 구성: 데이터 가공/변환, 모델 학습/예측, 평가(Evaluation) 프로세스로 구성 성능 평가 지표(Evaluation Metric): 모델이 분류인지 회귀인지에 따라 여러 종류로 나뉨 회귀: 대부분 실제값과 예측값의 오차 평균값에 기반 ex) 오차에 절댓값을 씌운 뒤 평균 오차를 구하거나 오차의 제곱값에 루트를 씌운 뒤, 평균 오차를 구하는 방법→ 기본적으로 예측 오차를 가지고 정규화 수준을 재가공하는 방법 (5장에서 다시 설명) 분류: 일반적으로는 실제 결과 데이터와 예측 결과 데이터가 얼마나 정확하고 오류가 적게 발생하는가에 기반 단, 단순히 이러한 정확도만 가지고 판단할 경우 잘못된 평가 결과로 빠질 수 있음 0과 1로 결정값이 한정되는 이진 분류의 성능 평가 지표에 관해 집중적으로 살펴볼 예정 0이냐 1이냐 혹은 긍정/부정을 판단하는 이진 분류에서는, 정확도보다는 다른 성능 평가 지표가 더 중요시되는 경우가 많음 분류의 성능 평가 지표 정확도(Accuracy) 오차행렬(Confusion Matrix) 정밀도(Precision) 재현율(Recall) F1 스코어 ROC AUC 분류의 나눔 이진 분류: 결정 클래스 값 종류 유형에 따라 긍정/부정과 같은 2개의 결괏값만을 가짐 멀티 분류: 여러 개의 결정 클래스 값을 가지는 멀티 분류 01. 정확도(Accuracy) 정확도: 실제 데이터에서 예측 데이터가 얼마나 같은지 판단하는 지표$$ 정확도(Accuracy) = \\frac{예측 결과가 동일한 데이터 수}{전체 예측 데이터 수} $$ 정확도는 직관적으로 모델 예측 성능을 나타내는 평가 지표 단, 이진 분류의 경우 데이터 구성에 따라 ML 모델 성능을 왜곡할 수 있어서 정확도 수치 하나로만 성능을 평가하지 않음 - 예시 2장의 타이타닉 예제 수행 결과를 보면 정확도의 한계를 볼 수 있음 ML 알고리즘을 적용한 후 예측 정확도 결과가 보통 80%대였지만, 탑승객이 남자인 경우보다 여자인 경우 생존 확률이 높았기 때문에 별다른 알고리즘 적용 없이 성별이 여자인 경우 무조건 생존, 남자인 경우 사망으로 예측 결과를 예측해도 비슷한 수치가 나올 수 있음→ 성별 조건 하나만으로 결정하는 수준 낮은 알고리즘도 높은 정확도를 나타내는 상황이 발생할 수 있음 - 추가 실습 사이킷런의 BaseEstimator 클래스를 상속받아 아무런 학습을 하지 않고, 성별에 따라 생존자를 예측하는 단순한 Classifier를 생성 (사이킷런은 BaseEstimator를 상속받으면 Customized 형태의 Estimator를 개발자가 생성할 수 있게 함) MyDummyClassifier 클래스: 학습을 수행하는 fit() 메서드는 아무것도 수행하지 않고, 예측을 수행하는 predict() 메서드는 단순이 Sex 피처가 1이면 0, 그렇지 않으면 1로 예측하는 매우 단순한 Classifier import pandas as pd import numpy as np from IPython.display import Image import warnings warnings.filterwarnings('ignore') from sklearn.base import BaseEstimator class MyDummyClassifier(BaseEstimator): # fit() 메서드는 아무것도 학습하지 않음 def fit(self, X, y=None): pass # predict() 메서드는 단순히 Sex 피처가 1이면 0, 아니면 1로 예측 def predict(self, X): pred = np.zeros( (X.shape[0],1) ) for i in range(X.shape[0]): if X['Sex'].iloc[i] == 1: pred[i] = 0 else : pred[i] = 1 return pred ## 생성된 MyDummyClassifier를 이용해 타이타닉 생존자 예측 수행 from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.preprocessing import LabelEncoder ## Null 처리 함수 def fillna(df): df['Age'].fillna(df['Age'].mean(), inplace=True) df['Cabin'].fillna('N', inplace=True) df['Embarked'].fillna('N', inplace=True) df['Fare'].fillna(0, inplace=True) return df ## 머신러닝에 불필요한 피처 제거 def drop_features(df): df.drop(['PassengerId', 'Name', 'Ticket'], axis=1, inplace=True) return df ## Label Encoding 수행 def format_features(df): df['Cabin'] = df['Cabin'].str[:1] features = ['Cabin', 'Sex', 'Embarked'] for feature in features: le = LabelEncoder() le.fit(df[feature]) df[feature] = le.transform(df[feature]) return df ## 앞에서 실행한 Data Preprocessing 함수 호출 def transform_features(df): df = fillna(df) df = drop_features(df) df = format_features(df) return df # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df = pd.read_csv('../data/titanic/train.csv') y_titanic_df = titanic_df['Survived'] X_titanic_df = titanic_df.drop(['Survived'], axis=1) X_titanic_df = transform_features(X_titanic_df) X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=0) # 위에서 생성한 Dummy Classifier를 활용해서 학습/예측/평가 수행 myclf = MyDummyClassifier() myclf.fit(X_train, y_train) mypredictions = myclf.predict(X_test) print('Dummy Classifier의 정확도는: &#123;0:.4f&#125;'.format(accuracy_score(y_test, mypredictions))) Dummy Classifier의 정확도는: 0.7877 단순한 알고리즘으로 예측하더라도 데이터 구성에 따라 정확도 결과는 약 78.77%로 꽤 높은 수치가 나올 수 있음→ 정확도를 평가 지표로 사용할 때는 신중할 필요가 있음 특히, 불균형한(imbalanced) 레이블 값 분포에서 ML 모델의 성능을 판단할 경우, 적합한 평가 지표가 아님 ex) 100개의 데이터가 있고 이 중 90개의 데이터 레이블이 0, 단 10개의 데이터 레이블이 1이라고 한다면 무조건 0으로 예측 결과를 반환하는 ML 모델의 경우라도 정확도가 90%가 됨 - MNIST 데이터 세트로 살펴보기 MNIST 데이터 세트를 변환해 불균형한 데이터 세트로 만든 뒤, 정확도 지표 적용 시 발생하는 문제 살펴보기 MNIST 데이터 세트는 0부터 9까지 숫자 이미지 픽셀 정보를 가지고 있으며, 숫자 Digit를 예측하는데 사용됨 사이킷런은 load_digits() API를 통해 MNIST 데이터 세트를 제공 원래 MNIST 데이터 세트는 레이블 값이 0부터 9까지 있는 멀티 레이블 분류를 위한 것이나, 이를 레이블 값이 7인 것만 True, 나머지 값은 모두 False로 변환해 이진 분류 문제로 바꾸어 실습→ 전체 데이터의 10%만 True, 나머지 90%는 False인 불균형한 데이터 세트로 변형 MNIST 데이터셋을 multi classification에서 binary classification으로 변경 불균형한 데이터 세트에 모든 데이터를 False로, 즉 0으로 예측하는 classifier를 이용해 정확도를 측정하면 약 90%에 가까운 예측 정확도를 나타냄 아무것도 하지 않고 무조건 특정한 결과로 찍어도 데이터 분포도가 균일하지 않은 경우, 높은 수치가 나타날 수 있음 - ex) step 1. 불균형한 데이터 세트와 Dummy Classifier 생성 from sklearn.datasets import load_digits from sklearn.model_selection import train_test_split from sklearn.base import BaseEstimator from sklearn.metrics import accuracy_score import numpy as np import pandas as pd class MyFakeClassifier(BaseEstimator): def fit(self, x, y): pass # 입력값으로 들어오는 X 데이터 세트 크기만큼 모두 0값으로 만들어 변환 def predict(self, X): return np.zeros((len(X), 1), dtype=bool) # 사이킷런의 내장 데이터 세트인 load_digits()를 이용해 MNIST 데이터 로딩 digits = load_digits() # digits 번호가 7번이면 True고, 이를 astype(int)로 1로 변환, 7번이 아니면 False고 0으로 변환 y = (digits.target == 7).astype(int) X_train, X_test, y_train, y_test = train_test_split(digits.data, y, random_state=11) - step 2. 불균형한 데이터로 생성한 y_test 데이터 분포도를 확인하고 MyFakeClassifier를 이용해 예측과 평가 수행 # 불균형한 레이블 데이터 분포도 확인 print('레이블 테스트 세트 크기:', y_test.shape) print('테스트 세트 레이블 0과 1의 분포도') print(pd.Series(y_test).value_counts()) # Dummy Classifier로 학습/예측/정확도 평가 fakeclf = MyFakeClassifier() fakeclf.fit(X_train, y_train) fakepred = fakeclf.predict(X_test) print('모든 예측을 0으로 하여도 정확도는:&#123;:.3f&#125;'.format(accuracy_score(y_test, fakepred))) 레이블 테스트 세트 크기: (450,) 테스트 세트 레이블 0과 1의 분포도 0 405 1 45 dtype: int64 모든 예측을 0으로 하여도 정확도는:0.900 단순히 predict() 결과를 np.zero()로 모두 0값으로 반환함에도 불구하고 450개의 테스트 데이터 세트에 수행한 예측 정확도는 90% 단지 모든 것을 0으로만 예측해도 MyFakeClassifier의 정확도가 90%로 유수의 ML 알고리즘과 비슷한 결과를 냄→ 정확도 평가 지표는 불균형한 레이블 데이터 세트에서 성능 세트로 사용해서는 안 됨→ 정확도를 분류 평가 지표로 사용 시, 한계를 극복하기 위해 여러 가지 분류 지표를 함께 적용 02. 오차 행렬 오차 행렬(confusion matrix, 혼동행렬) 학습된 분류 모델이 예측을 수행하면서 얼마나 헷갈리는지(confused) 보여주는 지표→ 이진 분류의 예측 오류가 얼마인지, 어떤 유형의 예측 오류가 발생하는지를 나타내는 지표 오차 행렬은 4분면 행렬에서 실제 레이블 클래스 값과 예측 레이블 클래스 값이 어떠한 유형을 가지고 매핑되는지를 나타냄 4분면의 왼쪽, 오른쪽을 예측된 클래스 값 기준으로 Negative와 Positive로 분류하고 4분면의 위, 아래를 실제 클래스 값 기준으로 Negative와 Positive로 분류하면 예측 클래스와 실제 클래스 값 유형에 따라 결정되는 TN, FP, FN, TP 형태로 오차 행렬의 4분면이 만들어짐 True/False: 예측값과 실제값이 같음/틀림, Negative/Positive: 예측 결괏값이 부정(0) / 긍정(1) TN: 예측값을 Negative 값 0으로 예측, 실제 값도 Negative 값 0 FP: 예측값을 Positive 값 1로 예측, 실제 값은 Negative 값 0 FN: 예측값을 Negative 값 0으로 예측, 실제 값은 Positive 값 1 TP: 예측값을 Positive 값 1로 예측, 실제 값도 Positive 값 1 사이킷런은 오차 행렬을 구하기 위해 confusion_matrix() API를 제공 - 정확도 예제에서 다룬 MyFakeClassifier의 예측 성능 지표를 오차 행렬로 표현해보기(예측 결과인 fakepred와 실제 결과인 y_test를 confusion_matrix()의 인자로 입력해 오차 행렬을 confusion_matrix()를 이용해 배열 형태로 출력) from sklearn.metrics import confusion_matrix confusion_matrix(y_test, fakepred) array([[405, 0], [ 45, 0]], dtype=int64) 출력된 오차 행렬은 ndarray 형태 이진 분류의 TN, FP, FN, FP는 상단 도표와 동일한 위치를 가지고 array에서 가져올 수 있음→ TN은 array[0,0]으로 405, FP는 array[0,1]로 0, FN은 array[1,0]으로 45, TP는 array[1,1]로 0에 해당 앞 절의 MyFakeClassifier는 load_digits()에서 target == 7인지 아닌지에 따라 클래스 값을 Ture/False 이진 분류로 변경한 데이터 세트를 사용해서 무조건 Negative로 예측하는 Classifier였고 테스트 데이터 세트의 클래스 값 분포는 0이 405건, 1이 45건 TN: 전체 450건 데이터 중, 무조건 Negative 0으로 예측해서 True가 된 결과 405건 FP: Positive 1로 예측한 건수가 없으므로 0건 FN: Positive 1인 건수 45건을 Negative로 예측해서 False가 된 결과 45건 TP: Positive 1로 예측한 건수가 없으므로 0건 TP, TN, FP, TN 값은 Classifier 성능의 여러 면모를 판단할 수 있는 기반 정보를 제공 이 값을 조합해 Classifier 성능을 측정할 수 있는 주요 지표인 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 값을 알 수 있음 cf) 정확도는 예측값과 실제 값이 얼마나 동일한가에 관한 비율만으로 결정 → 오차 행렬에서 True에 해당하는 값인 TN과 TP에 좌우됨 정확도 = 예측 결과와 실제 값이 동일한 건수 / 전체 데이터 수 = $\\frac{TN + TP}{TN + FP + FN + TP}$ 일반적으로 불균형한 레이블 클래스를 가지는 이진 분류 모델에서는 많은 데이터 중 중점적으로 찾아야 하는 매우 적은 수의 결괏값에 Positive를 설정해 1값을 부여, 그렇지 않은 경우는 Negative로 0값을 부여하는 경우가 많음 ex) 사기 행위 예측 모델: 사기 행위 Positive 양성, 1 / 정상 행위 Negative 음성, 0 암 검진 예측 모델: 암이 양성일 경우 Positive 양성, 1 / 암이 음성일 경우 Negative 음성, 0 불균형한 이진 분류 데이터 세트에서는 Positive 데이터 건수가 매우 작기 때문에 데이터에 기반한 ML 알고리즘은 Positive보다는 Negative로 예측 정확도가 높아지는 경향 발생 10,000건의 데이터 세트에서 9,900건이 Negative고 100건이 Positive라면 Negative로 예측하는 경향이 더 강해 TN은 매우 커지고 TP는 매우 작아짐 Negative로 예측할 때, 정확도가 높기 때문에 FN(Negative로 예측할 때 틀린 데이터 수)이 매우 작고, Positive로 예측하는 경우가 작기 때문에 FP 역시 작아짐→ 정확도 지표는 비대칭한 데이터 세트에서 Positive에 관한 예측 정확도를 판단하지 못한 채 Negative에 관한 예측 정확도만으로도 분류의 정확도가 매우 높게 나타나는 수치적인 판단 오류를 일으킴 - 정리 정확도는 분류(Classification) 모델의 성능을 측정할 수 있는 한 가지 요소 03. 정밀도와 재현율 정밀도 &amp; 재현율: Positive 데이터 세트의 예측 성능에 조금 더 초점을 맞춘 평가 지표 앞서 만든 MyFakeClassifier는 Positive로 예측한 TP 값이 하나도 없기 때문에, 정밀도와 재현율 값이 모두 0 정밀도 = $\\frac{TP}{FP + TP}$ 재현율 = $\\frac{TP}{FN + TP}$ 정밀도: 예측값을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율 공식의 분모인 FP + TP: 예측을 Positive로 한 모든 데이터 건수 / 분자인 TP: 예측과 실제 값이 Positive로 일치한 데이터 건수 Positive 예측 성능을 더욱 정밀히 측정하기 위한 평가 지표로 ‘양성 예측도’라고도 불림 재현율: 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율 공식의 분모인 FN + TP: 실제 값이 Positive인 모든 데이터 건수 / 분자인 TP: 예측과 실제 값이 Positive로 일치한 데이터 건수 민감도(Sensitivity) 또는 TPR(True Positive Rate)라고도 불림 정밀도와 재현율 지표 중, 이진 분류 모델의 업무 특성에 따라 특정 평가 지표가 더 중요한 지표로 간주될 수 있음 재현율이 중요 지표인 경우: 실제 Positive 양성 데이터를 Negative로 잘못 판단할 시, 업무에 큰 영향이 발생하는 경우 정밀도가 중요 지표인 경우: 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단 시, 업무상 큰 영향이 발생하는 경우 - 정리 재현율과 정밀도 모두 TP를 높이는 데 초점 재현율은 FN(실제 Positive, 예측 Negative)를 낮추는 데, 정밀도는 FP를 낮추는 데 초점→ 서로 보완적인 지표로 분류의 성능을 평가하는 데 적용되며 두 수치 모두 높은 것이 가장 좋은 성능 (둘 중 어느 한 평가 지표만 매우 높고, 다른 수치는 매우 낮은 경우는 바람직하지 않음) - ex) 타이타닉 예제 오차 행렬 및 정밀도, 재현율을 모두 구해 예측 성능 평가하기 사이킷런은 정밀도 계산을 위해 precision_score()를, 재현율 계산을 위해 recall_score()를 API로 제공 # 평가 간편히 적용하기: coufusion, matrix, accuracy, precision, recall 등 평가를 한 번에 호출하는 get_clf_eval() 함수 만들기 from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix def get_clf_eval(y_test, pred): confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) print('오차행렬') print(confusion) print('정확도 : &#123;:.4f&#125;\\n정밀도 : &#123;:.4f&#125;\\n재현율 : &#123;:.4f&#125;'.format(accuracy, precision, recall)) 로지스틱 회귀 기반으로 타이타닉 생존자를 예측하고 confusion matrix, accuracy, precision, recall 평가 수행 from sklearn.linear_model import LogisticRegression # 원본 데이터를 재로딩, 데이터 가공, 학습 데이터/테스트 데이터 분할 titanic_df = pd.read_csv('../data/titanic/train.csv') y_titanic_df = titanic_df['Survived'] X_titanic_df = titanic_df.drop('Survived', axis=1) X_titanic_df = transform_features(X_titanic_df) X_train, X_test, y_train, y_test = train_test_split(X_titanic_df, y_titanic_df, test_size = 0.2, random_state = 11) lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) get_clf_eval(y_test, pred) 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 정밀도(Precision)에 비해 재현율(Recall)이 낮게 나옴 1. 정밀도/재현율 트레이드오프 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율의 수치를 높일 수 있음단, 정밀도와 재현율은 상호 보완적인 평가 지표로 한쪽을 강제로 높이면 다른 하나의 수치가 떨어지기 쉬움 사이킷런의 분류 알고리즘은 예측 데이터가 특정 레이블(Label, 결정 클래스 값)에 속하는지 계산하기 위해, 먼저 개별 레이블별로 결정 확률을 구함 → 예측 확률이 큰 레이블값으로 예측 사이킷런은 개별 데이터별로 예측 확률을 반환하는 메서드인 predict_proba()를 제공 predict_proba(): 학습 완료된 사이킷런 Classifier 객체에서 호출 가능하며 테스트 피처 데이터 세트를 파라미터로 입력해주면 테스트 피처 레코드의 개별 클래스 예측 확률을 반환 (predict() 메서드와 유사하지만 반환 결과가 예측 결과 클래스 값이 아닌 예측 확률 결과) - 이진 분류에서 predict_proba()를 수행해 반환되는 ndarray는 첫 번째 칼럼이 클래스 값 0에 대한 예측 확률, 두 번째 칼럼이 클래스 값 1에 대한 예측 확률 pred_proba = lr_clf.predict_proba(X_test) pred = lr_clf.predict(X_test) print('pred_proba() 결과 shape: &#123;0&#125;'.format(pred_proba.shape)) print('pred_proba array에서 앞 3개만 샘플로 추출 \\n:', pred_proba[:3]) # 예측 확률 array와 예측 결괏값 aaray를 병합(concetenate)해 예측 확률과 결괏값을 한눈에 확인 pred_proba_result = np.concatenate([pred_proba, pred.reshape(-1, 1)], axis=1) print('두 개의 class 중에서 더 큰 확률을 클래스 값으로 예측 \\n', pred_proba_result[:3]) pred_proba() 결과 shape: (179, 2) pred_proba array에서 앞 3개만 샘플로 추출 : [[0.4623509 0.5376491 ] [0.87875882 0.12124118] [0.87717457 0.12282543]] 두 개의 class 중에서 더 큰 확률을 클래스 값으로 예측 [[0.4623509 0.5376491 1. ] [0.87875882 0.12124118 0. ] [0.87717457 0.12282543 0. ]] 반환 결과인 ndarray는 0과 1에 대한 확률을 나타내므로 첫 번째 칼럼 값과 두 번째 칼럼 값을 더하면 1이 됨 맨 마지막 줄의 predict() 메서드의 결과, 비교에서도 나타나듯이 두 개의 칼럼 중에서 더 큰 확률 값으로 predict() 메서드가 최종 예측 predict() 메서드는 predict_proba() 메서드에 기반해 생성된 API predict()는 predict_proba() 호출 결과로 반환된 배열에서 분류 결정 임계값보다 큰 값이 들어 있는 칼럼의 위치를 받아서 최종적으로 예측 클래스를 결정하는 API - 코드로 구현해보기 threshold 변수를 특정 값으로 설정하고 Binarizer 클래스를 객체로 생성→ 생성된 Binarizer 객체의 fit_transform() 메서드를 이용해 넘파이 ndarray를 입력→ 입력된 ndarray 값이 지정된 threshold보다 같거나 작으면 0 값으로, 크면 1 값으로 변환해 반환 from sklearn.preprocessing import Binarizer X = [[1, -1, 2], [2, 0, 0], [0, 1.1, 1.2]] # X의 개별 원소들이 threshold 값보다 같거나 작으면 0을, 크면 1을 반환 binarizer = Binarizer(threshold=1.1) print(binarizer.fit_transform(X)) [[0. 0. 1.] [1. 0. 0.] [0. 0. 1.]] 입력된 X 데이터 세트에서 Binarizer의 threshold 값이 1.1보다 같거나 작으면 0, 크면 1로 변환됨을 알 수 있음 - Binarizer를 이용해 사이킷런 predict()의 의사(pseudo) 코드 만들기 from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값. 분류 결정 임계값임. custom_threshold = 0.5 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binarizer 적용 pred_proba_1 = pred_proba[:, 1].reshape(-1, 1) binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 위 의사 코드로 계산된 평가 지표는 앞 예제의 타이타닉 데이터로 학습된 로지스틱 회귀 Classifier 객체에서 호출된 predict()로 계산된 지표 값과 정확히 일치→ predict()가 predict_proba()에 기반함을 알 수 있음 # 추가. 분류 결정 임계값을 0.5에서 0.4로 낮춰보기 from sklearn.preprocessing import Binarizer # Binarizer의 threshold 설정값. 분류 결정 임계값임. (0.5 → 0.4) custom_threshold = 0.4 # predict_proba() 반환값의 두 번째 칼럼, 즉 Positive 클래스 칼럼 하나만 추출해 Binarizer 적용 pred_proba_1 = pred_proba[:, 1].reshape(-1, 1) binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_1) custom_predict = binarizer.transform(pred_proba_1) get_clf_eval(y_test, custom_predict) 오차행렬 [[98 20] [10 51]] 정확도 : 0.8324 정밀도 : 0.7183 재현율 : 0.8361 임계값을 낮추니 재현율 수치가 올라가고 정밀도가 떨어짐→ 분류 결정 임계값은 Positive 예측값을 결정하는 확률의 기준→ 확률을 0.5가 아닌 0.4부터 Positive로 예측을 너그럽게 하여 임계값이 낮아질수록 True 값이 많아짐 Positive 예측값이 많아지면 상대적으로 재현율 값이 높아짐→ 양성 예측을 많이 하다보니 실제 양성을 음성으로 예측하는 획수가 상대적으로 줄기 때문 - 임계값을 0.4부터 0.6까지 0.05씩 증가시키며 평가 지표 조사하기 # 테스트를 수행할 모든 임계값을 리스트 객체로 저장 thresholds = [0.4, 0.45, 0.5, 0.55, 0.6] def get_eval_by_threshold(y_test, pred_proba_c1, thresholds): #thresholds list 객체 내의 값을 iteration 하면서 평가 수행 for custom_threshold in thresholds: binarizer = Binarizer(threshold=custom_threshold).fit(pred_proba_c1) custom_predict = binarizer.transform(pred_proba_c1) print('\\n임계값: ', custom_threshold) get_clf_eval(y_test, custom_predict) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds) 임계값: 0.4 오차행렬 [[98 20] [10 51]] 정확도 : 0.8324 정밀도 : 0.7183 재현율 : 0.8361 임계값: 0.45 오차행렬 [[103 15] [ 12 49]] 정확도 : 0.8492 정밀도 : 0.7656 재현율 : 0.8033 임계값: 0.5 오차행렬 [[104 14] [ 13 48]] 정확도 : 0.8492 정밀도 : 0.7742 재현율 : 0.7869 임계값: 0.55 오차행렬 [[109 9] [ 15 46]] 정확도 : 0.8659 정밀도 : 0.8364 재현율 : 0.7541 임계값: 0.6 오차행렬 [[112 6] [ 16 45]] 정확도 : 0.8771 정밀도 : 0.8824 재현율 : 0.7377 (지금까지 임계값 변화에 따른 평가 지표 값을 알아보는 코드를 작성) 사이킷런은 이와 유사한 precision_recall_curve() API를 제공 precision_recall_curve() API의 입력 파라미터와 반환 값은 아래와 같음 입력 파라미터 y_true: 실제 클래스값 배열(배열 크기 = [데이터 건수] probas_pred: Positive 칼럼의 예측 확률 배열(배열 크기 = [데이터 건수] 반환값 정밀도: 임계값별 정밀도 값을 배열로 반환 재현율: 임계값별 재현율 값을 배열로 반환 - 추가) precision_recall_curve()로 타이타닉 예측 모델의 임계값별 정밀도와 재현율 구하기 from sklearn.metrics import precision_recall_curve # 레이블 값이 1일 때의 예측확률을 추출 pred_proba_class1 = lr_clf.predict_proba(X_test)[ : , 1] # 실제값 데이터 세트와 레이블 값이 1일 때 예측확률을 precision_recall_curve의 인자로 반환 precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_class1) print('반환된 분류 결정 임계값 배열의 shape: ', thresholds.shape) # 반환된 임계값 배열 로우가 147건 이므로 샘플로 10건만 추출하되, 임계값을 15 Step으로 추출 thr_index = np.arange(0, thresholds.shape[0], 15) print('샘플 추출을 위한 임계값 배열의 index 10개: ', thr_index) print('샘플용 10개의 임계값: ', np.round(thresholds[thr_index], 2)) # 15 step 단위로 추출된 임계값에 따른 정밀도와 재현율 값 print('샘플 임계값별 정밀도 : ', np.round(precisions[thr_index], 3)) print('샘플 임계값별 재현율 : ', np.round(recalls[thr_index], 3)) 반환된 분류 결정 임계값 배열의 shape: (143,) 샘플 추출을 위한 임계값 배열의 index 10개: [ 0 15 30 45 60 75 90 105 120 135] 샘플용 10개의 임계값: [0.1 0.12 0.14 0.19 0.28 0.4 0.56 0.67 0.82 0.95] 샘플 임계값별 정밀도 : [0.389 0.44 0.466 0.539 0.647 0.729 0.836 0.949 0.958 1. ] 샘플 임계값별 재현율 : [1. 0.967 0.902 0.902 0.902 0.836 0.754 0.607 0.377 0.148] 추출된 임계값 샘플 10개에 해당하는 정밀도 값과 재현율 값을 살펴보면 임계값이 증가할수록 정밀도값은 동시에 높아지나 재현율 값은 낮아짐 precision_recall_curve() API는 정밀도와 재현율의 임계값에 따른 값 변화를 곡선 형태의 그래프로 시각화하는 데 이용할 수 있음 # API 이용하여 정밀도, 재현율 곡선 시각화하기 import matplotlib.pyplot as plt import matplotlib.ticker as ticker %matplotlib inline def precision_recall_curve_plot(y_test, pred_proba_c1): # threshold ndarray와 이 threshold에 따른 정밀도, 재현율 ndarray 추출 precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba_c1) # x축을 threshold 값, y축을 정밀도, 재현율로 그리기 plt.figure(figsize=(8,6)) thresholds_boundary = thresholds.shape[0] plt.plot(thresholds, precisions[0: thresholds_boundary], linestyle= '--', label='precision') plt.plot(thresholds, recalls[0: thresholds_boundary], label='recall') # threshold의 값 X축의 scale을 0.1 단위로 변경 stard, end = plt.xlim() plt.xticks(np.round(np.arange(stard, end, 0.1), 2)) # x축, y축 label과 legend, 그리고 grid 설정 plt.xlabel('Threshold value') plt.ylabel('Precision and Recall value') plt.legend() plt.grid() plt.show() precision_recall_curve_plot(y_test, lr_clf.predict_proba(X_test)[:,1]) (정밀도는 점선, 재현율은 실선으로 표현) 임계값이 낮을수록 많은 수의 양성 예측으로 재현율 값이 극도로 높아지고 정밀도 값이 극도로 낮아짐 임계값을 증가시킬수록 재현율 값이 낮아지고 정밀도 값이 높아짐 2. 정밀도와 재현율의 맹점 임계값의 변경은 정밀도와 재현율을 상호 보완할 수 있는 수준에서 해야 함 정밀도 또는 재현율 평가 지표 수치 중 하나를 극단적으로 높이는 방법이나 잘못된 방법 정밀도가 100%가 되는 방법 확실한 기준이 되는 경우만 Positive로 예측하고 나머지는 모두 Negative로 예측 정밀도 = TP / (TP + FP) 전체 환자 1,000명 중 확실한 Positive 징후만 가진 암환자가 1명이라고 하면, 한 명만 Positive로 예측하더라도 정밀도는 1/(1+0) = 100%가 됨 재현율이 100%가 되는 방법 모든 환자를 Positive로 예측 재현율 = TP / (TP + FN) 전체 환자 1,000명을 다 Positive로 예측하면 실제 양성인 사람이 30명 정도여도, TN이 수치에 포함되지 않고 FN은 0이므로 정밀도는 1/(1+0)으로 100%가 됨 04. F1 스코어 F1 스코어: 정밀도와 재현율을 결합한 지표, 정밀도와 재현율이 어느 한 쪽으로 치우치지 않을 때 상대적으로 높은 값을 가짐 - F1 &#x3D; $\\frac&#123;2&#125;&#123;&#123;1&#125;&#123;recall&#125;+&#123;1&#125;&#123;precision&#125;&#125;$ &#x3D; 2 x $\\frac&#123;precision x recall&#125;&#123;precision + recall&#125;$ 만일 A 예측 모델의 정밀도가 0.9, 재현율이 0.1로 극단적인 차이가 나고, B 예측 모델은 정밀도가 0.5, 재현율이 0.5로 큰 차이가 없다면 A 예측 모델의 F1 스코어는 0.18이고, B 예측 모델의 F1 스코어는 0.5로 B 모델이 A 모델에 비해 매우 우수한 F1 스코어를 가지게 됨 사이킷런은 F1 스코어를 구하기 위해 f1_score()라는 API를 제공 from sklearn.metrics import f1_score f1 = f1_score(y_test, pred) print('F1 스코어: &#123;:.4f&#125;'.format(f1)) F1 스코어: 0.7805 # 추가. 타이타닉 생존자 예측에서 임계값을 변화시키며 F1 스코어를 포함한 평가 지표 구하기 def get_clf_eval(y_test, pred): confusion = confusion_matrix(y_test, pred) accuracy = accuracy_score(y_test, pred) precision = precision_score(y_test, pred) recall = recall_score(y_test, pred) # F1 스코어 추가 f1 = f1_score(y_test, pred) print('오차행렬') print(confusion) # F1 score print 추가 print('\\n정확도: &#123;:.4f&#125;\\n정밀도: &#123;:.4f&#125;\\n재현율: &#123;:.4f&#125;\\nF1: &#123;:.4f&#125;'.format(accuracy, precision, recall, f1)) thresholds = [0.4, 0.45, 0.5, 0.55, 0.6] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1, 1), thresholds) 임계값: 0.4 오차행렬 [[98 20] [10 51]] 정확도: 0.8324 정밀도: 0.7183 재현율: 0.8361 F1: 0.7727 임계값: 0.45 오차행렬 [[103 15] [ 12 49]] 정확도: 0.8492 정밀도: 0.7656 재현율: 0.8033 F1: 0.7840 임계값: 0.5 오차행렬 [[104 14] [ 13 48]] 정확도: 0.8492 정밀도: 0.7742 재현율: 0.7869 F1: 0.7805 임계값: 0.55 오차행렬 [[109 9] [ 15 46]] 정확도: 0.8659 정밀도: 0.8364 재현율: 0.7541 F1: 0.7931 임계값: 0.6 오차행렬 [[112 6] [ 16 45]] 정확도: 0.8771 정밀도: 0.8824 재현율: 0.7377 F1: 0.8036 05. ROC 곡선과 AUC ROC 곡선(Receiver Operation Characteristic Curve) FPR(Fales Positive Rate)이 변할 때, TPR(True Positive Rate)이 어떻게 변하는지 나타내는 곡선 FPR을 X 축으로, TPR을 Y 축으로 잡으면 FPR 변화에 따른 TPR 변화가 곡선 형태로 나타남 TPR(True Positive Rate): 재현율(Recall)이자 민감도(Sensitivity) =&gt; TPR = TP / (FN + TP) 실제값 Positive가 정확히 예측돼야 하는 수준(질병 보유자를 질병 보유했다고 양성 판정) TNR(True Negative Rate)이자 특이성(Specificity) =&gt; TNR = TN / (TN + FP) TNR인 특이성은 아래 공식으로 구할 수 있음 FPR = FP / (FP + TN) = 1 - TNR = 1 - 특이성 ROC 곡선은 FPR을 0부터 1까지 변경하면서 TPR의 변화 값을 구함 사이킷런은 ROC 곡선을 구하기 위해 roc_curve() API를 제공 입력 파라미터 y_true: 실제 클래스 값 array (array shape = [데이터 건수] y_score: predict_proba()의 반환값 array에서 Positive 칼럼의 예측 확률이 보통 사용됨 (array.shape = [n_samples] 반환 값 fpr: fpr 값을 array로 반환 tpr: tpr 값을 array로 반환 threshold: threshold 값 array # roc_curve() API를 이용해 타이타닉 생존자 예측 모델의 FPR, TPR, 임계값 구하기 from sklearn.metrics import roc_curve # 레이블 값이 1일 때 예측 확률 추출 pred_proba_class1 = lr_clf.predict_proba(X_test)[:,1] fprs, tprs, thresholds = roc_curve(y_test, pred_proba_class1) # 반환된 임계값 배열 로우가 47건이므로 샘플로 10건만 추출하되 임계값을 5step으로 추출 # threshold[0]은 max(예측확률) + 1로 임의 설정, 이를 제외하기 위해 np.arrange는 1부터 시작 thr_index = np.arange(1, thresholds.shape[0], 5) print('샘플 추출을 위한 임계값 배열의 index 10개: ', thr_index) print('샘플용 10개의 임계값: ', np.round(thresholds[thr_index], 2)) # 5 step으로 추출된 임계값에 따른 FPR, TPR 값 print('샘플 임계값별 FPR: ', np.round(fprs[thr_index], 3)) print('샘플 임계값별 TPR: ', np.round(tprs[thr_index], 3)) 샘플 추출을 위한 임계값 배열의 index 10개: [ 1 6 11 16 21 26 31 36 41 46 51] 샘플용 10개의 임계값: [0.97 0.65 0.63 0.56 0.45 0.4 0.35 0.15 0.13 0.11 0.11] 샘플 임계값별 FPR: [0. 0.017 0.034 0.076 0.127 0.169 0.203 0.466 0.585 0.686 0.797] 샘플 임계값별 TPR: [0.033 0.639 0.721 0.754 0.803 0.836 0.885 0.902 0.934 0.967 0.984] 결과를 살펴보면 임계깞이 1에 가까운 값에서 점점 작아지며 FPR이 점점 커짐FPR이 조금씩 커질 때 TPR은 가파르게 커짐 # ROC 곡선 시각화 def roc_curve_plot(y_test, pred_proba_c1): #임계값에 따른 FPR, TPR 값을반환 받음 fprs, tprs, thresholds = roc_curve(y_test, pred_proba_c1) # ROC곡선을 그래프로 그림 plt.plot(fprs, tprs, label='ROC') # 가운데 대각선 직선을 그림 plt.plot([0,1], [0,1], 'k--', label='Random') # FPR X축의 Scale을 0.1 단위로 변경, X, Y축 명 설정 등 start, end = plt.xlim() plt.xticks(np.round(np.arange(start, end, 0.1), 2)) plt.xlim(0, 1) plt.ylim(0, 1) plt.xlabel('FPR(1-Sensitivity)') plt.ylabel('TPR(Recall)') plt.legend() roc_curve_plot(y_test, pred_proba[:, 1]) 일반적으로 ROC 곡선 자체는 FPR과 TPR의 변화값을 보는 데 이용 분류의 성능 지표로는 ROC 곡선 면적에 기반한 AUC 값으로 결정 AUC(Area Under Curve): 곡선 밑의 면적 값으로 1에 가까울 수록 좋은 수치AUC 수치가 커지려면 FPR이 작은 사태에서 얼마나 큰 TPR을 얻을 수 있느냐가 관건가운데 대각선 직선은 랜덤 수준(동전 던지기 수준) 이진 분류 AUC 값으로 0.5→ 보통의 분류는 0.5 이상의 AUC 값을 가짐 from sklearn.metrics import roc_auc_score pred = lr_clf.predict(X_test) roc_score = roc_auc_score(y_test, pred) print('ROC AUC 값 : &#123;:.4f&#125;'.format(roc_score)) ROC AUC 값 : 0.8341 06. 피마 인디언 당뇨병 예측피마 인디언 당뇨병 데이터 세트 구성 살펴보기 Pregnancies: 임신횟수 Glucose: 포도당 부하 검사 수치 BloodPressure: 혈압 SkinThickness: 팔 삼두근 뒤쪽의 피하지방 측정값 Insulin: 혈청 인슐린 BMI: 체질량 지수 DiabetesPedigreeFunction : 당뇨 내력 가중치 값 Age: 나이 Outcome: 당뇨여부(0 또는 1) import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve from sklearn.preprocessing import StandardScaler, Binarizer from sklearn.linear_model import LogisticRegression import warnings warnings.filterwarnings('ignore') # 데이터 불러오기 diabetes_data = pd.read_csv('../data/pima-indians/diabetes.csv') print(diabetes_data['Outcome'].value_counts()) diabetes_data.head(3) 0 500 1 268 Name: Outcome, dtype: int64 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 전체 768개의 데이터 중, Negative 값 0이 500개, Positive 값 1이 268개 # feature 타입과 Null 개수 세어보기 diabetes_data.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 768 entries, 0 to 767 Data columns (total 9 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Pregnancies 768 non-null int64 1 Glucose 768 non-null int64 2 BloodPressure 768 non-null int64 3 SkinThickness 768 non-null int64 4 Insulin 768 non-null int64 5 BMI 768 non-null float64 6 DiabetesPedigreeFunction 768 non-null float64 7 Age 768 non-null int64 8 Outcome 768 non-null int64 dtypes: float64(2), int64(7) memory usage: 54.1 KB Null 값은 없으며 피처 타입은 모두 숫자형 - 로지스틱 회귀를 이용해 예측 모델 생성하기 # 피처 데이터 세트 X, 레이블 데이터 세트 y 추출 # 맨 끝이 Outcome 칼럼으로 레이블 값, 칼럼 위치 -1을 이용해 추출 X = diabetes_data.iloc[:, :-1] y = diabetes_data.iloc[:, -1] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 156, stratify=y) # 로지스틱 회귀로 학습, 예측 및 평가 수행 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) get_clf_eval(y_test, pred) 오차행렬 [[88 12] [23 31]] 정확도: 0.7727 정밀도: 0.7209 재현율: 0.5741 F1: 0.6392 # 임계값별로 정밀도 - 재현율 출력 pred_proba = lr_clf.predict_proba(X_test)[:, 1] precision_recall_curve_plot(y_test, pred_proba) 재현율 곡선을 보면 임계값을 0.42 정도로 낮추면 정밀도와 재현율이 어느 정도 균형을 맞출 것그러나, 두 지표 모두 0.7이 되지 않는 수치 - 임계값을 인위 조작하기 전에 다시 데이터 점검해보기 # 원본 데이터 DataFrane describe() 메서드로 피처 값의 분포도 살피기 diabetes_data.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; Pregnancies Glucose BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 min() 값이 0으로 된 피처가 상당히 많음Glucose 피처는 포도당 수치로 min 값이 0으로 나올 수는 없음 # min() 값이 0으로 된 피처에 대해 0 값의 건수 및 전체 데이터 건수 대비 몇 퍼센트의 비율로 존재하는지 확인해보기 feature_list = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI'] def hist_plot(df): for col in feature_list: df[col].plot(kind='hist', bins=20).set_title('Histogram of '+col) plt.show() hist_plot(diabetes_data) SkinThickness와 Insulin의 0 값은 전체의 29.56%, 48.7%로 많은 수준 - 일괄 삭제 대신 위 피처의 0 값을 평균값으로 대체 # 위 컬럼들에 대한 0 값의 비율 확인 zero_count = [] zero_percent = [] for col in feature_list: zero_num = diabetes_data[diabetes_data[col]==0].shape[0] zero_count.append(zero_num) zero_percent.append(np.round(zero_num/diabetes_data.shape[0]*100,2)) zero = pd.DataFrame([zero_count, zero_percent], columns=feature_list, index=['count', 'percent']).T zero .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; count percent Glucose 5.0 0.65 BloodPressure 35.0 4.56 SkinThickness 227.0 29.56 Insulin 374.0 48.70 BMI 11.0 1.43 # zero_features 리스트 내부 저장된 개별 피처의 0 값을 NaN 값으로 대체 diabetes_data[feature_list] = diabetes_data[feature_list].replace(0, np.nan) # 위 5개 feature 에 대해 0값을 평균 값으로 대체 mean_features = diabetes_data[feature_list].mean() diabetes_data[feature_list] = diabetes_data[feature_list].replace(np.nan, mean_features) X = diabetes_data.iloc[:, :-1] y = diabetes_data.iloc[:, -1] # StandardScaler 클래스를 상용하여 데이터 세트에 스케일링 적용 scaler = StandardScaler() X_scaled = scaler.fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state=156, stratify = y) # 로지스틱 회귀로 학습, 예측, 평가 수행 lr_clf = LogisticRegression() lr_clf.fit(X_train, y_train) pred = lr_clf.predict(X_test) get_clf_eval(y_test, pred) 오차행렬 [[89 11] [21 33]] 정확도: 0.7922 정밀도: 0.7500 재현율: 0.6111 F1: 0.6735 데이터 변환과 스케일링으로 성능 수치가 일정 수준 개선하지만, 여전히 재현율 수치 개선이 필요함 - 분류 결정 임계값을 변화시키면서 재현율 값의 성능 수치 개선 정도를 확인하기 # 임계값을 0.3에서 0.5까지 0.03씩 변화시키면서 재현율과 다른 평가 지표의 값 변화를 출력 thresholds = [0.3, 0.33, 0.36, 0.39, 0.42, 0.45, 0.48, 0.50] pred_proba = lr_clf.predict_proba(X_test) get_eval_by_threshold(y_test, pred_proba[:, 1].reshape(-1, 1), thresholds) 임계값: 0.3 오차행렬 [[68 32] [10 44]] 정확도: 0.7273 정밀도: 0.5789 재현율: 0.8148 F1: 0.6769 임계값: 0.33 오차행렬 [[74 26] [11 43]] 정확도: 0.7597 정밀도: 0.6232 재현율: 0.7963 F1: 0.6992 임계값: 0.36 오차행렬 [[75 25] [13 41]] 정확도: 0.7532 정밀도: 0.6212 재현율: 0.7593 F1: 0.6833 임계값: 0.39 오차행렬 [[82 18] [16 38]] 정확도: 0.7792 정밀도: 0.6786 재현율: 0.7037 F1: 0.6909 임계값: 0.42 오차행렬 [[85 15] [18 36]] 정확도: 0.7857 정밀도: 0.7059 재현율: 0.6667 F1: 0.6857 임계값: 0.45 오차행렬 [[86 14] [19 35]] 정확도: 0.7857 정밀도: 0.7143 재현율: 0.6481 F1: 0.6796 임계값: 0.48 오차행렬 [[88 12] [20 34]] 정확도: 0.7922 정밀도: 0.7391 재현율: 0.6296 F1: 0.6800 임계값: 0.5 오차행렬 [[89 11] [21 33]] 정확도: 0.7922 정밀도: 0.7500 재현율: 0.6111 F1: 0.6735 0.33: 정확도와 정밀도를 희생하고 재현율을 높이는 데 가장 좋은 임계값 0.48: 전체적인 성능 평가 지표를 유지하며 재현율을 약간 향상시키는 좋은 임계값 - 앞서 학습된 로지스틱 회귀 모델을 이용해 임계값을 0.48로 낮춘 상태에서 다시 예측하기 # 임계값을 0.48로 설정한 Binarizer 생성 binarizer = Binarizer(threshold=0.48) # 위에서 구한 predict_proba() 예측 확률 array에서 1에 해당하는 칼럼값을 Binarizer 변환하기 pred_th_048 = binarizer.fit_transform(pred_proba[:, 1].reshape(-1, 1)) get_clf_eval(y_test, pred_th_048) 오차행렬 [[88 12] [20 34]] 정확도: 0.7922 정밀도: 0.7391 재현율: 0.6296 F1: 0.6800 07. 정리 지금까지 살펴본 것 분류에 사용되는 정확도, 오차 행렬, 정밀도, 재현율, F1 스코어, ROC-AUC 이진 분류 레이블 값이 불균형하게 분포될 경우, 정확도만으로 머신러닝 모델의 예측 성능을 평가할 수 없음 오차 행렬 Negative와 Positive 값을 가지는 실제 클래스 값과 예측 클래스 값이 TN, FP, FN, TP로 매핑되는 4분면 행렬을 기반으로 예측 성능 평가 정확도, 정밀도, 재현율 수치는 위 4가지 값을 다양하게 결합하여 만들어짐 정확도, 정밀도, 재현율 수치를 통해 분류 모델 예측 성능 오류가 어떤 모양으로 발생하는지 확인 가능 정밀도(Precision)와 재현율(Recall) Positive 데이터 세트의 예측 성능에 좀 더 초점을 맞춘 지표 재현율이 더 중요한 지표인 경우: 실제 Positive 양성인 데이터 예측을 Negative로 잘못 판단할 경우 업무상 큰 영향이 발생할 때 재현율이 특별히 강조돼야 할 경우 분류의 결정 임계값(Threshold)을 조정해 정밀도 또는 재현율 수치 높이기 F1 스코어 정밀도와 재현율을 결합한 평가 지표 정밀도와 재현율이 어느 한쪽으로 치우치지 않을 때 높은 지표값을 가짐 ROC_AUC는 일반적으로 이진 분류의 성능 평가를 위해 가장 많이 사용되는 지표 AUC(Area Under Curve) 값은 ROC 곡선 밑의 면적을 구한 것(일반적으로 1에 가까울수록 좋은 수치)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"}],"tags":[{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"정확도","slug":"정확도","permalink":"https://ne-choi.github.io/tags/%EC%A0%95%ED%99%95%EB%8F%84/"},{"name":"재현율","slug":"재현율","permalink":"https://ne-choi.github.io/tags/%EC%9E%AC%ED%98%84%EC%9C%A8/"},{"name":"F1스코어","slug":"F1스코어","permalink":"https://ne-choi.github.io/tags/F1%EC%8A%A4%EC%BD%94%EC%96%B4/"}],"author":"ne_choi"},{"title":"ch02. 사이킷런으로 시작하는 머신러닝","slug":"Study/Python/ML/ch02_사이킷런으로_시작하는_머신러닝","date":"2020-11-28T15:00:00.000Z","updated":"2021-01-20T03:58:12.354Z","comments":true,"path":"/2020/11/29/Study/Python/ML/ch02_사이킷런으로_시작하는_머신러닝/","link":"","permalink":"https://ne-choi.github.io/2020/11/29/Study/Python/ML/ch02_%EC%82%AC%EC%9D%B4%ED%82%B7%EB%9F%B0%EC%9C%BC%EB%A1%9C_%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94_%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D/","excerpt":"","text":"해당 자료는 파이썬 머신러닝 완벽가이드 공부를 위한 필사본입니다. Chapter 02. 사이킷런으로 시작하는 머신러닝01. 사이킷런 소개와 특징 사이킷런(scikit-learn) 파이썬 머신러닝 라이브러리 중 가장 많이 사용되는 라이브러리 파이썬 기반의 머신러닝을 위한 가장 쉽고 효율적인 개발 라이브러리를 제공 사이킷런 특징 쉽고 가장 파이썬스러운 API 제공 머신러닝을 위한 다양한 알고리즘, 개발을 위한 편리한 프레임워크와 API 제공 오랜 기간 실전 환경에서 검증됐으며, 매우 많은 환경에서 사용되는 성숙한 라이브러리 Anaconda를 설치하면 기본적으로 사이킷런까지 설치가 되기에 설치할 필요는 없음 # 사이킷런 버전 확인 import sklearn print(sklearn.__version__) 0.23.1 02. 첫 번째 머신러닝 - 붓꽃 품종 예측 붓꽃 데이터 세트로 붓꽃의 품종을 분류(classification) 붓꽃 데이터 세트: 꽃잎 길이, 너비, 꽃받침 길이, 너비 피차(feature)를 기반으로 꽃 품종을 예측하기 위한 데이터 세트 분류는 대표적인 지도학습(Supervised Learning) 방법의 하나 지도학습 학습을 위한 다양한 피처와 분류 결정값인 레이블 데이터로 모델을 학습한 뒤, 별도의 테스트 데이터 세트에서 미지의 레이블을 예측→ 지도학습은 명확한 정답이 주어진 데이터를 먼저 학습한 뒤 미지의 정답을 예측하는 방식 학습 데이터 세트: 학습을 위해 주어진 데이터 세트 테스트 데이터 세트: 머신러닝 모델의 예측 성능 평가를 위해 주어진 데이터 세트 # 사이킷런에서 자체적으로 제공하는 데이터 세트를 생성하는 모듈의 모임 from sklearn.datasets import load_iris # sklearn.tree 내 모듈은 트리 기반 ML 알고리즘을 구현한 클래스 모임 from sklearn.tree import DecisionTreeClassifier # 학습 데이터와 검증 데이터, 예측데이터로 데이터를 분리하거나 최적의 하이퍼 파라미터로 평가하기 위한 다양한 모듈의 모임 from sklearn.model_selection import train_test_split import pandas as pd # 붓꽃 데이터 세트 로딩 iris = load_iris() # iris.data는 Iris 데이터 세트에서 피처만으로 된 데이터를 numpy로 가짐 iris_data = iris.data # iris.target은 붓꽃 데이터 세트에서 레이블(결정값) 데이터를 numpy로 가짐 iris_label = iris.target print('iris target값:', iris_label) print('iris target명:', iris.target_names) # 붓꽃 데이터 세트를 자세히 보기 위해 DataFrame으로 변환 iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) iris_df['label'] = iris.target iris_df.head(3) iris target값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] iris target명: [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) label 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 피처는 총 4개, 레이블은 3개(0: setosa 품종, 1: versicolor 품종, 2: virginica 품종) 학습용 데이터와 테스트용 데이터 분리하기 train_test_split() API를 사용하면, 학습 데이터와 테스트 데이터를 test_size 파라미터 입력 값의 비율로 쉽게 분할함 test_size=0.2 ← 전체 데이터 중, 테스트 데이터가 20%, 학습 데이터가 80%로 분할됨 dt_clf = DecisionTreeClassifier() iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size = 0.2, random_state=11) train_test_split() iris_data: 피처 데이터 세트 iris_label: 레이블 데이터 세트 test_size=0.2: 전체 데이터 세트 중 테스트 데이터 세트 비율 = 20% random_state: 호출 시마다 같은 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값 (여기서는 값 고정을 위해 임의 숫자를 넣음) train_test_split(): 호출 시 무작위로 데이터를 분리 → random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트용 데이터가 생성됨 train_test_split() 구분 X_train X_test y_train y_test 학습용 피처 데이터 세트 테스트용 피처 데이터 세트 학습용 레이블 데이터 세트 테스트용 레이블 데이터 세트 의사결정나무로 학습과 예측 수행 사이킷런 의사결정나무 클래스인 DecisionTreeClassifier를 객체로 생성 생성된 DecisionTreeClassifier 객체의 fit() 메서드에 학습용 피처 데이터 속성과 결정값 데이터 세트를 입력해 호출 # DecisionTreeClassifier 객체 생성 dt_clf = DecisionTreeClassifier(random_state=11) # 같은 학습/예측 결과를 위해 random 값 임의 지정 # 학습 수행 dt_clf.fit(X_train, y_train) DecisionTreeClassifier(random_state=11) 위 코드 실행 결과, DecisionTreeClassifier 객체는 학습 데이터 기반으로 학습 완료 학습된 객체를 이용해 예측 수행 예측은 반드시 학습 데이터가 아닌 다른 데이터를 이용해야 하며, 일반적으로 테스트 데이터 세트를 이용 DecisionTreeClassifier 객체의 predict() 메서드에 테스트용 피처 데이터 세트를 입력해 호출하면, 학습된 모델 기반에서 테스트 데이터 세트에 대한 예측값을 반환 # 학습 완료된 객체에서 테스트 데이터 세트로 예측 수행 pred = dt_clf.predict(X_test) 예측 결과 기반으로 DecisionTreeClassifier의 예측 성능 평가하기 해당 예제에서는 정확도(예측 결과가 실제 레이블값과 얼마나 일치하는지) 평가 사이킷런에서는 정확도 측정을 위해 accuracy_score() 함수 제공 accuracy_score: 첫 번째 파라미터로 실제 레이블 데이터 세트, 두 번째 파라미터로 예측 레이블 데이터 세트 입력 from sklearn.metrics import accuracy_score print('예측 정확도: &#123;0: 4f&#125;'.format(accuracy_score(y_test, pred))) 예측 정확도: 0.933333 학습한 의사결정나무 알고리즘 예측 정확도가 약 0.9333(93.33%)으로 측정됨 데이터 세트 분리: 데이터를 학습 데이터와 테스트 데이터로 분리 모델 학습: 학습 데이터를 기반으로 ML 알고리즘을 적용해 모델을 학습시킴 예측 수행: 학습된 ML 모델로 테스트 데이터의 분류(즉, 붓꽃 종류) 예측 평가: 예측된 결괏값과 테스트 데이터의 실제 결괏값을 비교해 ML 모델 성능을 평가 03. 사이킷런의 기반 프레임워크 익히기1. Estimator 이해 및 fit(), predict() 메서드 사이킷런은 API 일관성과 개발 편의성을 제공하기 위한 패키지 사이킷런을 ML 모델 학습을 위해 fit()을, 학습된 모델의 예측을 위해 predict() 메서드를 제공 지도학습의 주요 두 축인 분류(classification)와 회귀(regression)의 다양한 예측 결과를 반환 사이킷런에서는 분류 알고리즘을 구현한 클래스를 classifier로, 회귀 알고리즘을 구현한 클래스를 regressor로 지칭 사이킷런은 매우 많은 유형의 classifier와 regressor 클래스를 제공: 이를 합쳐 Estimator 클래스라고 부름 사이킷런에서 비지도학습인 차원 축소, 피처 추출 등을 구현한 클래스 역시 대부분 fit()과 transform()을 적용 비지도학습과 피처 추출에서 fit()은 지도학습의 fit() 같이 학습을 의미하는 것이 아니라, 입력 데이터의 형태에 맞춰 데이터를 변환하기 위한 사전 구조를 맞추는 작업 fit()으로 변환을 위한 사전 구조를 맞추면 이후 입력 데이터의 차원 변환, 클러스터링, 피처 추출 등의 실제 작업은 transform()으로 수행 사이킷런은 fit()과 transform()을 하나로 결합한 fit_transform()도 함께 제공 2. 사이킷런의 주요 모듈 교재 참고 3. 내장된 예제 데이터 세트 분류나 회귀 연습용 예제 데이터 API명 설명 datasets.load_boston() 회귀 용도이며, 미국 보스턴의 집 피처들과 가격에 대한 데이터 세트 datasets.load_breast_cancer 분류 용도이며, 위스콘신 유방암 피처들과 악성/음성 레이블 데이터 세트 datasets.load_diabetes 회귀 용도이며, 당뇨 데이터 세트 datasets.load_digits() 분류 용도이며, 0에서 9까지 숫자 이미지 픽셀 데이터 세트 datasets.load_iris() 분류 용도이며, 붓꽃에 대한 피처를 가진 데이터 세트 - fetch 계열 명령은 데이터 용량이 커서 인터넷에서 내려받는 방식으로 사용 - fetch_covtype(): 회귀분석용 토지 조사 자료 - fetch_20newsgroups(): 뉴스 그룹 텍스트 자료 - fetch_lfw_people(): 얼굴 이미지 자료 - fetch_lfw_pairs(): 얼굴 이미지 자료 - fetch_rcv1(): 로이터 뉴스 말뭉치 - fetch_mldata(): ML 웹사이트에서 다운로드 - 분류와 클러스터링을 위한 표본 데이터 생성시 API명 설명 datasets.make_classifications() 분류를 위한 데이터 세트를 만듦, 높은 상관도, 불필요한 속성 등의 노이즈 효과를 위한 데이터를 무작위로 생성 datasets.make_blobs() 클러스터링을 위한 데이터 세트를 무작위로 생성, 군집 지정 개수에 따라 여러 클러스터링을 위한 데이터 세트를 쉽게 만듦 사이킷런에 내장된 데이터 딕셔너리 엿보기 data: 피처 데이터 세트를 가리킴 target: 분류- 레이블값 데이터 세트, 회귀- 숫자 결괏값 데이터 세트 target_names: 개별 레이블 이름 feature_names: 피처 이름 DESCR: 데이터 세트에 관한 설명과 각 피처의 설명 설명 data, target: 넘파이 배열(ndarray) 타입 target_names, feature_names: 넘파이 배열 또는 파이썬 리스트(list) 타입 DESCR: 스트링타입 피처 데이터값을 반환받기 위해서는 내장 데이터 세트 API를 호출한 뒤, key 값을 지정하면 됨 # 붓꽃 데이터 세트 생성 from sklearn.datasets import load_iris iris_data = load_iris() print(type(iris_data)) &lt;class &#39;sklearn.utils.Bunch&#39;&gt; load_iris() API 반환 결과는 sklearn.utils.Bunch 클래스 (딕셔너리 자료형과 유사) keys = iris_data.keys() print('붓꽃 데이터 세트의 키들:', keys) 붓꽃 데이터 세트의 키들: dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;]) 데이터 키는 피처들의 데이터값을 가리킴 데이터 세트는 딕셔너리 형태로, 데이터 세트.data(or 데이터 세트[‘data’])로 추출 가능 # load_iris()가 반환하는 객체의 키가 가리키는 값 출력 print('\\n feature_names의 type:', type(iris_data.feature_names)) print('feature_names의 shape:', len(iris_data.feature_names)) print(iris_data.feature_names) print('\\n target_names의 type:', type(iris_data.target_names)) print('target_names의 shape:', len(iris_data.target_names)) print(iris_data.target_names) print('\\n data의 type:', type(iris_data.data)) print('data의 shape:', iris_data.data.shape) print(iris_data['data']) print('\\n target의 type:', type(iris_data.target)) print('target의 shape:', iris_data.target.shape) print(iris_data.target) feature_names의 type: &lt;class &#39;list&#39;&gt; feature_names의 shape: 4 [&#39;sepal length (cm)&#39;, &#39;sepal width (cm)&#39;, &#39;petal length (cm)&#39;, &#39;petal width (cm)&#39;] target_names의 type: &lt;class &#39;numpy.ndarray&#39;&gt; target_names의 shape: 3 [&#39;setosa&#39; &#39;versicolor&#39; &#39;virginica&#39;] data의 type: &lt;class &#39;numpy.ndarray&#39;&gt; data의 shape: (150, 4) [[5.1 3.5 1.4 0.2] [4.9 3. 1.4 0.2] [4.7 3.2 1.3 0.2] [4.6 3.1 1.5 0.2] [5. 3.6 1.4 0.2] [5.4 3.9 1.7 0.4] [4.6 3.4 1.4 0.3] [5. 3.4 1.5 0.2] [4.4 2.9 1.4 0.2] [4.9 3.1 1.5 0.1] [5.4 3.7 1.5 0.2] [4.8 3.4 1.6 0.2] [4.8 3. 1.4 0.1] [4.3 3. 1.1 0.1] [5.8 4. 1.2 0.2] [5.7 4.4 1.5 0.4] [5.4 3.9 1.3 0.4] [5.1 3.5 1.4 0.3] [5.7 3.8 1.7 0.3] [5.1 3.8 1.5 0.3] [5.4 3.4 1.7 0.2] [5.1 3.7 1.5 0.4] [4.6 3.6 1. 0.2] [5.1 3.3 1.7 0.5] [4.8 3.4 1.9 0.2] [5. 3. 1.6 0.2] [5. 3.4 1.6 0.4] [5.2 3.5 1.5 0.2] [5.2 3.4 1.4 0.2] [4.7 3.2 1.6 0.2] [4.8 3.1 1.6 0.2] [5.4 3.4 1.5 0.4] [5.2 4.1 1.5 0.1] [5.5 4.2 1.4 0.2] [4.9 3.1 1.5 0.2] [5. 3.2 1.2 0.2] [5.5 3.5 1.3 0.2] [4.9 3.6 1.4 0.1] [4.4 3. 1.3 0.2] [5.1 3.4 1.5 0.2] [5. 3.5 1.3 0.3] [4.5 2.3 1.3 0.3] [4.4 3.2 1.3 0.2] [5. 3.5 1.6 0.6] [5.1 3.8 1.9 0.4] [4.8 3. 1.4 0.3] [5.1 3.8 1.6 0.2] [4.6 3.2 1.4 0.2] [5.3 3.7 1.5 0.2] [5. 3.3 1.4 0.2] [7. 3.2 4.7 1.4] [6.4 3.2 4.5 1.5] [6.9 3.1 4.9 1.5] [5.5 2.3 4. 1.3] [6.5 2.8 4.6 1.5] [5.7 2.8 4.5 1.3] [6.3 3.3 4.7 1.6] [4.9 2.4 3.3 1. ] [6.6 2.9 4.6 1.3] [5.2 2.7 3.9 1.4] [5. 2. 3.5 1. ] [5.9 3. 4.2 1.5] [6. 2.2 4. 1. ] [6.1 2.9 4.7 1.4] [5.6 2.9 3.6 1.3] [6.7 3.1 4.4 1.4] [5.6 3. 4.5 1.5] [5.8 2.7 4.1 1. ] [6.2 2.2 4.5 1.5] [5.6 2.5 3.9 1.1] [5.9 3.2 4.8 1.8] [6.1 2.8 4. 1.3] [6.3 2.5 4.9 1.5] [6.1 2.8 4.7 1.2] [6.4 2.9 4.3 1.3] [6.6 3. 4.4 1.4] [6.8 2.8 4.8 1.4] [6.7 3. 5. 1.7] [6. 2.9 4.5 1.5] [5.7 2.6 3.5 1. ] [5.5 2.4 3.8 1.1] [5.5 2.4 3.7 1. ] [5.8 2.7 3.9 1.2] [6. 2.7 5.1 1.6] [5.4 3. 4.5 1.5] [6. 3.4 4.5 1.6] [6.7 3.1 4.7 1.5] [6.3 2.3 4.4 1.3] [5.6 3. 4.1 1.3] [5.5 2.5 4. 1.3] [5.5 2.6 4.4 1.2] [6.1 3. 4.6 1.4] [5.8 2.6 4. 1.2] [5. 2.3 3.3 1. ] [5.6 2.7 4.2 1.3] [5.7 3. 4.2 1.2] [5.7 2.9 4.2 1.3] [6.2 2.9 4.3 1.3] [5.1 2.5 3. 1.1] [5.7 2.8 4.1 1.3] [6.3 3.3 6. 2.5] [5.8 2.7 5.1 1.9] [7.1 3. 5.9 2.1] [6.3 2.9 5.6 1.8] [6.5 3. 5.8 2.2] [7.6 3. 6.6 2.1] [4.9 2.5 4.5 1.7] [7.3 2.9 6.3 1.8] [6.7 2.5 5.8 1.8] [7.2 3.6 6.1 2.5] [6.5 3.2 5.1 2. ] [6.4 2.7 5.3 1.9] [6.8 3. 5.5 2.1] [5.7 2.5 5. 2. ] [5.8 2.8 5.1 2.4] [6.4 3.2 5.3 2.3] [6.5 3. 5.5 1.8] [7.7 3.8 6.7 2.2] [7.7 2.6 6.9 2.3] [6. 2.2 5. 1.5] [6.9 3.2 5.7 2.3] [5.6 2.8 4.9 2. ] [7.7 2.8 6.7 2. ] [6.3 2.7 4.9 1.8] [6.7 3.3 5.7 2.1] [7.2 3.2 6. 1.8] [6.2 2.8 4.8 1.8] [6.1 3. 4.9 1.8] [6.4 2.8 5.6 2.1] [7.2 3. 5.8 1.6] [7.4 2.8 6.1 1.9] [7.9 3.8 6.4 2. ] [6.4 2.8 5.6 2.2] [6.3 2.8 5.1 1.5] [6.1 2.6 5.6 1.4] [7.7 3. 6.1 2.3] [6.3 3.4 5.6 2.4] [6.4 3.1 5.5 1.8] [6. 3. 4.8 1.8] [6.9 3.1 5.4 2.1] [6.7 3.1 5.6 2.4] [6.9 3.1 5.1 2.3] [5.8 2.7 5.1 1.9] [6.8 3.2 5.9 2.3] [6.7 3.3 5.7 2.5] [6.7 3. 5.2 2.3] [6.3 2.5 5. 1.9] [6.5 3. 5.2 2. ] [6.2 3.4 5.4 2.3] [5.9 3. 5.1 1.8]] target의 type: &lt;class &#39;numpy.ndarray&#39;&gt; target의 shape: (150,) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] 04. Model Selection 모듈 소개 사이킷런의 model_selection 모듈은 학습 데이터와 데이터 세트를 분리하거나, 교차 검증 분할 및 평가, Estimator의 하이퍼 파라미터를 튜닝하기 위한 함수와 클래스 제공 1. 학습/테스트 데이터 세트 분리- train_test_split()from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score iris = load_iris() dt_clf = DecisionTreeClassifier() train_data = iris.data train_label = iris.target dt_clf.fit(train_data, train_label) # 학습 데이터 세트로 예측 수행 pred = dt_clf.predict(train_data) print('예측 정확도:', accuracy_score(train_label, pred)) 예측 정확도: 1.0 정확도가 100%인 이유: 이미 학습한 학습 데이터 세트를 기반으로 예측했기 때문 예측을 수행한 데이터 세트는 학습한 학습용 데이터 세트가 아닌, 테스트 데이터 세트여야 함 train_test_split() 함수로 학습 데이터/테스트 데이터 세트 분리하기 파라미터 설명 test_size전체 데이터에서 테스트 데이터 세트 크기를 얼마로 샘플링할지 결정 (디폴트는 0.25, 즉 25%) train_size전체 데이터에서 학습용 데이터 세트 크기를 얼마로 샘플링할 것인가 결정(통상적으로 test_size를 사용하고, 해당 파라미터는 잘 사용하지 않음) shuffle데이터 분리 전, 데이터를 미리 섞을지 결정, 디폴트는 True, 데이터를 분산해서 좀 더 효율적인 학습 및 테스트 데이터 세트를 만드는 데 사용 random_staterandom_state는 호출할 때마다 동일한 학습/테스트용 데이터 세트를 생성하기 위해 주어지는 난수 값, train_test_split()는 호출 시 무작위로 데이터를 분리하므로 random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트용 데이터를 생성 → 연습 시에는 random_state에 일정 숫자값을 주어 변하지 않도록 함 train_test_split()의 반환값은 튜플 형태→ 학습용 데이터의 피처 데이터 세트, 테스트용 데이터의 피처 데이터 세트, 학습용 데이터의 레이블 데이터 세트, 테스트용 데이터의 레이블 데이터 세트가 순차적으로 반환 붓꽃 데이터 세트를 train_test_split()으로 테스트 데이터 세트를 전체의 30%, 학습 데이터 세트를 70%로 분리 from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split dt_clf = DecisionTreeClassifier() iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.3, random_state=121) dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) print('예측 정확도: &#123;0:4f&#125;'.format(accuracy_score(y_test, pred))) 예측 정확도: 0.955556 테스트 데이터로 예측을 수행한 결과 정확도가 약 95.56% 붓꽃 데이터는 데이터 양(150개)이 크지 않아 전체 30%인 텟트 데이터로 알고리즘 예측 성능을 판단하기 적절하지 않음 3. 교차 검증 과적합(Overfitting): 모델이 학습 데이터에만 과도하게 최적화되어, 실제 예측을 다른 데이터로 수행할 경우 예측 성능이 과도하게 떨어지는 것 과적합을 방지하기 위해 교차 검증을 이용해 다양한 학습과 평가를 수행 K 폴드 교차 검증 K 폴드 교차 검증: 가장 보편적으로 사용되는 교차 검증 기법 K개의 데이터 폴드 세트를 만들어서 K번마늠 각 폴드 세트에 학습과 검증 평가를 반복적으로 수행하는 방법 사이킷런에서는 K 폴드 교차 검증 프로세스를 구현하기 위해 KFold와 StratifiedKFold 클래스를 제공 from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.model_selection import KFold import numpy as np iris = load_iris() features = iris.data label = iris.target df_clf = DecisionTreeClassifier(random_state=156) # 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성 kfold = KFold(n_splits=5) cv_accuracy = [] print('붓꽃 데이터 세트 크기:', features.shape[0]) 붓꽃 데이터 세트 크기: 150 KFold(n_splits=5)로 KFold 객체를 생성했으니 생성된 KFold 객체의 split()을 호출해 전체 붓꽃 데이터를 5개의 폴드 데이터 세트로 분리 학습용 데이터 세트는 120개 검증 테스트 데이터 세트는 30개 KFold 객체는 split()을 호출하면 학습용/검증용 데이터로 분할할 수 있는 인덱스를 반환 n_iter = 0 # KFold 객체의 split()를 호출하면 폴드별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환 for train_index, test_index in kfold.split(features): # KFold.split()으로 반환된 인덱스로 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) n_iter += 1 # 반복 시마다 정확도 측정 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#&#123;0&#125; 교차 검증 정확도: &#123;1&#125;, 학습 데이터 크기: &#123;2&#125;, 검증 데이터 크기: &#123;3&#125;' .format(n_iter, accuracy, train_size, test_size)) print('#&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index)') cv_accuracy.append(accuracy) # 개별 iteration별 정확도를 합하여 평균 정확도 계산 print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy)) #1 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30 #&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index) #2 교차 검증 정확도: 1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30 #&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index) #3 교차 검증 정확도: 0.8333, 학습 데이터 크기: 120, 검증 데이터 크기: 30 #&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index) #4 교차 검증 정확도: 0.9333, 학습 데이터 크기: 120, 검증 데이터 크기: 30 #&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index) #5 교차 검증 정확도: 0.8333, 학습 데이터 크기: 120, 검증 데이터 크기: 30 #&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;.format(n_iter, test_index) ## 평균 검증 정확도: 0.91998 Stratified K 폴드 Stratified K 폴드: 불균형한 분포도를 가진 레이블 데이터 집합을 위한 K 폴드 방식 불균형한 분포도: 특정 레이블 값이 특이하게 많거나 매우 적어서 값의 분포가 한쪽으로 치우치는 것 e.g) 대출 사기 데이터처럼 대출 사기 1값이 매우 적고, 대부분 데이터가 정상 대출 0인 경우 import pandas as pd iris = load_iris() iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names) iris_df['label']=iris.target iris_df['label'].value_counts() 2 50 1 50 0 50 Name: label, dtype: int64 레이블 값은 0, 1, 2 값 모두 50개로 동일 Setosa, Versicolor, Virginica 품종 모두 50개 # 분할 파라미터 설정 = 3개 kfold = KFold(n_splits=3) # 교차 검증 횟수를 세기 위한 변수 n_iter=0 # 반복문을 통해 교차 검증 수행 for train_index, test_index in kfold.split(iris_df): n_iter += 1 label_train= iris_df['label'].iloc[train_index] label_test= iris_df['label'].iloc[test_index] print('## 교차 검증: &#123;0&#125;'.format(n_iter)) print('학습 레이블 데이터 분포:\\n', label_train) ## 교차 검증: 1 학습 레이블 데이터 분포: 50 1 51 1 52 1 53 1 54 1 .. 145 2 146 2 147 2 148 2 149 2 Name: label, Length: 100, dtype: int32 ## 교차 검증: 2 학습 레이블 데이터 분포: 0 0 1 0 2 0 3 0 4 0 .. 145 2 146 2 147 2 148 2 149 2 Name: label, Length: 100, dtype: int32 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 0 1 0 2 0 3 0 4 0 .. 95 1 96 1 97 1 98 1 99 1 Name: label, Length: 100, dtype: int32 from sklearn.model_selection import StratifiedKFold skf = StratifiedKFold(n_splits=3) n_iter=0 for train_index, test_index in skf.split(iris_df, iris_df['label']): n_iter += 1 label_train= iris_df['label'].iloc[train_index] label_test= iris_df['label'].iloc[test_index] print('## 교차 검증: &#123;0&#125;'.format(n_iter)) print('학습 레이블 데이터 분포:\\n', label_train.value_counts()) print('검증 레이블 데이터 분포:\\n', label_test.value_counts()) ## 교차 검증: 1 학습 레이블 데이터 분포: 2 34 1 33 0 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 1 17 0 17 2 16 Name: label, dtype: int64 ## 교차 검증: 2 학습 레이블 데이터 분포: 1 34 2 33 0 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 17 0 17 1 16 Name: label, dtype: int64 ## 교차 검증: 3 학습 레이블 데이터 분포: 0 34 2 33 1 33 Name: label, dtype: int64 검증 레이블 데이터 분포: 2 17 1 17 0 16 Name: label, dtype: int64 출력 결과 학습 레이블과 검증 레이블 데이터값 분포도가 동일하게 할당 첫 번째 교차 검증: 학습 레이블은 0, 1, 2 값이 각각 33개 → 레이블별로 동일하게 할당 검증 레이블: 0, 1, 2 값이 각각 17개 → 레이블별로 동일하게 할당 → 위와 같이 분할되어야 레이블값 모두 학습 가능하며, 이에 기반해 검증할 수 있음 dt_clf = DecisionTreeClassifier(random_state=156) skfold = StratifiedKFold(n_splits=3) n_iter=0 cv_accuracy=[] # StratifiedKFold의 split() 호출 시, 반드시 레이블 데이터 세트도 추가 입력 필요 for train_index, test_index in skfold.split(features, label): # split()으로 반환된 인덱스를 이용해 학습용, 검증용 테스트 데이터 추출 X_train, X_test = features[train_index], features[test_index] y_train, y_test = label[train_index], label[test_index] # 학습 및 예측 dt_clf.fit(X_train, y_train) pred = dt_clf.predict(X_test) # 반복 시마다 정확도 측정 n_iter += 1 accuracy = np.round(accuracy_score(y_test, pred), 4) train_size = X_train.shape[0] test_size = X_test.shape[0] print('\\n#&#123;0&#125; 교차 검증 정확도 :&#123;1&#125;, 학습 데이터 크기: &#123;2&#125;, 검증 데이터 크기: &#123;3&#125;' .format(n_iter, accuracy, train_size, test_size)) print('#&#123;0&#125; 검증 세트 인덱스: &#123;1&#125;'.format(n_iter, test_index)) cv_accuracy.append(accuracy) # 교차 검증별 정확도 및 평균 정확도 계산 print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4)) print('## 평균 검증 정확도:', np.mean(cv_accuracy)) #1 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #1 검증 세트 인덱스: [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115] #2 교차 검증 정확도 :0.94, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #2 검증 세트 인덱스: [ 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132] #3 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50 #3 검증 세트 인덱스: [ 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149] ## 교차 검증별 정확도: [0.98 0.94 0.98] ## 평균 검증 정확도: 0.9666666666666667 3개의 Stratified K 폴드로 교차 검증한 결과: 평균 검증 정확도가 약 96.04%로 측정 Stratified K 폴드의 경우, 원본 데이터의 레이블 분포도 특성을 반영한 학습/검증 데이터 세트를 만들 수 있으므로 왜곡된 레이블 데이터 세트에서는 반드시 Stratified K 폴드를 이용해 교차 검증해야 함 분류(Classification)의 교차 검증은 K 폴드 대신, Stratified K 폴드로 분할되어야 함 회귀(Regression)에서는 Stratified K 폴드가 지원되지 않음 ← 회귀 결정값은 이산값 형태 레이블이 아니라 연속된 숫자값이므로 결정값별로 분포를 정하는 의미가 없음 교차 검증을 보다 간편하게 - cross_val_score( ) 사이킷런은 교차 검증을 좀 더 편리하게 수행할 수 있게 해주는 API를 제공 대표적인 API: cross_val_score() KFold로 데이터를 학습하고 예측하는 코드를 보면 ① 폴드 세트를 설정하고 ② for 루프에서 반복으로 학습 및 테스트 데이터의 인덱스를 추출한 뒤 ③ 반복적으로 학습과 예측을 수행하고 예측 성능을 반환 cross_val_score( )는 위 과정을 한 번에 수행해주는 API cross_val_score( ) API의 선언 형태cross_val_score(estimator, X, y=None, scoring=None, cv=None, n_jobs=1, verbose=0, fit_params=None, pre_dispatch='2*n_jobs') 주요 파라미터 esmitator : 사이킷런의 분류 알고리즘 클래스인 Classifier 또는 회귀 알고리즘 클래스인 Regressor를 의미 X : 피처 데이터 세트 y : 레이블 데이터 세트 scoring : 예측 성능 평가 지표를 기술 cv : 교차 검증 폴드 수 cross_val_score( ) 수행 후 반환 값은 scoring 파라미터로 지정된 성능 지표 측정값을 배열 형태로 반환 cross_val_score( )는 classifier가 입력되면 Stratified K 폴드 방식으로 레이블값의 분포에 따라 학습/테스트 세트를 분할 cf) 회귀인 경우에는 Stratified K 폴드 방식으로 분할할 수 없으므로 K 폴드 방식으로 분할 from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import cross_val_score , cross_validate from sklearn.datasets import load_iris iris_data = load_iris() dt_clf = DecisionTreeClassifier(random_state=156) data = iris_data.data label = iris_data.target # 성능 지표는 정확도(accuracy) , 교차 검증 세트 는 3개 scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3) print('교차 검증별 정확도:',np.round(scores, 4)) print('평균 검증 정확도:', np.round(np.mean(scores), 4)) 교차 검증별 정확도: [0.98 0.94 0.98] 평균 검증 정확도: 0.9667 cross_val_score( )는 cv로 지정된 횟수만큼 scoring 파라미터로 지정된 평가 지표로 평가 결괏값을 배열로 반환, 일반적으로 이를 평균해 평가 수치로 사용 cross_val_score( ) API는 내부에서 Estimator를 학습(fit), 예측(predict), 평가(evaluation)시켜주므로 간단하게 교차 검증을 수행할 수 있음 cross_val_score( )와 앞 예제의 StratifiedKFold의 수행 결과를 비교해 보면 각 교차 검증별 정확도와 평균 검증 정확도가 모두 동일 → cross_val_score( )가 내부적으로 StratifiedKFold를 이용하기 때문 4. GridSearchCV - 교차 검증과 최적 하이퍼 파라미터 튜닝을 한 번에 사이킷런은 GridSearchCV API를 이용해 Classifier나 Regressor와 같은 알고리즘에 사용되는 하이퍼 파라미터를 순차적으로 입력하면서 편리하게 최적의 파라미터를 도출할 수 있는 방안을 제공(Grid는 격자라는 뜻으로, 촘촘하게 파라미터를 입력하면서 테스트를 하는 방식) e.g) 결정 트리 알고리즘의 여러 하이퍼 파라미터를 순차적으로 변경하면서 최고 성능을 가지는 파라미터 조합을 찾고자 한다면 다음과 같이 파라미터의 집합을 만들고 이를 순차적으로 적용하면서 최적화를 수행하면 됨 cf) 데이터 핸들링 - 피처 엔지니어링, ML 모형 핸들링 - 하이퍼 파라미터 튜닝 grid_parameters = &#123;'max_depth':[1, 2, 3], 'min_samples_split':[2,3] &#125; 6회에 걸쳐 파라미터를 순차적으로 바꿔 실행하면서 최적의 파라미터와 수행 결과를 도출 순번 max_depth min_samples_split 1 1 2 2 1 3 3 2 2 4 2 3 5 3 2 6 3 3 GridSearchCV: 사용자가 튜닝하고자 하는 여러 종류의 하이퍼 파라미터를 다양하게 테스트하면서 최적의 파라미터를 편리하게 찾게 해주지만 동시에 순차적으로 파라미터를 테스트하므로 수행시간이 상대적으로 오래 걸림 이 경우, 순차적으로 6회에 걸쳐 하이퍼 파라미터를 변경하면서 교차 검증 데이터 세트에 수행 성능을 측정 CV가 3회라면 개별 파라미터 조합마다 3개의 폴딩 세트를 3회에 걸쳐 학습/평가해 평균값으로 성능을 측정 6개의 파라미터 조합이라면 총 CV 3회 * 6개 파라미터 조합 = 18회의 학습/평가가 이뤄짐 GridSearchCV 클래스의 생성자로 들어가는 주요 파라미터 estimator : classifier, regressor, pipeline이 사용될 수 있다. param_grid : key + 리스트 값을 가지는 딕셔너리가 주어짐, estimator의 튜닝을 위해 파라미터명과 사용될 여러 파라미터 값을 지정 scoring : 예측 성능을 측정할 평가 방법을 지정, 보통은 사이킷런의 성능 평가 지표를 지정하는 문자열(예 : 정확도의 경우 ‘accuracy’)로 지정하나 별도의 성능 평가 지표 함수도 지정 가능 cv : 교차 검증을 위해 분할되는 학습 / 테스트 세트의 갯수를 지정 refit : 디폴트가 True이며 True로 생성 시 가장 최적의 하이퍼 파라미터를 찾은 뒤 입력된 esitmator 객체를 해당 하이퍼 파라미터로 재학습시킴 from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import GridSearchCV # 데이터를 로딩하고 학습 데이터와 테스트 데이터 분리 iris_data = load_iris() X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=121) dtree = DecisionTreeClassifier() ## 파라미터를 딕셔너리 형태로 설정 parameters = &#123;'max_depth': [1,2,3], 'min_samples_split': [2,3]&#125; 학습 데이터 세트를 GridSearchCV 객체의 fit(학습 데이터 세트) 메서드에 인자로 입력 GridSearchCV 객체의 fit 메서드를 수행하면 학습 데이터를 cv에 기술된 폴딩 세트로 분할해 param_grid에 기술된 하이퍼 파라미터를 순차적으로 변경하면서 학습/평가를 수행하고 결과를 cv_results_(gridsearchcv의 결과 세트, 딕셔너리 형태로 key값과 리스트 형태의 value값을 가짐) 속성에 기록 import pandas as pd # param_grid의 하이퍼 파라미터를 3개의 train, test set fold로 나누어 테스트 수행 설정 ## refit=True가 default, True인 경우 가장 좋은 파라미터 설정으로 재학습시킴 grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True, return_train_score=True) # 붓꽃 학습 데이터로 param_grid의 하이퍼 파라미터를 순차적으로 학습/평가 grid_dtree.fit(X_train, y_train) # GridSearchCV 결과를 추출해 DataFrame으로 변환 scores_df = pd.DataFrame(grid_dtree.cv_results_) scores_df[['params', 'mean_test_score', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; params mean_test_score rank_test_score split0_test_score split1_test_score split2_test_score 0 {'max_depth': 1, 'min_samples_split': 2} 0.700000 5 0.700 0.7 0.70 1 {'max_depth': 1, 'min_samples_split': 3} 0.700000 5 0.700 0.7 0.70 2 {'max_depth': 2, 'min_samples_split': 2} 0.958333 3 0.925 1.0 0.95 3 {'max_depth': 2, 'min_samples_split': 3} 0.958333 3 0.925 1.0 0.95 4 {'max_depth': 3, 'min_samples_split': 2} 0.975000 1 0.975 1.0 0.95 5 {'max_depth': 3, 'min_samples_split': 3} 0.975000 1 0.975 1.0 0.95 총 6개의 결과를 볼 수 있으며, 하이퍼 파라미터 max_depth와 min_samples_split을 순차적으로 6번 변경하면서 학습 및 평가를 수행했음을 나타냄 맨 마지막에서 두 번째 행을 보면, 평가 결과 예측 성능이 1위라는 의미 split0_test_score, split1_test_score, split2_test_score는 CV가 3인 경우, 즉 3개의 폴딩 세트에서 각각 테스트한 성능 수치 mean_test_score는 이 세 개 성능 수치를 평균화한 것 GridSearchCV 객체의 fit()을 수행하면, 최고 성능을 나타낸 하이퍼 파라미터 값과 그때의 평가 결괏값이 best_params, best_score_ 속성에 기록 print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_) print('GridSearchCV 최고 정확도: &#123;0:4f&#125;'.format(grid_dtree.best_score_)) GridSearchCV 최적 파라미터: &#123;&#39;max_depth&#39;: 3, &#39;min_samples_split&#39;: 2&#125; GridSearchCV 최고 정확도: 0.975000 GridSearchCV 객체의 생성 파라미터로 refit=True가 default refit = True면 GridSearchCV가 최적 성능을 나타내는 하이퍼 파라미터로 Estimator를 학습해 best_estimator_로 저장 # GridSearchCV refit으로 이미 학습된 estimator 반환 estimator = grid_dtree.best_estimator_ # GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습됨 pred = estimator.predict(X_test) print('테스트 데이터 세트 정확도: &#123;0:4f&#125;'.format(accuracy_score(y_test, pred))) 테스트 데이터 세트 정확도: 0.966667 05. 데이터 전처리 사이킷런 ML 알고리즘 적용 전, 알아야 할 사항 결손값(NaN, Null 값은 허용되지 않음) → 고정된 다른 값으로 변환해야 함 Null 값이 얼마 되지 않으면 피처의 평균값 등으로 간단히 대체 가능 단, Null 값이 대부분이라면 해당 피처를 드롭하는 편이 좋음 사이킷런 머신러닝 알고리즘은 문자열 값을 입력 값으로 허용하지 않음 모든 문자열 값은 인코딩하여 숫자형으로 변환해야 함 문자열 피처: 카테고리형 피처, 텍스트형 피처 1. 데이터 인코딩 레이블 인코딩(Label encoding) 카테고리 피처를 코드형 숫자 값으로 변환하는 것 01, 02(문자형)이 아닌 1, 2와 같은 숫자형 값으로 변환해야 함 사이킷런의 레이블 인코딩은 LabelEncoder 클래스로 구현 from sklearn.preprocessing import LabelEncoder items=['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서'] # LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행 encoder = LabelEncoder() encoder.fit(items) labels = encoder.transform(items) print('인코딩 변환값:',labels) 인코딩 변환값: [0 1 4 5 3 3 2 2] # 문자열 값이 어떤 숫자 값으로 인코딩됐는지 알고 싶을 경우 print('인코딩 클래스:',encoder.classes_) # 인코딩된 값을 디코딩하기 print('디코딩 원본 값:',encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3])) 인코딩 클래스: [&#39;TV&#39; &#39;냉장고&#39; &#39;믹서&#39; &#39;선풍기&#39; &#39;전자레인지&#39; &#39;컴퓨터&#39;] 디코딩 원본 값: [&#39;전자레인지&#39; &#39;컴퓨터&#39; &#39;믹서&#39; &#39;TV&#39; &#39;냉장고&#39; &#39;냉장고&#39; &#39;선풍기&#39; &#39;선풍기&#39;] 데이터 인코딩 단점 숫자 값으로 변환되어 몇몇 ML 알고리즘에서 크고 작음의 특성이 반영될 수 있음 원-핫 인코딩(One-Hot Encoding) 피처 값 유형에 따라 새로운 피처를 추가하여, 고유값에 해당하는 칼럼에만 1을 표시하고 나머지 칼럼에는 0을 표시하는 방식 사이킷런에서 OneHotEncoder 클래스로 쉽게 변환 가능 주의할 점 OneHotEncoder로 변환하기 전에 모든 문자열 값이 숫자형 값으로 변환되어야 함 입력값으로 2차원 데이터가 필요하다는 점 참고: 딥 러닝을 이용한 자연어 처리 입문 from sklearn.preprocessing import OneHotEncoder import numpy as np items=['TV', '냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서'] # 숫자값 변환을 위해 LabelEncoder로 변환 encoder = LabelEncoder() encoder.fit(items) labels = encoder.transform(items) # 2차원 데이터로 변환 labels = labels.reshape(-1, 1) # 원-핫 인코딩 적용 oh_encoder = OneHotEncoder() oh_encoder.fit(labels) oh_labels = oh_encoder.transform(labels) print('원-핫 인코딩 데이터') print(oh_labels.toarray()) print('원-핫 인코딩 데이터 차원') print(oh_labels.shape) 원-핫 인코딩 데이터 [[1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0.] [0. 0. 0. 0. 0. 1.] [0. 0. 0. 1. 0. 0.] [0. 0. 0. 1. 0. 0.] [0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0.]] 원-핫 인코딩 데이터 차원 (8, 6) # get_dummies()를 이용하면 바로 변환 가능 import pandas as pd df = pd.DataFrame(&#123;'item':['TV', '냉장고','전자렌지','컴퓨터','선풍기','선풍기','믹서','믹서']&#125;) pd.get_dummies(df) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; item_TV item_냉장고 item_믹서 item_선풍기 item_전자렌지 item_컴퓨터 0 1 0 0 0 0 0 1 0 1 0 0 0 0 2 0 0 0 0 1 0 3 0 0 0 0 0 1 4 0 0 0 1 0 0 5 0 0 0 1 0 0 6 0 0 1 0 0 0 7 0 0 1 0 0 0 2. 피처 스케일링과 정규화 피처 스케일링(feature scaling) 서로 다른 변수값 범위를 일정한 수준으로 맞추는 작업 표준화(Standardization)와 정규화(Normalization)가 있음 표준화 데이터 피처 각각 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환 xi_new = $\\frac{xi - mean(x)}{stdev(x)}$ 정규화 서로 다른 피처 크기를 통일하기 위해 크기 변환 xi_new = $\\frac{xi - min(x)}{max(x) - min(x)}$ 사이킷런 전처리에서 제공하는 Normalizer 모듈과 일반적인 정규화는 약간의 차이가 있음 선형대수에서의 정규화 개념이 적용, 개별 백터 크기를 맞추기 위해 변환 xi_new = $\\frac{xi}{xi^2^ + yi^2^ + zi^2^}$ (분모 루트, 아래 사진 참고) 3. StandardScaler 표준화를 쉽게 지원하기 위한 클래스 개별 피처를 평균이 0이고 분산이 1인 값으로 변환해줌 사이킷런에서 구현한 RBF 커널을 이용하는 서포트 백터 머신, 선형 회귀, 로지스틱 회귀는 가우시안 분포를 가지고 있다는 가정 하에 구현됐기 때문에 사전에 표준화를 적용하는 것이 예측 성능 향상에 중요한 요소가 됨 from sklearn.datasets import load_iris import pandas as pd # 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환 iris = load_iris() iris_data = iris.data iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names) print('feature들의 평균값') print(iris_df.mean()) print('\\nfeature들의 분산값') print(iris_df.var()) feature들의 평균값 sepal length (cm) 5.843333 sepal width (cm) 3.057333 petal length (cm) 3.758000 petal width (cm) 1.199333 dtype: float64 feature들의 분산값 sepal length (cm) 0.685694 sepal width (cm) 0.189979 petal length (cm) 3.116278 petal width (cm) 0.581006 dtype: float64 StandardScaler를 이용해 각 피처를 한 번에 표준화해 변환 from sklearn.preprocessing import StandardScaler # StandardScaler 객체 생성 scaler = StandardScaler() # StandardScaler로 데이터 세트 변환, fit()과 transform() 호출 scaler.fit(iris_df) iris_scaled = scaler.transform(iris_df) # transform() 시, 스케일 변환된 데이터 세트가 Numpy ndarray로 반환되 이를 DataFrame으로 변환 iris_df_scaled = pd.DataFrame(data=iris_scaled, columns=iris.feature_names) print('feature들의 평균값') print(iris_df_scaled.mean()) print('\\nfeature들의 분산값') print(iris_df_scaled.var()) feature들의 평균값 sepal length (cm) -1.690315e-15 sepal width (cm) -1.842970e-15 petal length (cm) -1.698641e-15 petal width (cm) -1.409243e-15 dtype: float64 feature들의 분산값 sepal length (cm) 1.006711 sepal width (cm) 1.006711 petal length (cm) 1.006711 petal width (cm) 1.006711 dtype: float64 모든 칼럼 값 평균이 0에 아주 가까운 값으로, 분산은 1에 아주 가까운 값으로 변환됨을 확인할 수 있음 3. MinMaxScaler MinMaxScaler 데이터값을 0과 1 사이 범위 값으로 변환 음수 값이 있으면 -1에서 1값으로 변환 데이터 분포가 가우시안 분포가 아닐 경우에는 Min, Max Scale 적용 가능 from sklearn.preprocessing import MinMaxScaler # MinMaxScaler 객체 생성 scaler = MinMaxScaler() # MinMaxScaler로 데이터 세트 변환, fit()과 transform()호출 scaler.fit(iris_df) iris_scaled=scaler.transform(iris_df) # transform() 시 스케일 변환된 데이터 세트가 NumPy ndarry로 변환돼 이를 DataFrame으로 변환 iris_df_scaled=pd.DataFrame(data=iris_scaled, columns=iris.feature_names) print('feature 최솟값') print(iris_df_scaled.min()) print('\\nfeature 최댓값') print(iris_df_scaled.max()) feature 최솟값 sepal length (cm) 0.0 sepal width (cm) 0.0 petal length (cm) 0.0 petal width (cm) 0.0 dtype: float64 feature 최댓값 sepal length (cm) 1.0 sepal width (cm) 1.0 petal length (cm) 1.0 petal width (cm) 1.0 dtype: float64 4. 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점 StandardScaler나 MinMaxScaler와 같은 Scaler 객체로 데이터 스케일링 변환 시, fit(), transform(), fit_transform() 메소드를 이용 fit(): 데이터 변환을 위한 기준 정보 설정(최댓값, 최솟값 등) transform(): 설정된 정보를 이용해 데이터 변환 fir_transform(): fit()과 transform()을 한 번에 적용하는 기능 # 테스트 데이터에 fit()을 적용할 때 발생하는 문제 알아보기 from sklearn.preprocessing import MinMaxScaler import numpy as np # 학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성 # Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경 train_array = np.arange(0,11).reshape(-1,1) test_array = np.arange(0,6).reshape(-1,1) # 학습 데이터인 train_array부터 MinMaxScaler를 이용해 변환 ## MinMaxScaler 객체에 별도 feature_range 파라미터 값을 지정하지 않으면 0~1 값으로 변환 scaler = MinMaxScaler() ## fit()하면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정됨 scaler.fit(train_array) ## 1/10 scale로 train_arrau 데이터 변환, 원본 10 → 1로 변환 train_scaled = scaler.transform(train_array) print('원본 train_array 데이터:', np.round(train_array.reshape(-1),2)) print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1),2)) # 테스트 데이터 세트를 변환: fit()을 호출해 스케일링 기준 정보를 적용한 뒤, transform()을 수행한 결과 확인 ## MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터 최솟값이 0, 최댓값이 5로 설정 scaler.fit(test_array) ## 1/5 scale로 test_array 데이터 변환. 원본 5 → 1로 변환 test_scaled = scaler.transform(test_array) ## test_array의 scale 변환 출력 print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1),2)) print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1),2)) 원본 train_array 데이터: [ 0 1 2 3 4 5 6 7 8 9 10] Scale된 train_array 데이터: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] 원본 test_array 데이터: [0 1 2 3 4 5] Scale된 test_array 데이터: [0. 0.2 0.4 0.6 0.8 1. ] 출력 결과를 확인하면 학습 데이터와 테스트 데이터 스케일링이 맞지 않음을 알 수 있음 학습 데이터와 테스트 데이터의 서로 다른 원본값이 동일한 값으로 변환되는 결과 초래 머신러닝 모델은 학습 데이터를 기반으로 학습됨 테스트 데이터는 학습 데이터의 스케일링 기준에 따라야 하며, 테스트 데이터의 1값은 학습 데이터와 동일하게 0.1값으로 변환되어야 함 따라서, 테스트 데이터에 다시 fit()을 적용해서는 안 되며, 학습 데이터로 이미 fit()이 적용된 Scaler 객체를 이용해 transform()으로 변환해야 함 # 재시도: 테스트 데이터에 fit() 호출하지 않음 scaler = MinMaxScaler() scaler.fit(train_array) train_scaled = scaler.transform(train_array) print('원본 train_array 데이터:', np.round(train_array.reshape(-1),2)) print('Scale된 train_array 데이터:', np.round(train_scaled.reshape(-1),2)) # test_array에 Scale 변환할 때는 반드시 fit() 호출하지 않고 transform()만으로 변환 test_scaled = scaler.transform(test_array) print('\\n원본 test_array 데이터:', np.round(test_array.reshape(-1),2)) print('Scale된 test_array 데이터:', np.round(test_scaled.reshape(-1),2)) 원본 train_array 데이터: [ 0 1 2 3 4 5 6 7 8 9 10] Scale된 train_array 데이터: [0. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ] 원본 test_array 데이터: [0 1 2 3 4 5] Scale된 test_array 데이터: [0. 0.1 0.2 0.3 0.4 0.5] 06. 사이킷런으로 타이타닉 생존자 예측하기 타이타닉 탑승자 데이터 Passengerid : 탑승자 번호 survived : 생존 여부 0 : 사망 / 1 : 생존 pclass : 티켓의 선실 등급 sex : 성별 name :이름 Age : 나이 sibsp : 같이 탑승한 형제자매 또는 배우자 인원수 parch : 같이 탑승한 부모님 또는 어린이 인원수 ticket : 티켓 번호 fare : 요금 cabin : 선실 번호 embarked : 중간 정착 항구 C = Cherbourg, Q = Queenstown, S = Southampton import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline titanic_df = pd.read_csv('./titanic_train.csv') titanic_df.head(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S print('\\n ### 학습 데이터 정보 ### \\n') print(titanic_df.info()) ### 학습 데이터 정보 ### &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 PassengerId 891 non-null int64 1 Survived 891 non-null int64 2 Pclass 891 non-null int64 3 Name 891 non-null object 4 Sex 891 non-null object 5 Age 714 non-null float64 6 SibSp 891 non-null int64 7 Parch 891 non-null int64 8 Ticket 891 non-null object 9 Fare 891 non-null float64 10 Cabin 204 non-null object 11 Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.7+ KB None 데이터 확인 Range Index: DataFrame 인덱스의 범위, 891개의 로우로 구성 Column: 열 12개(object는 string 타입으로 보아도 무방) 사이킷런 머신러닝 알고리즘은 Null 값을 허용하지 않으므로, Null값 처리가 필요 DataFrame의 fillna() 함수로 Null 값을 평균 또는 고정값으로 변경 Age는 평균 나이, 나머지 칼럼은 ‘N’값으로 변경 titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True) titanic_df['Cabin'].fillna('N', inplace=True) titanic_df['Embarked'].fillna('N', inplace=True) print('데이터 세트 Null 값 개수:', titanic_df.isnull().sum().sum()) 데이터 세트 Null 값 개수: 0 # 남은 문자열 피처 값 분류 살펴보기 print('Sex 값 분포:\\n', titanic_df['Sex'].value_counts()) print('\\nCabin 값 분포 :\\n', titanic_df['Cabin'].value_counts()) print('\\nEmbarked 값 분포 :\\n', titanic_df['Embarked'].value_counts()) Sex 값 분포: male 577 female 314 Name: Sex, dtype: int64 Cabin 값 분포 : G6 4 B96 B98 4 C23 C25 C27 4 E101 3 F2 3 .. C32 1 C90 1 D49 1 B73 1 C46 1 Name: Cabin, Length: 147, dtype: int64 Embarked 값 분포 : S 644 C 168 Q 77 Name: Embarked, dtype: int64 결과 해석 Cabin(선실): Ndl 687건으로 가장 많은 것과 속성값이 정리되지 않은 것으로 확인 e.g) 여러 Cabin이 한 번에 표기된 값이 4건이나 됨 Cabin의 경우 선실 번호 중, 선실 등급을 나타내는 첫 번째 알파벳이 중요해 보임 # Cabin 속성 앞 문자만 추출하기 titanic_df['Cabin'] = titanic_df['Cabin'].str[:1] print(titanic_df['Cabin'].head(3)) 0 N 1 C 2 N Name: Cabin, dtype: object Women and Children First 시절로 성별에 따른 생존 비율 비교 titanic_df.groupby(['Sex', 'Survived'])['Survived'].count() Sex Survived female 0 81 1 233 male 0 468 1 109 Name: Survived, dtype: int64 Survived 칼럼: 레이블로서 결정 클래스 값(0: 사망, 1: 생존) 여성 314명 중, 233명(약 74.2%) 생존 남성 577명 중, 109명(약 18.8%) 생존 # Seaborn 패키지를 이용하여 성별 생존 비율 비교하기 sns.barplot(x='Sex', y='Survived', data=titanic_df) &lt;matplotlib.axes._subplots.AxesSubplot at 0x188fdd03610&gt; 부자와 가난한 사람 생존 확률 확인 객실 등급으로 부를 추출해보기 성별도 추가 # 객실 등급별 생존 확률 확인하기 sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df) &lt;matplotlib.axes._subplots.AxesSubplot at 0x188fe470130&gt; 결과 해석 여성의 경우, 일/이등실에 따른 생존 확률 차이는 적으나, 삼등실의 경우 생존 확률이 상대적으로 많이 떨어짐 남성의 경우, 일등실 생존 확률이 이/삼등실 생존 확률보다 월등히 높음 Age에 따른 생존 확률 알아보기 0 - 5: Baby 6 - 12: Child 13 - 18: Teenager 19 - 25: Student 26 - 35: Young Adult 36 - 60: Adult 61 - : Elderly ~ -1: Unknown (오류값) # 입력 age에 따라 구분값을 반환하는 함수 설정 # DataFrame의 apply lambda 식에 사용 def get_category(age): cat = '' if age &lt;= -1: cat = 'Unknown' elif age &lt;= 5: cat = 'Baby' elif age &lt;= 12: cat = 'Child' elif age &lt;= 18: cat = 'Teenager' elif age &lt;= 25: cat = 'Student' elif age &lt;= 35: cat = 'Young Adult' elif age &lt;= 60: cat = 'Adult' else: cat = 'Elderly' return cat # 막대그래프 크기 figure를 더 크게 설정 plt.figure(figsize=(10,6)) # X축의 값을 순차적으로 표시하기 위한 설정 group_names= ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Elderly'] # lambda 식에 위에서 생성한 get_category() 함수를 반환값으로 지정 titanic_df['Age_cat'] =titanic_df['Age'].apply(lambda x : get_category(x)) sns.barplot(x='Age_cat', y='Survived',hue ='Sex', data=titanic_df,order=group_names) titanic_df.drop('Age_cat',axis=1,inplace=True) 결과 해석 여자 baby의 경우 비교적 생존 확률 높음 여자 child는 다른 연령대에 비해 생존 확률 낮음 여자 elderly의 경우 생존 확률이 매우 높음 남은 문자열 카테고리 피처를 숫자형 카테고리 피처로 변환 인코딩: 사이킷런의 LabelEncoder 클래스를 이용하여 레이블 인코딩 적용 # 여러 칼럼을 encode_features() 함수를 생성해 한 번에 변환하기 from sklearn import preprocessing def encode_features(dataDF): features = ['Cabin', 'Sex', 'Embarked'] for feature in features: le = preprocessing.LabelEncoder() le = le.fit(dataDF[feature]) dataDF[feature] = le.transform(dataDF[feature]) return dataDF titanic_df = encode_features(titanic_df) titanic_df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris 1 22.0 1 0 A/5 21171 7.2500 7 3 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... 0 38.0 1 0 PC 17599 71.2833 2 0 2 3 1 3 Heikkinen, Miss. Laina 0 26.0 0 0 STON/O2. 3101282 7.9250 7 3 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) 0 35.0 1 0 113803 53.1000 2 3 4 5 0 3 Allen, Mr. William Henry 1 35.0 0 0 373450 8.0500 7 3 Sex, Cabin, Embarked 속성이 숫자형으로 바뀜 # Null 처리 함수 def fillna(df): df['Age'].fillna(df['Age'].mean(),inplace = True) df['Cabin'].fillna('N',inplace=True) df['Embarked'].fillna('N', inplace = True) df['Fare'].fillna(0,inplace = True) return df #머신러닝 알고리즘에 불필요한 속성 제거 def drop_features(df): df.drop(['PassengerId', 'Name', 'Ticket'],axis=1,inplace=True) return df #레이블 인코딩 수행 def format_features(df): df['Cabin'] = df['Cabin'].str[:1] features = ['Cabin','Sex','Embarked'] for feature in features: le = LabelEncoder() le = le.fit(df[feature]) df[feature] = le.transform(df[feature]) return df # 앞에서 설정한 데이터 전처리 함수 호출 def transform_features(df): df = fillna(df) df = drop_features(df) df= format_features(df) return df 원본 데이터 가공 위해 원본 csv 파일 재로딩 Survived 속성만 별로도 분리해 클래스 결정값 데이터 세트로 만들기 Survived 속성을 드롭해 피처 데이터 세트 만들기 생성된 데이터 세트에 transform_features()를 적용해 데이터 가공 #원본 데이터 재로딩하고, 피처 데이터 세트와 레이블 데이터 세트 추출. titanic_df = pd.read_csv(\"./titanic_train.csv\") y_titanic_df=titanic_df['Survived'] X_titanic_df=titanic_df.drop('Survived',axis=1) X_titanic_df = transform_features(X_titanic_df) # train_test_split() API를 이용해 별도의 테스트 데이터 세트 추출, 세트 크기는 전체의 20% 설정 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test=train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state=11) ML 알고리즘 결정 트리, 랜덤 포레스트, 로지스틱 회귀로 타이타닉 생존자 예측하기 결정 트리: DecisionTreeClassifier 클래스 랜덤 포레스트: RandomForestClassifier 클래스 로지스틱 회귀: LogisticRegression 클래스 제공 사이킷런 클래스를 이용해 train_test_split()으로 분리한 학습 데이터와 테스트 데이터를 기반으로 머신러닝 모델을 학습하고(fit) 예측할 것(predict) 예측 성능 평가 기준: 정확도 → accuracy_score() API 사용 random_state=11의 숫자는 예제 수행 시마다 같은 결과를 출력하기 위한 용도 from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score #결정트리, Random Forest, 로지스틱 회귀를 위한 사이킷런 Classifier 클래스 생성 dt_clf = DecisionTreeClassifier(random_state=11) rf_clf = RandomForestClassifier(random_state=11) lr_clf = LogisticRegression() #DecisionTreeClassifier 학습/예측/평가 dt_clf.fit(X_train, y_train) dt_pred = dt_clf.predict(X_test) print('DecisionTreeClassifier 정확도: &#123;0:.4f&#125;'.format(accuracy_score(y_test, dt_pred))) #RandomForestClassifier 학습/예측/평가 rf_clf.fit(X_train, y_train) rf_pred = rf_clf.predict(X_test) print('RandomForestClassifier 정확도:&#123;0:.4f&#125;'.format(accuracy_score(y_test, rf_pred))) # LogisticRegression 학습/예측/평가 lr_clf.fit(X_train, y_train) lr_pred = lr_clf.predict(X_test) print('LogisticRegression 정확도: &#123;0:.4f&#125;'.format(accuracy_score(y_test, lr_pred))) DecisionTreeClassifier 정확도: 0.7877 RandomForestClassifier 정확도:0.8547 LogisticRegression 정확도: 0.8492 최적화 작업을 수행하지 않았고, 데이터 양도 충분하지 않아 어떤 알고리즘이 가장 성능이 좋은지 평가할 수 없음 교차 검증을 위해 KFold 클래스(폴드 개수 5개로 설정), cross_val_score, GridSearchCV 클래스 모두 사용 from sklearn.model_selection import KFold def exec_kfold(clf, folds=5): # 폴드 세트를 5개인 KFold객체를 생성, 폴드 수만큼 예측결과 저장을 위한 리스트 객체 생성. kfold = KFold(n_splits=folds) scores = [] # KFold 교차 검증 수행. for iter_count , (train_index, test_index) in enumerate(kfold.split(X_titanic_df)): # X_titanic_df 데이터에서 교차 검증별로 학습과 검증 데이터를 가리키는 index 생성 X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index] y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index] # Classifier 학습, 예측, 정확도 계산 clf.fit(X_train, y_train) predictions = clf.predict(X_test) accuracy = accuracy_score(y_test, predictions) scores.append(accuracy) print(\"교차 검증 &#123;0&#125; 정확도: &#123;1:.4f&#125;\".format(iter_count, accuracy)) # 5개 fold에서의 평균 정확도 계산. mean_score = np.mean(scores) print(\"평균 정확도: &#123;0:.4f&#125;\".format(mean_score)) # exec_kfold 호출 exec_kfold(dt_clf , folds=5) 교차 검증 0 정확도: 0.7542 교차 검증 1 정확도: 0.7809 교차 검증 2 정확도: 0.7865 교차 검증 3 정확도: 0.7697 교차 검증 4 정확도: 0.8202 평균 정확도: 0.7823 평균 정확도: 약 78.23% cross_val_score() API로 교차 검증 수행 from sklearn.model_selection import cross_val_score scores = cross_val_score(dt_clf, X_titanic_df , y_titanic_df , cv=5) for iter_count,accuracy in enumerate(scores): print(\"교차 검증 &#123;0&#125; 정확도: &#123;1:.4f&#125;\".format(iter_count, accuracy)) print(\"평균 정확도: &#123;0:.4f&#125;\".format(np.mean(scores))) 교차 검증 0 정확도: 0.7430 교차 검증 1 정확도: 0.7753 교차 검증 2 정확도: 0.7921 교차 검증 3 정확도: 0.7865 교차 검증 4 정확도: 0.8427 평균 정확도: 0.7879 KFold와 평균 정확도가 다른 이유: cross_val_score가 stratifiedKFold를 이용해 폴드 세트를 분할하기 때문 - GridSearchCV를 이용해 DecisionTreeClassifier의 최적 하이퍼 파라미터를 찾고 예측 성능 측정하기 from sklearn.model_selection import GridSearchCV parameters = &#123;'max_depth':[2,3,5,10], 'min_samples_split':[2,3,5], 'min_samples_leaf':[1,5,8]&#125; grid_dclf = GridSearchCV(dt_clf , param_grid=parameters , scoring='accuracy' , cv=5) grid_dclf.fit(X_train , y_train) print('GridSearchCV 최적 하이퍼 파라미터 :',grid_dclf.best_params_) print('GridSearchCV 최고 정확도: &#123;0:.4f&#125;'.format(grid_dclf.best_score_)) best_dclf = grid_dclf.best_estimator_ # GridSearchCV의 최적 하이퍼 파라미터로 학습된 Estimator로 예측 및 평가 수행. dpredictions = best_dclf.predict(X_test) accuracy = accuracy_score(y_test , dpredictions) print('테스트 세트에서의 DecisionTreeClassifier 정확도 : &#123;0:.4f&#125;'.format(accuracy)) GridSearchCV 최적 하이퍼 파라미터 : &#123;&#39;max_depth&#39;: 3, &#39;min_samples_leaf&#39;: 5, &#39;min_samples_split&#39;: 2&#125; GridSearchCV 최고 정확도: 0.7992 테스트 세트에서의 DecisionTreeClassifier 정확도 : 0.8715 하이퍼 파라미터인 max_depth=3, min_samples_leaf=1, min_samples_split=2로 DecisionTreeClassifier를 학습시킨 뒤 예측 정확도가 약 87.15%로 향상→ 테스트용 데이터 세트가 작기 때문에 수치상으로 예측 성능이 많이 증가한 것으로 보임 07. 정리 사이킷런 많은 머신러닝 알고리즘 제공 쉽고 직관적인 API 프레임워크 편리하고 다양한 모듈 지원 머신러닝 애플리케이션 데이터의 가공 및 변환 과정의 처리 데이터를 학습 대이타와 테스트 데이터로 분리하는 데이터 시트 분리 작업 학습 데이터를 기반으로 머신러닝 알고리즘을 적용해 모델 학습 학습된 모델을 기반으로 테스트 데이터에 대한 예측 수행 예측된 결과값을 실제 결과값과 비교해 머신러닝 모델에 대한 평가 수행 데이터 전처리 작업 오류 데이터의 보정이나 결손(Null) 처리 등의 다양한 데이터 클렌징 작업 레이블 인코딩이나 원-핫 인코딩 같은 인코딩 작업 데이터의 스케일링/정규화 작업 등으로 머신러닝 알고리즘이 최적으로 수행되도록 사전에 데이터 처리 추가 머신러닝 모델은 학습 데이터 세트로 학습한 뒤 반드시 별도의 테스트 데이터 세트로 평가해야 함 테스트 데이터의 건수 부족이나 고정된 테스트 데이터 세트를 이용한 반복적인 모델의 학습과 평가는 해당 테스트 데이터 세트에만 치우치는 빈약한 머신러닝 모델을 만들 가능성이 높음 해결 방안 학습 데이터 세트를 학습 데이터와 검증 데이터로 구성된 여러 개의 폴드 세트로 분리해 교차검증 수행 (교차검증은 데이터셋이 적을 때, 많으면 시간이 너무 오래 걸림) 사이킷런은 교차 검증을 지원하기 위해 KFord, StratifiedKFold, cross_val_score 등의 다양한 클레스 함수를 제공 머신러닝 모델의 최적의 하이퍼 파라미터를 교차 검증을 통해 추출하기 위해 GridSearchCV를 제공","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"}],"tags":[{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"Titanic","slug":"Titanic","permalink":"https://ne-choi.github.io/tags/Titanic/"}],"author":"ne_choi"},{"title":"소비트렌드 코리아2020: 코로나19 발생 이후  색조 및 기초 화장품 수요 비교분석","slug":"Projects/Consumption_Trend_Analysis","date":"2020-11-20T00:00:00.000Z","updated":"2021-01-26T12:43:15.349Z","comments":true,"path":"/2020/11/20/Projects/Consumption_Trend_Analysis/","link":"","permalink":"https://ne-choi.github.io/2020/11/20/Projects/Consumption_Trend_Analysis/","excerpt":"","text":"setup, includeknitr::opts_chunk$set(echo &#x3D; TRUE, warning &#x3D; FALSE, message &#x3D; FALSE) KDX_Contest_2020_final_analysis코로나19 이후, 유의미한 소비 인사이트를 발굴하기 위해 뷰티 업종 소비 추이를 분석하였습니다. 해당 파일은 코드 위주의 파일이며 세부 내역은 최종 제출한 PPT로 확인 부탁드립니다. 1. 준비 작업1.1 패키지 설치 및 불러오기# 패키지 설치하기 install.packages(\"readxl\") install.packages(\"dplyr\") install.packages(\"tidyr\") install.packages(\"reshape2\") install.packages(\"ggplot2\") install.packages(\"lubridate\") install.packages(\"labeling\") install.packages(\"extrafont\") install.packages('devtools') devtools::install_github('bbc/bbplot') # 패키지 불러오기 library(readxl) library(dplyr) library(tidyr) library(reshape2) library(ggplot2) library(lubridate) library(labeling) library(extrafont) # 시각화 테마를 위한 bbplot 패키지 설치 if(!require(pacman))install.packages(&quot;pacman&quot;) pacman::p_load(&#39;dplyr&#39;, &#39;tidyr&#39;, &#39;gapminder&#39;, &#39;ggplot2&#39;, &#39;ggalt&#39;, &#39;forcats&#39;, &#39;R.utils&#39;, &#39;png&#39;, &#39;grid&#39;, &#39;ggpubr&#39;, &#39;scales&#39;, &#39;bbplot&#39;) 2. Mcorpotarion Data2.1 기초 &amp; 색조 화장품 엑셀 정리# 사용할 데이터만 정리하기(메이크업, 스킨케어) files &lt;- list.files(path &#x3D; &quot;data&#x2F;use&quot;, pattern &#x3D; &quot;*.xlsx&quot;, full.names &#x3D; T) products &lt;- sapply(files, read_excel, simplify &#x3D; FALSE) %&gt;% bind_rows(.id &#x3D; &quot;id&quot;) glimpse(products) 2.2 월별 추이 확인을 위한 전처리 및 시각화# 전체 필터 넣기 filter_products &lt;- group_by(products, 카테고리명, 구매날짜, 고객성별, 고객나이, 구매금액, 구매수) %&gt;% separate(구매날짜, into &#x3D; c(&quot;구매연월&quot;, &quot;삭제(일자)&quot;), sep &#x3D; 6) %&gt;% select(카테고리명, 구매연월, 고객성별, 고객나이, 구매금액, 구매수) head(filter_products, 2) # 성별&amp;나이 결측치 제거하기(성별 F, M, 나이 0 이상만 추출) nomiss_products &lt;- filter_products %&gt;% filter(!is.na(고객성별) &amp; !is.na(고객나이)) %&gt;% filter((고객성별 %in% c(&quot;F&quot;, &quot;M&quot;)), 고객나이 &gt; 0) head(nomiss_products) # &quot;메이크업 용품&quot; 카테고리 추출 cosmetics &lt;- filter(nomiss_products, 카테고리명 &#x3D;&#x3D; &quot;메이크업 용품&quot;) cosmetics # 월별 데이터 합계_메이크업 용품 summarise_cosmetics &lt;- cosmetics %&gt;% group_by(구매연월, 고객성별) %&gt;% summarise(금액합계 &#x3D; sum(구매금액)) summarise_cosmetics # &quot;스킨 케어&quot; 카테코리 추출 skincare &lt;- filter(nomiss_products, 카테고리명 &#x3D;&#x3D; &quot;스킨케어&quot;) skincare # 월별 데이터 합계_스킨케어 summarise_skincare &lt;- skincare %&gt;% group_by(구매연월, 고객성별) %&gt;% summarise(금액합계 &#x3D; sum(구매금액)) summarise_skincare # 시각화하기 ## &#39;단위: 억&#39; 적용 label_ko_num &#x3D; function(num)&#123; ko_num &#x3D; function(x)&#123; new_num &#x3D; x %&#x2F;% 100000000 return(paste(new_num, &#39;억&#39;, sep &#x3D; &#39;&#39;)) &#125; return(sapply(num, ko_num)) &#125; #색조 화장품(메이크업 용품)_월별 추이_ppt.12p library(ggplot2) graph_cosmetics &lt;- ggplot(summarise_cosmetics, aes(x &#x3D; 구매연월, y &#x3D; 금액합계, color &#x3D; 고객성별)) + geom_point(size &#x3D; 2) + scale_y_continuous(labels &#x3D; label_ko_num) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1, angle &#x3D; 45), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), legend.position &#x3D; &quot;bottom&quot;, axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + scale_colour_manual(values &#x3D; c(&quot;#EB3232&quot;, &quot;#FAAB18&quot;)) + bbc_style() graph_cosmetics # 기초 화장품(스킨케어)_월별 추이_ppt.12p graph_skincare &lt;- ggplot(summarise_skincare, aes(x &#x3D; 구매연월, y &#x3D; 금액합계, color &#x3D; 고객성별)) + geom_point(size &#x3D; 2) + scale_y_continuous(labels &#x3D; label_ko_num) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;,hjust &#x3D; 1, angle &#x3D; 45), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), legend.position &#x3D; &quot;bottom&quot;, axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + scale_colour_manual(values &#x3D; c(&quot;#EB3232&quot;, &quot;#FAAB18&quot;)) + bbc_style() graph_skincare 2.3 실제 분석을 위한 데이터 전처리 및 시각화# 성별&amp;나이 결측치 제거하기(성별 F, M, 나이 0 이상만 추출) nomiss_products &lt;- products %&gt;% filter(!is.na(고객성별) &amp; !is.na(고객나이)) %&gt;% filter((고객성별 %in% c(&quot;F&quot;, &quot;M&quot;)), 고객나이 &gt; 0) %&gt;% select(카테고리명, 구매날짜, 고객성별, 고객나이, OS유형, 구매금액, 구매수) # 비교값 만들기 compare_products &lt;- nomiss_products %&gt;% group_by(카테고리명, 구매날짜, 고객성별) %&gt;% summarise(금액합계 &#x3D; sum(구매금액)) head(compare_products) # 억 원 단위 생성 label_ko_num &#x3D; function(num)&#123; ko_num &#x3D; function(x)&#123; new_num &#x3D; x %&#x2F;% 100000000 return(paste(new_num, &#39;억&#39;, sep &#x3D; &#39;&#39;)) &#125; return(sapply(num, ko_num)) &#125; # 문자형 데이터 -&gt; 날짜 데이터로 전환 library(lubridate) final_products &lt;- compare_products %&gt;% mutate(구매일 &#x3D; ymd(구매날짜)) 시각화# 색조화장품(메이크업 용품) 데이터 시각화 _ppt.14p final_products cosmetics &lt;- final_products %&gt;% filter(카테고리명 &#x3D;&#x3D; &quot;메이크업 용품&quot;) font_import(pattern &#x3D; &quot;NanumSquare&quot;) # loadfonts(device &#x3D; &quot;win&quot;) theme_update(text &#x3D; element_text(family &#x3D; &quot;NanumSquare_ac Bold&quot;)) graph_cosmetics &lt;- ggplot(cosmetics, aes(x &#x3D; 구매일, y &#x3D; 금액합계, color &#x3D; 고객성별)) + geom_smooth() + geom_point(size &#x3D; 0.1) + scale_y_continuous(labels &#x3D; label_ko_num, breaks &#x3D; seq(0, 2000000000, by &#x3D; 250000000)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), ) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + scale_colour_manual(values &#x3D; c(&quot;#EB3232&quot;, &quot;#FAAB18&quot;)) + bbc_style() graph_cosmetics # 기초화장품(스킨케어) 데이터 시각화_ppt.14p skincare &lt;- final_products %&gt;% filter(카테고리명 &#x3D;&#x3D; &quot;스킨케어&quot;) font_import(pattern &#x3D; &quot;NanumSquare&quot;) # loadfonts(device &#x3D; &quot;win&quot;) theme_update(text &#x3D; element_text(family &#x3D; &quot;NanumSquare_ac Bold&quot;)) graph_skincare &lt;- ggplot(skincare, aes(x &#x3D; 구매일, y &#x3D; 금액합계, color &#x3D; 고객성별)) + geom_smooth() + geom_point(size &#x3D; 0.1) + scale_y_continuous(labels &#x3D; label_ko_num, breaks &#x3D; seq(0, 600000000, by &#x3D; 100000000)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), ) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + scale_colour_manual(values &#x3D; c(&quot;#EB3232&quot;, &quot;#FAAB18&quot;)) + bbc_style() graph_skincare 3. Shinhancard Data3.1 신한카드 ‘화장품’ 카테고리 데이터 전처리 # 신한카드 오프라인 구매 데이터 불러오기 shinhancard &lt;- read_xlsx(&quot;data&#x2F;Shinhancard.xlsx&quot;) # 신한카드 오프라인 구매 데이터 결측치 제거 shinhancard &lt;- shinhancard %&gt;% select(-c(6:8)) head(shinhancard) # 신한카드 데이터 필터링 filter_sh_beauty &lt;- shinhancard %&gt;% select(업종, 일별, 성별, 연령대별, &#39;카드이용건수(천건)&#39;) %&gt;% filter(업종 &#x3D;&#x3D; &quot;M018_화장품&quot;) head(filter_sh_beauty) # 신한카드 성별&amp;나이 결측치 제거하기(성별 F, M, 나이 0 이상만 추출) nomiss_sh_beauty &lt;- filter_sh_beauty %&gt;% filter(!is.na(성별) &amp; !is.na(연령대별)) %&gt;% filter((성별 %in% c(&quot;F&quot;, &quot;M&quot;)), 연령대별 &gt; 0) nomiss_sh_beauty # 신한카드 &#39;화장품&#39; 카테고리 구매수 합계 sum_sh_beauty &lt;- nomiss_sh_beauty %&gt;% group_by(일별, 성별) %&gt;% summarise(&#39;구매횟수&#39; &#x3D; sum(&#96;카드이용건수(천건)&#96;)) sum_sh_beauty # 신한카드 데이터 시계열 데이터로 변환 final_sh_beauty &lt;- sum_sh_beauty %&gt;% mutate(구매일자 &#x3D; ymd(일별)) final_sh_beauty 3.2 신한카드 데이터 시각화# 신한카드 &#39;화장품&#39; 카테고리 데이터 시각화_ppt.13p graph_sh_beauty &lt;- ggplot(final_sh_beauty, aes(x &#x3D; 구매일자, y &#x3D; 구매횟수, color &#x3D; 성별)) + geom_smooth() + geom_point(size &#x3D; 0.1) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + scale_y_continuous(breaks &#x3D; seq(0, 200, by &#x3D; 20)) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), ) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + scale_colour_manual(values &#x3D; c(&quot;#EB3232&quot;, &quot;#FAAB18&quot;)) + bbc_style() graph_sh_beauty 4. Naver Keyword Data4.1 마스크 키워드 검색량 데이터# 마스크 키워드 검색량 데이터 불러오기 mask &lt;- read_excel(&quot;data&#x2F;mask_keywords_data.xlsx&quot;) # 문자형 데이터를 숫자형으로 변환 mask$마스크검색량 &lt;- as.numeric(mask$마스크검색량) # 문자형 데이터를 날짜형으로 변환 final_mask &lt;- mask %&gt;% mutate(검색일자 &#x3D; ymd(구매날짜)) final_mask # 마스크 키워드 검색량 데이터 시각화_ppt.15p graph_mask &lt;- ggplot(final_mask, aes(x &#x3D; 검색일자, y &#x3D; 마스크검색량)) + geom_smooth(color &#x3D; &quot;#EB3232&quot;) + scale_y_continuous(breaks &#x3D; seq(0, 100, by &#x3D; 10)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + bbc_style() graph_mask 4.2 (색조 &amp; 기초) 화장품 키워드 검색량 데이터# (색조 &amp; 기초) 화장품 키워드 검색량 데이터 불러오기 makeup &lt;- read_excel(&quot;data&#x2F;색조 vs 기초 화장품 키워드 검색량.xlsx&quot;) # 문자형 데이터를 숫자형으로 변환 makeup$색조화장품 &lt;- as.numeric(makeup$색조화장품) makeup$기초화장품 &lt;- as.numeric(makeup$기초화장품) # 문자형 데이터를 날짜형으로 변환 trans_makeup &lt;- makeup %&gt;% mutate(검색일자 &#x3D; ymd(날짜)) trans_makeup # 색조 &amp; 기초 메이크업 화장품 키워드 검색량 데이터 시각화_ppt.16p graph_makeup &lt;- ggplot(trans_makeup, aes(x &#x3D; 검색일자, y &#x3D; &#96;색조 &amp; 기초 화장품 검색량&#96;)) + geom_line(aes(y &#x3D; &#96;색조화장품&#96;), color &#x3D; &quot;#EB3232&quot;) + geom_line(aes(y &#x3D; &#96;기초화장품&#96;), color &#x3D; &quot;#FAAB18&quot;) + scale_y_continuous(breaks &#x3D; seq(0, 100, by &#x3D; 10)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks &#x3D; NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + bbc_style() graph_makeup 4.3 (립 &amp; 아이) 화장품 키워드 검색량 데이터# (립 &amp; 아이) 화장품 키워드 검색량 데이터 불러오기 lipeye &lt;- read_excel(&quot;data&#x2F;메이크업 제품 비교(아이, 립).xlsx&quot;) # 문자형 데이터를 날짜형으로 변환 trans_lipeye &lt;- lipeye %&gt;% mutate(검색일자 &#x3D; ymd(날짜)) trans_lipeye # 립 &amp; 아이 메이크업 화장품 키워드 검색량 데이터 시각화_ppt.15p graph_lipeye &lt;- ggplot(trans_lipeye, aes(x &#x3D; 검색일자, y &#x3D; &#96;립 &amp; 아이 메이크업 검색량&#96;)) + geom_line(aes(y &#x3D; &#96;립 메이크업&#96;), color &#x3D; &quot;#EB3232&quot;) + geom_line(aes(y &#x3D; &#96;아이 메이크업&#96;), color &#x3D; &quot;#FAAB18&quot;) + scale_y_continuous(breaks &#x3D; seq(0, 100, by &#x3D; 10)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + geom_hline(yintercept &#x3D; 0, size &#x3D; 1, colour&#x3D;&quot;#999999&quot;) + bbc_style() graph_lipeye 4.4 (마스크프루프) 화장품 키워드 검색량 데이터# (마스크프루프) 화장품 키워드 검색량 데이터 불러오기 maskproof &lt;- read_excel(&quot;data&#x2F;마스크프루프 키워드 데이터.xlsx&quot;) # 문자형 데이터를 숫자형으로 변환 maskproof$마스크프루프 &lt;- as.numeric(maskproof$마스크프루프) # 문자형 데이터를 날짜형으로 변환 trans_maskproof &lt;- maskproof %&gt;% mutate(검색일자 &#x3D; ymd(날짜)) trans_maskproof # 마스크프루프 화장품 키워드 검색량 데이터 시각화 graph_maskproof &lt;- ggplot(trans_maskproof, aes(x &#x3D; 검색일자, y &#x3D; &#96;마스크프루프 제품 검색량&#96;)) + geom_line(aes(y &#x3D; &#96;마스크프루프&#96;), color &#x3D; &quot;#EB3232&quot;) + scale_y_continuous(breaks &#x3D; seq(0, 100, by &#x3D; 10)) + scale_x_date(date_breaks&#x3D;&quot;3 month&quot;, minor_breaks&#x3D;NULL, date_labels &#x3D; &quot;%Y.%m&quot;) + theme( axis.text.x &#x3D; element_text(size &#x3D; 8,family&#x3D; &quot;NanumSquare_ac&quot;, hjust &#x3D; 1), axis.text.y &#x3D; element_text(size &#x3D; 8,family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.x &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;), axis.title.y &#x3D; element_text(size &#x3D; 12, family &#x3D; &quot;NanumSquare_ac&quot;)) + bbc_style() graph_maskproof","categories":[{"name":"Project","slug":"Project","permalink":"https://ne-choi.github.io/categories/Project/"},{"name":"Contest","slug":"Project/Contest","permalink":"https://ne-choi.github.io/categories/Project/Contest/"}],"tags":[{"name":"공모전","slug":"공모전","permalink":"https://ne-choi.github.io/tags/%EA%B3%B5%EB%AA%A8%EC%A0%84/"},{"name":"시각화","slug":"시각화","permalink":"https://ne-choi.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"EDA","slug":"EDA","permalink":"https://ne-choi.github.io/tags/EDA/"}],"author":"ne-choi"},{"title":"ADsP 자격증: Part03. 데이터 분석_2","slug":"Study/ADsP/Part03_데이터_분석_2","date":"2020-11-16T15:00:00.000Z","updated":"2021-01-20T03:39:04.044Z","comments":true,"path":"/2020/11/17/Study/ADsP/Part03_데이터_분석_2/","link":"","permalink":"https://ne-choi.github.io/2020/11/17/Study/ADsP/Part03_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_2/","excerpt":"","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 4장. 통계 분석1절. 통계분석의 이해1. 통계 특정집단을 대상으로 수행한 조사 / 실험 결과의 요약된 형태 조사 대상에 따라 총조사(census)와 표본조사로 구분 2. 통계자료의 획득 방법 총 조사(전수 조사, census) 대상 집단 모두를 조사하면 많은 비용과 시간이 소요되므로 특별한 경우를 제외하고는 사용되지 않음 표본조사 모집단에서 샘플을 추출하여 진행하는 조사 모집단(population): 조사하고자 하는 대상 집단 전체 원소(element): 모집단을 구성하는 개체 표본(sample): 조사하기 위해 추출한 모집단의 일부 원소 모수(parameter): 표본 관측에 의해 구하고자 하는 모집단에 대한 정보 모집단의 정의, 표본 크기, 조사 방법, 조사 기간, 표본 추출 방법을 정확히 명시해야 함 표본 추출 방법 단순 랜덤 추출법(simple random sampling) 각 샘플에 번호를 부여해 임의의 n개를 추출하는 방법 각 샘플이 선택될 확률은 동일(비복원, 복원(추출 element를 다시 집어넣음) 추출) 계통추출법(systematic sampling) 단순랜덤추출법의 변형된 방식 임의 위치에서 매 k번째 행목을 추출하는 방법 번호를 부여한 샘플을 나열하여 K개씩 (K = N/n) n개의 구간으로 나누고 첫 구간(1, 2, …, K)에서 하나를 임의로 선택한 후 K개씩 띄어서 n개의 표본을 선택 집락추출법(cluster random sampling) 군집을 구분하고 군집별로 단순랜덤 추출법을 수행한 후, 모든 자료를 활용하거나 샘플링 하는 방법 지역 표본 추출, 다단계 표본 추출 층화추출법(stratified random sampling) 이질적인 원소들로 구성된 모집단에서 각 계층을 대표할 수 있도록 표본을 추출하는 방법 유사한 원소끼리 몇 개의 층(stratum)으로 나누어 각 층에서 랜덤 추출하는 방법 비례층화추출법, 불비례층화추출법 측정(measurement) 측정 표본조사나 실험 과정에서 추출된 원소들이나 실험 단위로부터 주어진 목적에 적합하도록 관측하여 자료를 얻는 것 측정 방법 내용 명목척도 측정 대상이 어느 집단에 속하는지 분류할 때 사용 (성별, 출생지 구분) 순서척도 측정 대상의 서열관계를 관측하는 척도 (만족도, 선호도, 학년, 신용등급) 구간척도(등간척도) 측정 대상이 갖는 속성의 양을 측정, 구간이나 구간 사이 간격에 의미 있는 자료 (온도, 지수) +,- 가능 *,/ 불가능 비율척도 간격(차이) 비율이 의미를 가지는 자료, 절대적 기준인 0이 존재하고 사칙연산 가능, 제일 많은 정보를 가지는 척도 (무게, 나이, 시간, 거리) 질적 척도: 명목척도, 순서척도 → 범주형 자료, 숫자 크기 차이가 계산되지 않는 척도 양적 척도: 구간척도, 비율척도 → 수치형 자료, 숫자 크기 차이를 계산할 수 있는 척도 3. 통계분석 통계분석 특정한 집단이나 불확실한 현상을 대상으로, 자료를 수집해 대상 집단 정보를 구하고 통계분석 방법을 이용하여 의사결정 하는 과정 기술통계(descriptive statistic) 주어진 자료로부터 어떠한 판단이나 예측과 같은 주관이 섞일 수 있는 과정을 배제하여 통계집단의 특성을 수량화하여 객관적인 데이터로 나타내는 통계분석 방법론 sample에 대한 특성인 평균, 표준편차, 중위수, 최빈값, 그래프, 왜도, 첨도 등을 구하는 것 통계적 추론(추측통계, inference statistics) 수집된 자료를 이용해 대상 집단(모집단)에 관한 의사결정을 하는 것 sample을 통해 모집단을 추정 모수추정 표본집단으로부터 모집단의 특성인 모수(평균, 분산 등)를 분석하여 모집단 추론 가설검정 대상집단에 관해 특정한 가설을 설정한 후에 가설 채택 여부를 결정하는 방법론 예측 미래의 불확실성을 해결해 효율적인 의사결정을 하기 위해 활용 예. 회귀분석, 시계열분석 등 4. 확률 및 확률분포 확률 표본공간 S에 부분집합인 각 사상에 대해 실수값을 가지는 함수의 확률값이 0과 1 사이에 있고 전체 확률의 합이 1인 것을 의미 표본공간 Ω의 부분집합인 사건 E의 확률은 표본공간의 원소 개수에 대한 사건 E 개수의 비율로 확률을 P(E)라고 할 때, 다음과 같의 정의 P(E) = $\\frac[n(E)][N(Ω)]$ 표본공간(sample space, Ω) 어떤 실험을 실시할 때 나타날 수 있는 모든 결과 집합 사건(event) 관찰자가 관심 있는 사건, 표본공간의 부분집합 원소(element) 나타날 수 있는 개별 결과 확률변수(random variable) 특정값이 나타날 가능성이 확률적으로 주어지는 변수 정의역(domain)이 표본공간, 치역(range)이 실수값 (0 &lt; y &lt; 1)인 함수 0이 아닌 확률을 갖는 실수값이 형태에 따라, 이산형 확률변수(discrete random variable)와 연속형 확률변수(continuous random variable)로 구분 덧셈정리(배반 X) 사건 A와 사건 B가 동시에 일어날 수 있을 때(교집합 성립) 일어날 확률 P(A 또는 B): P(A∪B) = P(A) + P(B) - P(A∩B) 사건 B가 주어졌을 때, 사건 A의 조건부확률: P(A|B) = P(A∩B)/P(B) 덧셈정리(배반 O) 사건 A와 사건 B가 동시에 일어나지 않을 때 사건 A or 사건 B 중, 한 쪽만 일어날 확률: P(A∪B) = P(A) + P(B) 곱셈정리 사건 A와 B가 서로 무관계하게 나타날 때(독립사건) 사건 B가 주어졌을 때, 사건 A의 조건부확률: P(A|B) = P(A) 확률분포 이산형 확률 변수 베르누이 확률분포(Bernoulli distribution) 결과가 2개만 나오는 경우 동전 던지기, 시험의 합격/불합격, 안타를 칠 확률 이항분포(Binomial distribution) 베르누이 시행을 n번 반복했을 때, k번 성공할 확률 5번 타석에 들어와서 3번 안타를 칠 확률 → n=5, k=3, 안타를 칠 확률 P(x)=타율 성공할 확률 P가 0이나 1에 가깝지 않고 n이 충분히 크면 정규분포에 가까워짐, 1/2에 가까우면 종 모양 기하분포(Geometric distribution) 성공확률이 p인 베르누이 시행에서 첫 번째 성공이 있기까지 x번 실패할 확률 5번 타석에 들어와서 3번째 타석에서 안타를 칠 확률 다항분포(Multinomial distribution) 세 가지 이상의 결과를 가지는 반복 시행에서 발생하는 확률 분포 (이항분포 확장한 것) 포아송분포(Poisson distribution) 시간과 공간 내에서 발생하는 사건의 발생횟수에 대한 확률분포 책에 오타가 5p당 10개 나온다고 할 때, 한 페이지에 오타가 3개 나올 확률 최근 5경기에서 10개의 홈런을 쳤다고 할 때, 오늘 경기에서 홈런을 치지 못할 확률 연속형 확률 변수 균일분포(일양분포, Uniform distribution) 모든 확률변수 X가 균일한 확률을 가지는 확률분포 (다트의 확률분포) 정규분포(Normal distribution) 평균이 μ이고 표준편차가 σ인 X의 확률밀도함수 표준편차가 클 경우 그래프가 퍼져보임 표준정규분포: 평균 0, 표준편차 1 → 정규분포를 표준정규분포로 만드는 공식: z = $\\frac{X-μ}{σ}$ 지수분포(Exponential distribution) 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포 전자레인지 수명 시간, 콜센터에 전화가 걸려올 때까지의 시간, 은행 고객 내방에 걸리는 시간, 버스가 올 때까지 시간 t-분포(t-distribution) 데이터가 연속형일 때, 두 집단 평균이 동일한지 알고 싶을 때 사용 평균이 0을 중심으로 좌우가 동일한 분포 정규분포보다 퍼져 있고 자유도가 커질수록 정규분포에 가까워짐 X^2^-분포(chi-square distribution) 두 집단 간의 동질성 검정에 활용 범주형 자료에 얻어진 관측값과 기대값 차이를 보는 적합성 검정에 활용 모평균과 모분산이 알려지지 않은 모집단의 모분산에 대한 가설 검정에 사용되는 분포 6 F-분포(F-distribution) 두 집단 간 분산의 동일성 검정에 사용 확률변수는 항상 양의 값만 갖고 x^2^-분포와 달리 자유도를 2개 가지며 자유도가 커질수록 정규분포에 가까워짐 5. 추정과 가설 검정 추정의 개요 확률표본(random sample) 확률분포는 분포를 결정하는 평균, 분산 등 모수(parameter)를 가지고 있음 특정한 확률분포로부터 독립적으로 반복해 표본을 추출하는 것 각 관찰값들은 서로 독립적이며 동일한 분포를 가짐 추정 표본으로부터 미지의 모수를 추측하는 것 점추정(point estimation) ‘모수가 특정한 값일 것‘이라고 추정하는 것 표본의 평균, 중위수, 최빈값 등을 사용 점추정량의 조건, 표본평균, 분산 불편성: 가능한 표본에서 얻은 추정량의 기대값은 모집단의 모수와 편의(차이)가 없음 효율성: 추정량의 분산이 작을수록 좋음 일치성: 표본 크기가 아주 커지면, 추정량이 모수와 거의 같아짐 충족성: 추정량은 모수에 대해 모든 정보를 제공 표본평균: 모집단 평균(모평균)을 추정하기 위한 추정량, 확률표본의 평균값 표본분산: 모집단의 분산(모분산)을 추정하기 위한 추정량 구간추정(interval estimation) 점추정의 정확성을 보완하기 위해, 확률로 표현된 믿음의 정도 하에서 모수가 특정한 구간에 있을 것이라고 선언하는 것 항상 추정량 분포에 대한 전제와, 구해진 구간 안에 모수가 있을 가능성의 크기(신뢰수준(confidence interval))가 주어져야 함 참고: 모분산을 알 때는 분자에 σ, 모를 때는 S를 넣음 가설검정 모집단에 대한 가설을 설정하고, 표본관찰을 통해 가설의 채택여부를 결정하는 분석 방법 표본 관찰 또는 실험을 통해 귀무가설과 대립가설 중 하나를 선택 귀무가설이 옳다는 전제 하에 검정통계량 값을 구하고, 이 값이 나타날 가능성의 크기에 의해 귀무가설 채택 여부를 결정 귀무가설(null hypothesis, H0) ‘비교하는 값과 차이가 없다, 동일하다’를 기본개념으로 하는 가설 대립가설(alternative hypothesis, H1) 뚜렷한 증거가 있을 때 주장하는 가설 검정통계량(test statistic) 관찰된 표본으로부터 구하는 통계량, 검정 시 가설 진위를 판단하는 기준 유의수준(significance level, α) 귀무가설이 옳은데도 기각하는 확률 크기 기각역(critical regoin, C) 귀무가설이 옳다는 전제 하에서 구한 검정통계량 분포에서, 확률이 유의수준 α인 부분 반대는 채택역(acceptance region) 제1종 오류와 제2종 오류 사실 \\ 가설검정 결과 H0가 사실이라고 판정 H0가 사실 아니라고 판정 H0가 사실 옳은 결정 제1종 오류(α) H0가 사실 아님 제2종 오류(β) 옳은 결정 두 가지 오류는 상충관계라, 가설검정에서는 제1종 오류 크기를 0.1, 0.05, 0.01 등으로 고정한 뒤, 제2종 오류가 최소가 되도록 기각역을 설정함 6. 비모수 검정 모수적 방법 검정하고자 하는 모집단의 분포에 대해 가정하고, 가정 하에서 검정통계량과 검정통계량 분포를 유도해 검정 실시 비모수적 방법 자료가 추출된 모집단의 분포에 대한 아무 제약을 가하지 않고 검정 실시 관측된 자료가 특정분포를 따른다고 가정할 수 없는 경우에 이용 관측된 자료 수가 많지 않거나(30개 미만) 자료가 개체 간의 서열관계를 나타내는 경우에 이용 모수적 검정 vs 비모수적 검정 가설의 설정 모수적 검정: 가정된 분포의 모수에 대해 가설 설정 비모수적 검정: 가정된 분포 x → 가설은 단지 분포의 형태가 동일하다/동일하지 않다’처럼 분포 형태를 설명 검정 방법 모수적 검정: 관측된 자료로 구한 표본평균, 표본분산 등 이용해 검정 비모수적 검정: 관측값의 절대적 크기에 의존하지 않는 관측값의 순위나 두 관측값 차이의 부호 등 이용해 검정 비모수적 검정의 예 부호 검정, 윌콕슨의 순위합검정, 윌콕슨의 부호순위합검정, 만-위트니의 U검정, 런검정, 스피어만의 순위상관계수 2절. 기초 통계분석1. 기술통계 기술통계(Descriptive Statistics) 자료 특성을 그림, 통계량을 사용해 쉽게 파악할 수 있도록 정리하는 것 자료를 요약하는 기초적 통계를 의미 데이터 분석에 앞서 대략적 통계적 수치를 계산 → 통찰력 얻기에 유리 통계량에 이한 자료 정리 중심위치의 측도 산포의 측도: 분산, 표준편차, 범위, 사분위수 범위 등 분포 형태에 관한 측도 왜도: 분포의 비대칭 정도를 나타내는 측도 m3 &gt; 0: 오른쪽으로 긴 꼬리를 갖는 분포 (최빈값 &lt; 중앙값 &lt; 평균) m3 = 0: 좌우가 대칭인 분포 m3 &lt; 0: 왼쪽으로 긴 꼬리를 갖는 분포 (평균 &lt; 중앙값 &lt; 최빈값) 그래프를 이용한 자료 정리 히스토그램 표로 된 도수분포를 그림으로 나타낸 것 막대그래프 vs 히스토그램 막대그래프 범주(category)형으로 구분된 데이터를 표현 → 의도에 따라 범주의 순서를 바꿀 수 있음 직업, 종교, 음식 히스토그램 연속(continuous)형으로 표시된 데이터 → 임의로 순서를 바꿀 수 없고 막대의 간격이 없음 몸무게, 성적, 연봉 히스토그램의 생성 계급의 수는 2^k^ ≥ n을 만족하는 최소의 정수 log2n = k에서 최소의 정수 계급 간격은 $\\frac{(최대값 - 최소값)}{계급수}$로 파악 가능 계급 수와 간격이 변하면 히스토그램 모양도 변함 줄기-잎 그림(stem-and leaf plot) 상자그림(Box plot) 다섯 숫자 요약을 통해 그림으로 표현(최소값, Q1, Q2, Q3, 최대값) 사분위수 범위(IQR): Q3 - Q1 안울타리(inner fence): Q1 - 1.5 X IQR 또는 Q3 + 1.5 X IQR 바깥울타리(outer fence): Q1 - 3 X IQR 또는 Q3 + 3 X IQR 보통이상점(mild outlier): 안쪽울타리와 바깥울타리 사이 자료 극단이상점(extreme outlier): 바깥울타리 밖 자료 2. 인과관계의 이해 용어 종속변수(반응변수, y) 다른 변수의 영향을 받는 변수 독립변수(설명변수, x) 영향을 주는 변수 산점도(sxatter plot) 좌표평면 위에 점들로 표현한 그래프 공분산(covariance) 두 확률변수 X, Y 방향의 조합(선형성) 공분산 부호로 두 변수의 방향성 확인 가능 공분산 부호가 +: 두 변수는 양의 방향성, 공분산 부호가 -: 두 변수는 음의 방향성을 가짐 X, Y가 서로 독립이면, cov(X,Y) = 0 3. 상관분석 상관분석(Correlation Analysis) 두 변수 간 관계의 정도를 알아보기 위한 분석 방법 상관계수(Correlation coefficient)이용 상관관계 특성 상관계수 범위 해석 0.7 &lt; r ≤ 1 강한 양(+)의 상관이 있다 0.3 &lt; r ≤ 0.7 약한 양(+)의 상관이 있다 0 &lt; r ≤ 0.3 거의 상관이 없다 r = 0 상관관계(선형, 직선)가 존재하지 않는다 -0.3 ≤ r &lt; 0 거의 상관이 없다 -0.7 ≤ r &lt; -0.3 약한 음(-)의 상관이 있다 -1 ≤ r &lt; -0.7 강한 음(-)의 상관이 있다 상관분석 유형 구분 피어슨 스피어만 개념 등간척도 이상으로 측정된 두 변수의 상관관계 측정 방식 서열척도인 두 변수 상관관계 측정 방식 특징 연속형 변수, 정규성 가정, 대부분 많이 사용 순서형 변수, 비모수적 방법, 순위 기준 상관관계 측정 상관계수 피어슨 r(적률상관계수) 순위상관계수(p, 로우) 상관분석을 위한 R 분산: var(x,y=NULL, na.rm=FALSE) 공분산: cov(x,y=NULL, use=”everything”, method=c(“pearson”, “kendall”, “spearman”)) 상관관계: cor(x,y=NULL, use=”everything”, method=c(“pearson”, “kendall”, “spearman”)) 상관관계(Hmisc 패키지): rcorr(matrix(data명), type=c(“pearson”, “kendall”, “spearman”)) 상관분석의 가설 검정 상관계수 r이 0이면 입력변수 x와 출력변수 y사이에는 아무런 관계가 없음 (귀무가설: r=0, 대립가설: r≠0) t 검정통계량을 통해 얻은 p-value값이 0.05 이하인 경우, 대립가설을 채택하게 되어 데이터에서 구한 상관계수를 활용할 수 있게 됨 상관분석 예제 cov: 공분산 cor: 상관계수 p-value: 유의수준 0.05보다 작게 나타나면 상관계수가 있음 3절. 회귀분석1. 회귀분석 회귀분석 하나나 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계기법 변수 사이의 인과관계를 밝히고 모형을 적합하여 관심 있는 변수를 예측하거나 추론하기 위한 분석 방법 독립변수의 개수가 하나면 단순선형회귀분석, 독립변수 개수가 두 개 이상이면 다중선형회귀분석 회귀분석의 변수 영향 받는 변수(y): 반응변수(response variable), 종속변수(dependent variable), 결과변수(outcome variable) 영향 주는 변수(x): 설명변수(explanatory variable), 독립변수(independent variable), 예측변수(predictor variable) 선형회귀분석의 가정 선형성 입력변수와 출력변수의 관계가 선형 (가장 중요한 가정) 등분산성 오차 분산이 입력변수와 무관하게 일정 잔차플롯(산점도)를 활용해 잔차와 입력변수 간 아무런 관련성이 없게 무작위적으로 고루 분포돼야 등분산성 가정 만족 독립성 입력변수와 오차는 관련 없음 자기상관(독립성)을 알아보기 위해 Durbin-Waston 통계량 사용 시계열 데이터에서 많이 활용 비상관성 오차들끼리 상관이 없음 정상성(정규성) 오차 분포가 정규분포를 따름 Q-Q plot, Kolmogolov-Sirnov 검정, Shaprio-Wilk 검정 등 활용 가정에 대한 검증 단순선형회귀분석 입력변수와 출력변수 간 선형성을 점검하기 위해 산점도 확인 다중선형회귀분석 선형회귀분석 가정인 선형성, 등분산성, 독립성, 정상성이 모두 만족하는지 확인 2. 단순선형회귀분석 하나의 독립변수가 종속변수에 미치는 영향을 추정할 수 있는 통계기법 회귀분석 검토사항 회귀계수가 유의미한가? 해당 계수 t 통계량의 p-값이 0.05보다 작으면 해당 회귀계수가 통계적으로 유의하다고 볼 수 있음 모형이 설명력을 갖는가? **결정계수(R^2^)**를 확인 결정계수는 0~1값을 가지며, 높을수록 추정된 회귀식의 설명력이 높아짐 모형이 데이터를 잘 적합하는가? 잔차를 그래프로 그리고 회귀진단 회귀계수의 추정 최소제곱법, 최소자승법 측정값을 기초로 적당한 제곱합을 만들고 이를 최소로 하는 값을 구해 측정결과를 처리 잔차제곱이 가장 작은 선을 구하는 것 회귀분석의 검정 회귀계수의 검정 회귀계수 β1이 0이면 입력변수 X와 출력변수 y 사이에는 아무런 인과관계가 없음 회귀계수 β1이 0이면 적합된 추정식은 아무 의미가 없음 (귀무가설 β1=0, 대립가설 1≠0) 3. 다중선형회귀분석 다중선형회귀분석(다변량회귀분석) 다중회귀식 Y = β0 + β1X1 + β2X2 + … + βkXk + ε 모형의 통계적 유의성 모형의 통계적 유의성은 F통계량으로 확인 유의수준 5% 하에서 F통계량의 p-값이 0.05보다 작으면 추정된 회귀식은 통계적으로 유의하다 볼 수 있음 F통계량이 크면 p-value가 0.05보다 작아지고 귀무가설을 기각함 → 모형이 유의하다고 결론 내릴 수 있음 회귀계수의 유의성 단변량 회귀분석의 회귀계수 유의성 검토와 같이 t통계량을 통해 확인 모든 회귀계수의 유의성이 통계적으로 검증되어야 선택된 변수 조합으로 모형 확인 가능 모형의 설명력 결정계수(R^2^)나 수정된 결정계수(R^2^α) 확인 모형의 적합성 잔차와 종속변수의 산점도로 모형이 데이터를 잘 적합하고 있는지 확인 데이터가 전제하는 가정을 만족하는가? 선형성, 독립성, 등분산성, 비상관성, 정상성 다중공선성(multicollinearity) 다중회귀분석에서 설명변수 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란함 다중공선성 검사 방법 분산팽창요인(VIF): 4보다 크면 다중공산성 존재한다고 볼 수 있고, 10보다 크면 심각한 문제가 있다고 해석 상태지수: 10 이상이면 문제 있다고 보고, 30보다 크면 심각한 문제가 있다고 해석 다중선형회귀분석에서 다중공선성 문제 발생 시, 문제 있는 변수를 제거하거나 주성분회귀, 능형회귀 모형을 적용하여 문제 해결 4. 회귀분석의 종류 종류 내용 단순회귀 독립변수가 1개이며 종속변수와의 관계가 직선 다중회귀 독립변수가 k개이며 종속변수와의 관계가 선형(1차 함수) 로지스틱 회귀 종속변수가 범주형(2진변수)인 경우에 적용, 단순 로지스틱 회귀 및 다중, 다항 로지스틱 회귀로 확장할 수 있음 다항회귀 독립변수와 종속변수와의 관계가 1차 함수 이상인 관계(단, k=1이면 2차 함수 이상) 곡선회귀 독립변수가 1개이며 종속변수와의 관계가 곡선 비선형회귀 회귀식 모양이 미지의 모수들의 선형관계로 이뤄져 있지 않은 모형 5. 회귀분석 사례 그래프 보고 푸는 문제 F-statistic: F-통계량 p-value: 유의수준 5% 하에서 추정되어야 해당 회귀 모형이 통계적으로 유의하다고 할 수 있음 Multiple R-squared: 결정계수, Adjusted R-squared: 수정된 결정계수 (0~1값을 가지며, 높을수록 회귀식의 설명력 높아짐) Pr: 회귀계수들의 p-값 6. 최적회귀방정식 설명변수 선택 상황에 따라 필요한 변수만 선택 y에 영향을 미칠 수 있는 모든 설명변수 x가 y값 예측에 참여 데이터에 설명변수 x 수가 많아지면 관리가 어려워, 가능한 범위 내에서 적은 수의 설명변수만 포함 모형선택(exploratiry analysis) 분석 데이터에 가장 잘 맞는 모형을 찾는 방법 가능한 모든 조합의 회귀분석(All possible regression): 가능한 모든 독립변수 조합에 대한 회귀모형을 생성한 뒤, 가장 적합한 회귀모형 선택 단계적 변수 선택(Stepwise Variable Selection) 전진선택법(forward selection) 절편만 있는 상수모형으로 시작해 중요하다고 생각되는 설명변수부터 모형에 추가 후진제거법(backward selection) 독립변수 후보 모두를 포함한 모형에서 출발해 가장 적은 영향을 주는 변수부터 제거 더 제거할 변수가 없을 때의 모형을 선택 단계선택법(stepwise selection) 전진선택법에 이해 변수를 추가하며, 새롭게 추가된 변수에 기인해 기존 변수 중요도가 약화되면 해당변수를 제거 단계별로 추가 또는 제거되는 변수 여부를 검토하고 더 이상 없을 때 중단 벌점화된 선택기준 모형 복잡도에 벌점을 주는 방법 AIC(Akaike information criterion) BIC(Bayesian information criterion) 모든 후보 모형에 대해 AIC 또는 BIC를 계산하고 값이 최소가 되는 모형을 선택 모형 선택의 일치성(consistency inselection) 자료 수가 늘어날 때 참인 모형이 주어진 모형 선택 기준의 최소값을 갖게 되는 성질 이론적으로 AIC에 대해 일치성이 성립하지 않지만, BIC는 주요 분포에서 이러한 성질이 성립 AIC 활용이 보편화된 방식 추가: RIC(Risk inflation criterion), CIC(Covariance inflation criterion), DIC(Deviation information criterion) 최적회귀방정식 사례: 교재 참고 변수 선택법 예제(유의확률 기반) 변수 선택법 예제(벌점화 전진선택법) 변수 선택법 예제(벌점화 후진제거법) 4절. 시계열 분석1. 시계열 자료 시계열 자료 시간의 흐름에 따라 관찰된 값 시계열 데이터 분석을 통해 미래의 값을 예측하고 경향, 주기, 계절성 등을 파악하여 활용 시계열 자료의 종류 비정상성 시계열 자료 시계열 분석을 실시할 때, 다루기 어려운 자료 정상성 시계열 자료 비정상 시계열을 핸들링해 다루기 쉬운 시계열 자료로 변환한 자료 2. 정상성 평균이 일정할 경우 모든 시점에 대해 일정한 평균을 가짐 평균이 일정하지 않은 시계열은 차분(difference)을 통해 정상화할 수 있음 차분? 현 시점 자료에서 전 시점 자료를 빼는 것 일반차분: 바로 전 시점 자료를 빼는 방법, 계절차분: 여러 시점 전의 자료를 빼는 방법 분산이 일정 분산도 시점에 의존하지 않고 일정해야 함 분산이 일정하지 않을 경우 변환(transformation)을 통해 정상화할 수 있음 공분산도 단지 시차에만 의존, 실제 특정 시점 t, s에는 의존하지 않음 정상 시계열 어떤 시점에서 평균과 분산, 특정한 시차의 길이를 갖는 자기공분산을 측정하더라도 동일한 값을 가짐 정상 시계열은 항상 그 평균값으로 회귀하려는 경향이 있으며, 그 평균값 주변에서의 변동은 대체로 일정한 폭을 가짐 정상 시계열이 아닌 경우 특정 기간의 시계열 자료로부터 얻은 정보를 다른 시기로 일반화할 수 없음 3. 시계열자료 분석방법 분석방법 회귀분석(계량경제) 방법, Box-Jenkins 방법, 지수평활법, 시계열 분해법 등 자료 형태에 따른 분석방법 일변량 시계열분석 Box-Jenkins(ARMA), 지수평활법, 시계열 분해법 시간(t)을 설명변수로 한 회귀모형주가, 소매물가지수 등 하나의 변수에 관심 갖는 경우의 시계열분석 다중 시계열분석 계량경제모형, 전이함수모형, 개입분석, 상태공간분석, 다변량 ARIMA 등 여러 개의 시간(t)에 따른 변수들을 활용하는 시계열 분석 이동평균법 지수평활법 4. 시계열모형교재 참고 5절. 다차원척도법1. 다차원척도법(Multidimensional Scaling) 객체간 근접성을 시각화하는 통계기법 군집분석과 같이 개체를 대상으로 변수들을 측정한 후, 개체 사이의 유사성/비유사성을 측정하여 개체들을 2차원 공간상에 점으로 표현하는 분석 방법 개체들을 2차원 또는 3차원 공간상에 점으로 표현하여 개체들 사이의 집단화를 시각적으로 표현하는 분석 방법 2. 다차원척도법 목적 데이터 속에 잠재해 있는 패턴, 구조를 찾아냄 찾아낸 구조를 소수 차원의 공간에 기하학적으로 표현 데이터 축소 목적으로 다차원척도법을 이용 → 데이터에 포함되는 정보를 끄집어내기 위한 탐색수단 다차원척도법에 의해 얻은 결과를, 데이터가 만들어진 현상이나 과정에 고유의 구조로서 의미 부여 3. 다차원척도법 방법 객체들의 거리 계산: 유클리드 거리행렬 활용 관측대상의 상대적 거리 정확도를 높이기 위해 적합 정도를 스트레스값으로 나타냄 각 개체를 공간상에 표현하기 위한 방법: 부적합도 기준으로 STRESS나 S-STRESS 사용 최적모형의 적합은 부적합도를 최소로 하는 반복알고리즘을 이용하며, 이 값이 일정 수준 이하가 될 때 최종적으로 적합된 모형으로 제시 STRESS와 적합도 수준 M은 개체들을 공간상에 표현하기 위한 방법으로 STRESS나 S-STRESS를 부적합도 기준으로 사용 STRESS 적합도 수준 0 완벽(perfect) 0.05 이내 매우 좋은(excellent) 0.05 ~ 0.10 만족(satisfactory) 0.10 ~ 0.15 보통(acceptable, but doubt) 0.15 이상 나쁨(poor) 4. 다차원척도법 종류 계량적 MDS(Metric MDS) 데이터가 구간척도나 비율척도인 경우 활용 N개의 케이스에 대해 P개의 특성변수가 있는 경우, 각 개체들 간 유클리드 거리행렬을 계산하고 개체들 간 비유사성 S(거리제곱 행렬의 선형함수)를 공간상에 표현 비계량적 MDS(nonmetric MDS) 데이터가 순서척도인 경우 활용 개체들 간 거리가 순서로 주어진 경우에는 순서척도를 거리의 속성과 같도록 변환하여 거리를 생성한 후 적용 6절. 주성분분석1. 주성분분석(Principal Component Analysis) 여러 변수들이 변량을 주성분이라는 서로 상관성이 높은 변수의 선형 결합으로 만들어 기존 상관성이 높은 변수들을 요약, 축소하는 기법 첫 번째 주성분으로 전체 변동을 가장 많이 설명할 수 있도록 하고, 두 번째 주성분으로는 첫 번째 주성분과는 상관성이 없어서(낮아서) 첫 번째 주성분이 설명하지 못하는 나머지 변동을 정보의 손실 없이 가장 많이 설명할 수 있도록 변수들의 선형조합을 만듦 2. 주성분분석의 목적 여러 변수들 간 내재하는 상관관계, 연관성을 이용해 소수의 주성분으로 차원을 축소함으로써 데이터를 이해하기 쉽고 관리하기 쉽게 함 다중공선성이 존재하는 경우, 상관성 없는(적은) 주성분으로 변수들을 축소하여 모형 개발에 활용 회귀분석이나 의사결정나무 등 모형 개발 시, 입력변수들 간 상관관계가 높은 다중공선성이 존재할 경우 모형이 잘못 만들어져 문제 발생 연관성 높은 변수를 주성분분석을 통해 차원을 축소한 후, 군집분석을 수행하면 군집화 결과와 연산속도 개선 가능 기계에서 나오는 센서데이터를 주성분분석으로 차원 축소 후, 시계열로 분포나 추세 변화를 분석하면 기계의 고장 징후를 사전에 파악하는 데 활용할 수 있음 3. 주성분분석 vs 요인분석 요인분석(Factor Analysis) 등간척도(혹은 비율척도)로 측정한 두 개 이상 변수에 잠재된 공통인자를 찾아내는 기법 공통점 모두 데이터를 축소하는 데 활용 원래 데이터를 활용하여 몇 개의 새로운 변수 생성 가능 차이점 생성된 변수의 수 요인분석은 몇 개라고 지정 없이(2 or 3, 4, 5 …) 만들 수 있음 주성분분석은 제1주성분, 제2주성분, 제3주성분 정도로 활용(대략 4개 이상은 넘지 않음) 생성된 변수 이름 요인분석은 분석자가 요인 이름을 명명 주성분분석은 주로 제1주성분, 제2주성분 등으로 표현 생성된 변수 간 관계 요인분석은 새 변수들은 기본적으로 대등한 관계를 가짐 요인분석은 어떤 것이 더 중요하다는 의미가 없음(분류/예측의 다음 단계로 사용되면 중요성 부여) 주성분분석은 제1주성분이 가장 중요, 그 다음 제2주성분이 중요 분석 방법의 의미 요인분석은 목표변수를 고려하지 않고 데이터가 주어지면 변수를 비슷한 성격으로 묶어서 새로운 (잠재)변수를 만듦 주성분분석은 목표변수를 고려하여 목표변수를 예측/분류하기 위해 원래 변수의 선형 결합으로 이뤄진 몇 개의 주성분(변수)를 찾게 됨 4. 주성분의 선택법 주성분분석 결과에서 누적기여율(cumulative proportion)이 85% 이상이면 주성분 수로 결정할 수 있음 scree plot을 활용하여 고유값(eigenvalue)이 수평을 유지하기 전 단계로 주성분의 수 선택 5. 주성분 분석 사례 교재 참고 5장. 정형 데이터 마이닝1절. 데이터마이닝의 개요1. 데이터마이닝 데이터마이닝 대용량 데이터에서 의미 있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법 통계분석과의 차이점 통계분석은 가설이나 가정에 따른 분석이나 검증을 함 데이터마이닝은 다양한 수리 알고리즘을 이용해 데이터베이스의 데이터로부터 의미 있는 정보를 찾아내는 방법을 통칭 종류 정보를 찾는 방법론에 따라 분석대상, 활용목적, 표현방법에 따라 인공지능, 의사결정나무, K-평균군집화, 연관분석, 회귀분석, 로짓분석, 최근접이웃 시각화분석, 분류, 군집화, 포케스팅 사용분야 병원: 환자 데이터를 이용하여 해당 환자에게 발생 가능성 높은 병 예측 병원: 기존 환자가 응급실에 왔을 때, 어떤 조치를 먼저 해야 하는지 결정 은행: 고객 데이터를 이용해 해당 고객의 우량/불량을 예측하여 대출 여부 판단 공항: 세관 검사에서 입국자 이력과 데이터를 이용해 관세품 반입 여부 예측 2. 데이터마이닝의 분석 방법 Supervised Data Prediction(지도학습) Unsupervised Data Prediction(비지도학습) 의사결정나무, 인공신경망, 일반화 선형 모형, 회귀분석, 로지스틱 회귀분석, 사례기반 추론, 최근접 이웃법 OLAP, 연관성 규칙발견, 군집분석, SOM 3. 분석 목적에 따른 작업 유형과 기법 예측(Predictive Modeling): 분류 규칙 설명(Descriptive Modeling): 연관 규칙, 연속 규칙, 데이터 군집화 작업유형 설명 사용기법 분류 규칙(Classification) 가장 많이 사용되는 작업으로 과거 데이터로부터 고객특성을 찾아 분륨형을 만들어 이를 토대로 새로운 레코드의 결과값을 예측하는 것, 목표 마케팅 및 고객 신용평가 모형에 활용 회귀분석, 판별분석, 신경망, 의사결정나무 연관규칙(Association) 데이터 안에 존재하는 항목간의 종속관계를 찾아내는 작업, 제품이나 서비스 교차판매, 매장진열, 첨부우편, 사기적발 등 분야에 활용 동시발생 매트릭스 연속규칙(Sequence) 연관 규칙에 시간 관련 정보가 포함된 형태, 고객 구매이력 속성이 반드시 필요, 목표 마케팅이나 일대일 마케팅에 활용 동시발생 매트릭스 데이터 군집화(Clustering) 고객 레코드를 유사한 특성을 지닌 몇 개의 소그룹으로 분할, 작업 특성이 분류규칙과 유사하나 분석대상 데이터에 결과값이 없음, 판촉활동이나 이벤트 대상 선정에 활용 K-Means Clustering 4. 데이터마이닝 추진단계 목적 설정 데이터 준비 가공 기법 적용 검증 5. 데이터마이닝을 위한 데이터 분할 개요 모델 평가용 테스트 데이터와 구축용 데이터로 분할 구축용 데이터로 모형을 생성하고 테스트 데이터로 모형이 얼마나 적합한지를 판단 데이터 분할 구축용(training data, 50%) 추정용, 훈련용 데이터라고도 불리며 데이터마이닝 모델을 만드는 데 활용 검정용(validation data, 30%) 구축된 모형의 과대추정 또는 과소추정을 미세 조정하는 데 활용 시험용(tast data, 20%) 테스트 데이터나 과거 데이터를 활용하여 모델의 성능을 검증하는 데 활용 데이터 양이 충분하지 않거나 입력 변수에 대한 설명이 충분한 경우 홀드아웃 방법 주어진 데이터를 랜덤하게 두 개의 데이터로 구분하여 사용 주로 학습용과 시험용으로 분리하여 사용 교차확인 방법 주어진 데이터를 k개의 하부집단으로 구분 k-1개의 집단을 학습용으로 나머지는 하부집단으로 검증용으로 설정하여 학습 k번 반복 측정한 결과를 평균낸 값을 최종값으로 사용 주로 10-fold 교차분석을 많이 사용 6. 성과분석 오분류에 대한 추정치 정분류율(Accuracy) Accuracy = $\\frac{TN + TP}{TN + TP + FN + FP}$ 오분류율(Error Rate) 1 - Accuracy = $\\frac{FN + FP}{TN + TP + FN + FP}$ 특이도(Specificity) Specificity = $\\frac{TN}{TN + FP}$ (TNR: True Negative Rate) 민감도(Sensitivity) Sensitivity = $\\frac{TP}{TP + FN}$ (TPR: True Positive Rate) 정확도(Precision) Precision = $\\frac{TP}{TP + FP}$ 재현율(Recall): 민감도와 같음 Recall = $\\frac{TP}{TP + FN}$ F1 Score F1 = 2 x $\\frac{Precision x Recall}{Precision + Recall}$ ROCR 패키지로 성과분석 ROC Curve(Receiver Operation Characteristic Curve) 가로축을 FPR(False Positive Rate = 1 - 특이도)값, 세로축을 TPR(Ture Positive Rate, 민감도)값으로 두어 시각화한 그래프 2진 분류(binary classfication)에서 모형 성능을 평가하기 위해 사용되는 척도 그래프가 왼쪽 상단에 가깝게 그려질수록 올바르게 예측한 비율은 높고 잘못 예측한 비율은 낮음을 의미 ROC 곡선 아래 면적을 의미하는 AUROC(Area Under ROC) 값이 클수록(1에 가까울 수록) 모형 성능이 좋다고 평가 TPR: 1인 케이스에 대한 1로 예측한 비율 FPR: 0인 케이스에 대한 1로 잘못 예측한 비율 AUROC를 이용한 정확도의 판단 기준 기준 구분 0.9 - 1.0 excellent (A) 0.8 - 0.9 good 0.7 - 0.8 fair 0.6 - 0.7 poor 0.5 - 0.6 fail 이익도표(Lift chart)* 분류모형 성능을 평가하기 위한 척도 (분류된 관측치에 대해 예측이 얼마나 잘 이루어졌는지) 임의로 나눈 등급별로 반응검출율, 반응률, 리프트 등 정보를 산출해 나타내는 도표 기본 향상도에 비해 반응률이 몇 배나 높은지 계산: 향상도(Lift) 각 등급은 예측확률에 따라 매겨진 순위이므로, 상위 등급에서는 더 높은 반응률을 보이는 것이 좋은 모형 2절. 분류분석1. 분류분석과 예측분석 분류분석의 정의 데이터가 어떤 그룹에 속하는지 예측할 때 사용하는 기법 클러스터링과 유사하나, 분류분석은 각 그룹이 정의되어 있음 교사학습(supervised learning)에 해당하는 예측기법 예측분석의 정의 시계열분석처럼 시간에 따른 값 두 개만을 이용해 앞으로의 매출 또는 온도 등을 예측하는 것 모델링을 하는 입력 데이터가 어떤 것인지에 따라 특성이 다름 여러 개의 다양한 설명변수(독립변수)가 아닌 한 개의 설명변수로 생각하면 됨 분류분석 vs 예측분석 공통점 레코드 특정 속성의 값을 미리 알아맞힐 수 있음 차이점 분류: 레코드(튜플)의 범주형 속성의 값을 맞힘(국/영/수 점수로 내신 등급 맞히기) 예측: 레코드(튜플)의 연속형 속성의 값을 맞힘(카드 회원 가입정보로 연 매출액 알아맞히기) 분류 모델링 신용평가모형(우량, 불량) 사기방지모형(사기, 정상) 이탈모형(이탈, 유지) 고객세분화(VVIP, VIP, GOLD, SILVER, BRONZE) 분류 기법 회귀분석, 로지스틱 회귀분석 의사결정나무, CART, C5.0 베이지안 분류 인공신경망 지지도벡터기계 k 최근접 이웃 규칙기반의 분류와 사례기반추록 2. 로지스틱 회귀분석(Logistic Regression) 반응변수가 범주형인 경우에 적용되는 회귀분석모형 새로운 설명변수(또는 예측변수)가 주어질 때, 반응변수의 각 범주(또는 집단)에 속할 확률이 얼마인지 추정(예측모형)하여, 추정 확률을 기준치에 따라 분류하는 목적(분류모형) 사후확률(Posterior Probability): 모형의 적합을 통해 추정된 확률 exp(β1): 나머지 변수가 주어질 때, x1이 한 단위 증가할 때마다 성공(Y=1)의 오즈가 몇 배 증가하는지 나타내는 값 표준 로지스틱 분포의 누적함수로 성공 확률을 추정 선형회귀분석 vs 로지스틱 회귀분석 목적 선형회귀분석 로지스틱 회귀분석 종속변수 연속형 변수 (0, 1) 계수 추정법 최소제곱법 최대우도추정법 모형 검정 F-검정, T-검정 카이제곱 검정(x^2^-test) glm() 함수를 활용하여 로지스틱 회귀분석을 실행 3. 의사결정나무 정의 분류함수를 의사결정 규칙으로 이뤄진 나무 모양으로 그리는 방법 연속적으로 발생하는 의사결정 문제를 시각화 계산결과가 의사결정나무에 직접적으로 나타나서 해석이 간편함 주어진 입력값에 대해 출력값을 예측하는 모형 → 분류나무와 회귀나무 모형 예측력과 해석력 의사결정나무의 활용 세분화 데이터를 비슷한 특성을 갖는 몇 개 그룹으로 분할해 그룹별 특성을 발견하는 것 분류 여러 예측변수에 근거해 관측개체의 목표변수 범주를 몇 개 등급으로 분류하고자 하는 경우에 사용 예측 자료에서 규칙을 찾고 이를 이용해 미래 사건을 예측하고자 하는 경우에 사용 차원축소 및 변수선택 많은 예측변수 중 목표변수에 큰 영향을 미치는 변수를 골라내고자 하는 경우에 사용 교호작용효과의 파악 여러 개 예측변수를 결합해 목표변수에 작용하는 규칙을 파악하고자 하는 경우 범주의 병합 또는 연속형 변수의 이산화: 범주형 목표변수의 범주를 소수 몇 개로 병합하거나, 연속형 목표변수를 몇 개의 등급으로 이산화하고자 하는 경우 의사결정나무 특징 장점 결과 설명 용이 모형 만들기가 계산적으로 복잡하지 않음 대용량 데이터에서도 빠르게 만들 수 있음 비정상 잡음 데이터도 민감함 없이 분류 가능 한 변수와 상관성 높은, 다른 불필요한 변수가 있어도 크게 영향 받지 않음 설명변수나 목표변수에 수치형변수와 범주형변수 모두 사용 가능 모형 분류 정확도가 높음 단점 새로운 자료에 대한 과대적합이 발생할 가능성이 높음 분류 경계선 부근 자료값에 대해 오차가 큼 설명변수 간 중요도 판단이 어려움 의사결정나무 분석 과정 성장: 적절한 정지규칙을 만족하면 중단 가지치기 타당성 평가 해석 및 예측 나무의 성장 분리규칙(splitting rule) 분리기준(splitting criterion) 이산형 목표변수 기준값 분리기준 카이제곱 통계량 p값 P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 지니지수 지니지수를 감소시키는 예측변수와 그때의 최적분리에 의해 자식마디 선택 엔트로피 지수 엔트로피 지수가 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 연속형 목표변수 기준값 분리기준 분산분석에서 F통계량 P값이 가장 작은 예측변수와 그때의 최적분리에 의해 자식마디 형성 분산의 감소량 분산 감소량을 최대화하는 기준의 최적분리에 의해 자식마디 형성 정지규칙 더이상 분리가 일어나지 않고 현재 마디가 끝마디가 되도록 하는 규칙 정지기준: 의사결정나무 깊이를 지정, 끝마디의 레코드 수의 최소 개수를 지정 나무의 가지치기(Pruning) 너무 큰 나무모형은 자료를 과대적합, 너무 작은 나무모형은 과소적합할 위험 나무 크기를 모형 복잡도로 볼 수 있으며, 최적 나무 크기는 자료로부터 추정하게 됨 일반적으로 사용되는 방법은 마디에 속하는 자료가 일정 수(가령 5) 이하일 때 분할을 정지 비용 - 복잡도 가지치기를 이용하여 성장시킨 나무를 가지치기하게 됨 4. 불순도의 여러 가지 측도 목표변수가 범주형 변수인 의사결정나무 분류규칙을 선택 카이제곱 통계량 각 셀에 대한 ((실제도수 - 기대도수)의 제곱 / 기대도수) 합으로 구할 수 있음 기대도수 = 열의 합계 x 합의 합계 / 전체합계 지니지수 노드의 불순도를 나타내는 값 지니지수 값이 클수록 이질적이며 순수도가 낮다고 볼 수 있음 엔트로피 지수 열역학에서 쓰는 개념으로 무질서 정도에 대한 측도 엔트로피 지수 값이 클수록 순수도가 낮다고 볼 수 있음 엔트로피 지수가 가장 작은 예측변수와 이때의 최적분리 규칙에 의해 자식마디 형성 5. 의사결정나무 알고리즘 CART 불순도의 측도로 출력변수가 범주형일 경우 지니지수를 이용, 연속형인 경우 분산을 이용한 이진분리 사용 개별 입력변수뿐 아니라 입력변수의 선형결합 중에서 최적의 분리를 찾을 수 있음 C4.5와 C5.0 CART와는 다르게 각 마디에서 다지분리가 가능 범주형 입력변수에 대하여는 범주 수만큼 분리가 일어남 불순도의 측도로는 엔트로피지수 사용 CHAID 가지치기 하지 않고 적당한 크기에서 나무모형의 성장을 중지 입력변수가 반드시 범주형 변수여야 함 불순도 측도로는 카이제곱 통계량 사용 3절. 앙상블 분석 앙상블 주어진 자료로부터 여러 개 예측모형을 만든 후, 조합하여 하나의 최종 예측 모형을 만드는 방법 다중 모델 조합, 분류기 조합 학습방법의 불안정성 학습자료의 작은 변화에 의해 예측모형이 크게 변하는 경우, 그 학습방법은 불안정함 가장 안정적인 방법 1-nearest neighbor: 가장 가까운 자료만 변하지 않으면 예측모형 변하지 않음 선형회귀모형: 최소제곱법으로 추정해 모형 결정 가장 불안정한 방법: 의사결정나무 앙상블 기법의 종류 배깅 주어진 자료에서 여러 개의 붓스트랩 자료를 생성하고 각 붓스트랩 자료에 예측모형을 만든 후 결합하여 최종 예측모형을 만드는 방법 붓스트랩(bootstrap): 주어진 자료에서 동일한 크기 표본을 랜덤 복원추출로 뽑은 자료 보팅(voting): 여러 개 모형으로부터 산출된 결과를 다수결에 의해 최종 결과를 선정하는 과정 배깅에서는 가지치기를 하지 않고 최대로 성장한 의사결정나무를 활용 훈련자료 모집단의 분포를 모르기 때문에 실제 문제에서는 평균예측모형을 구할 수 없음 → 훈련자료를 모집단으로 생각하고 평균예측모형을 구하여 분산을 줄이고 예측력을 향상시킬 수 있음 부스팅 예측력 약한 모형을 결합하여 강한 예측모형을 만드는 방법 훈련오차를 빠르고 쉽게 줄일 수 있음 배깅에 비해 많은 경우의 예측오차가 향상 Adaboost: 이진분류 문제에서 랜덤 분류기보다 조금 더 좋은 분류기 n개에 가중치를 설정하고 n개 분류기를 결합하여 최종 분류기 만드는 방법(단, 가중치 합은 1) 랜덤 포레스트(random forest) 분산이 크다는 의사결정나무 특징을 고려하여 배깅과 부스팅보다 더 많은 무작위성을 줌 약한 학습기를 생성한 후, 이를 선형 결합하여 최종 학습기를 만드는 방법 랜덤한 forest에는 많은 트리가 생성됨 정확도 측면에서 좋은 성과 이론적 설명이나 최종 결과 해석이 어렵지만, 예측력이 매우 높음 4절. 인공신경망 분석1. 인공신경망 분석(ANN) 인공신경망이란? 인간 뇌를 기반으로 한 추론 모델 뉴런: 기본적인 정보처리 단위 인간의 뇌를 형상화한 인공신경망 인간 뇌의 특징 100억 개 뉴런과 6조 개 시냅스의 결합체 인간의 뇌: 컴퓨터보다 빠르고, 복잡하고, 비선형적, 병렬적인 정보 처리 시스템 적응성에 따라 잘못된 답에 대한 뉴런 사이 연결은 약화되고, 올바른 답에 대한 연결이 강화됨 인간 뇌 모델링 뉴런은 가중치 있는 링크로 연결되어 있음 뉴런은 여러 입력 신호를 받으나, 출력 신호는 하나만 생성함 인공신경망의 학습 신경망은 가중치를 반복적으로 조정하며 학습 뉴런은 링크로 연결되어 있고, 각 링크에는 수치적인 가중치가 있음 신경망 가중치를 초기화 → 훈련 데이터로 가중치 갱신 → 신경망 구조 선택 → 활용할 학습 알고리즘 결정 → 신경망 훈련 인공신경망 특징 구조 입력 링크에서 여러 신호를 받아 새로운 활성화 수준을 계산하고 출력 링크로 출력 신호를 보냄 입력 신호는 미가공 데이터 또는 다른 뉴런의 출력이 될 수 있음 출력 신호는 문제의 최종적인 해(solution)가 되거나 다른 뉴런에 입력될 수 있음 뉴런의 계산 뉴런은 전이함수, 즉 활성화 함수를 사용 활성화 함수를 이용해 출력을 결정하며, 입력신호의 가중치 합을 계산하여 임계값과 비교 가중치 합이 임계값보다 작으면 뉴련의 출력은 -1, 같거나 크면 +1을 출력 뉴런의 활성화 함수 시그모이드 함수: 로지스틱 회귀분석과 유사하며 0~1의 확률값을 가짐 softmax 함수: 표준화지수 함수로도 불리며, 출력값이 여러 개로 주어지고 목표치가 다범주인 경우 각 범주에 속할 사후확률을 제공하는 함수 relu 함수: 입력값이 0 이하는 0, 0 이상은 x값을 가지는 함수, 최근 딥러닝에서 많이 활용 단일 뉴런의 학습(단층 퍼셉트론) 퍼셉트론은 선형 결합기와 하드 리미터로 구성 초평면(hyperplane)은 n차원 공간을 두 개의 영역으로 나눔 초평면을 선형 분리 함수로 정의 신경망 모형 구축 시 고려사항 입력변수 신경망 모형은 복잡성으로 인해 입력 자료 선택에 매우 민감 입력변수가 범주형 또는 연속형 변수일 때 아래 조건이 신경망 모형에 적합 범주형 변수: 모든 범주에서 일정 빈도 이상의 값을 갖고 각 범주 빈도가 일정할 때 연속형 변수: 입력변수 값의 범위가 변수간의 큰 차이가 없을 때 연속형 변수의 경우, 분포가 평균을 중심으로 대칭이 아니면 좋지 않은 결과를 도출하므로 아래 방법을 활용 변환: 고객 소득(대부분 평균 미만, 특정 고객 소득이 매우 큰) 로그 변환 범주화: 각 범주 빈도가 비슷해지도록 설정 범주형 변수의 경우 가변수화하여 적용 가능한 경우 모든 범주형 변수는 같은 범위를 갖도록 가변수화 하는 것이 좋음 가중치의 초기값과 다중 최소값 문제 역전파 알고리즘은 초기값에 따라 결과가 많이 달라짐 → 초기값 선택은 매우 중요한 문제 가중치가 0이면 시그모이드 함수는 선형, 신경망 모형은 근사적으로 선형모형이 됨 일반적으로 초기값은 0 근처로 랜덤하게 선택 → 초기 모형은 선형모형에 가깝고, 가중치 값이 증가할수록 비선형모형이 됨 참고: 초기값이 0이면 반복해도 값이 전혀 변하지 않고, 너무 크면 좋지 않은 해를 주는 문제점 내포 학습모드 온라인 학습모드(online learning mode) 각 관측값을 순차적으로 하나씩 신경망에 투입하여 가중치 추정값이 매번 바뀜 일반적으로 속도가 빠름, 훈련자료에 유사값 많은 경우 그 차이가 더 두드러짐 훈련자료가 비정상성과 같이 특이한 성질을 가진 경우가 좋음 국소최솟값에서 벗어나기 더 쉬움 확률적 학습모드(probabilistic learning mode) 온라인 학습모드와 같으나, 신경망에 투입되는 관측값의 순서가 랜덤 배치 학습모드(batch learning mode) 전체 훈련자료를 동시에 신경망에 투입 은닉층(hidden layer)과 은닉노드(hidden node)의 수 신경망을 적용 시, 가장 중요한 부분: 모형의 선택 은닉층과 은닉노드가 많으면 가중치가 많아져서 과대 적합 문제 발생 은닉층과 은닉노드가 적으면 과소적합 문제 발생 은닉층 수가 하나인 신경망: 범용 근사자 → 모든 매끄러운 함수 근사적 표현 가능 은닉노드 수는 적절히 큰 값으로 놓고 가중치를 감소시키며 적용하는 것이 좋음 과대 적합 문제 신경망에서는 많은 가중치를 추정해야 하므로 과대적합 문제가 빈번히 발생 알고리즘 조기종료와 가중치 감소 기법으로 해결할 수 있음 모형 적합 과정에서 검증오차가 증가하면 반복을 중지하는 조기종료 시행 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법 활용 5절. 군집분석1. 군집분석 개요 각 객체(대상)의 유사성을 측정하여 유사성이 높은 대상 집단을 분류하고, 군집에 속한 객체들의 유사성과 서로 다른 군집에 속한 객체간 상이성을 규명하는 분석 방법 특성에 따라 고객을 여러 개의 배타적인 집단으로 나눔 결과는 구체적인 군집분석 방법에 따라 차이 날 수 있음 군집 개수나 구조에 관한 가정 없이 데이터 사이 거리를 기준으로 군집화 유도 마케팅 조사에서 소비자의 상품구매행동이나 life style에 따른 소비자군을 분류하여 시장 전략 수립에 활용 특징 요인분석과의 차이 요인분석은 유사한 변수를 함께 묶는 목적 판별분석과의 차이 판별분석은 사전에 집단이 나뉜 자료를 통해 새로운 데이터를 기존 집단에 할당하는 것이 목적 2. 거리 군집분석에서는 관측 데이터 간 유사성이나 근접성을 측정해 어느 군집으로 묶을 수 있는지 판단해야 함 아래 각 거리 식은 교재 참고 연속형 변수의 경우 유클리디안 거리 데이터 유사성 측정할 때 많이 사용하는 거리, 통계적 개념 내포 x → 변수 산포 정도가 감안되지 않음 표준화 거리 해당변수 표준편차로 척도 변환 후, 유클리드안 거리를 계산하는 방법 표준화하게 되면 척도 차이, 분산 차이로 인한 왜곡을 피할 수 있음 마할라노비스 거리 통계적 개념이 포함된 거리이며 변수들의 산포를 고려하여 이를 표준화한 거리 두 백터 사이 거리를 산포를 의미하는 표본공분산으로 나눠주어야 함 그룹에 관한 사전 지식 없이는 표본공분산S를 계산할 수 없으므로 사용하기 곤란 체비셰프 거리 맨하탄 거리 유클리디안 거리와 함께 가장 많이 사용되는 거리 맨하탄 도시 건물에서 건물을 가기 위한 최단 거리를 구하기 위해 고안 캔버라 거리 민코우스키 거리 맨하탄 거리와 유클리디안 거리를 한 번에 표현한 공식 L1 거리(맨하탄거리), L2 거리(유클리디안 거리)라고 불림 범주형 변수의 경우 자카드 거리 자카드 계수 코사인 거리 유사도 기준으로 문서를 분류/그룹핑할 때 유용하게 사용 코사인 유사도 두 개체 백터 내적의 코사인 값을 이용하여 측정된 백터간의 유사한 정도 3. 계층적 군집분석 n개의 군집으로 시작해 점차 군집 개수를 줄여가는 방법 계층적 군집을 형성하는 방법에는 합병형 방법과 분리형 방법이 있음 최단연결법(single linkage, nearest neighbor) n*n 거리행렬에서 거리가 가장 가까운 데이터를 묶어서 군집 형성 군집과 군집 또는 데이터와의 거리 계산 시, 최단거리(min)를 거리로 계산하여 거리행렬 수정 진행 수정된 거리행렬에서 거리가 가까운 데이터/군집을 새로운 군집으로 형성 최장연결법(complete linkage, farthest neighbor) 군집과 군집/데이터와 거리 계산 시, 최장거리(max)를 거리로 계산하여 거리행렬을 수정하는 방법 평균연결법(average linkage) 군집과 군집/데이터와 거리 계산 시, 평균(mean)을 거리로 계산하여 거리행렬을 수정하는 방법 와드연결법(ward linkage) 군집 내 편차들의 제곱합을 고려한 방법 군집간 정보 손실 최소화를 위해 군집화 진행 군집화 거리행렬을 통해 가장 가까운 거리의 객체들간 관계를 규명하고 덴드로그램을 그림 덴드로그램을 보고 군집 개수를 변화해가며 적절한 군집 수 선정 군집 수는 분석 목적에 따라 선정할 수 있지만, 5개 이상은 잘 활용하지 않음 군집화 단계 거리행렬을 기준으로 덴드로그램을 그림 덴드로그램 최상단부터 세로축 개수에 따라 가로선을 그어 군집 개수 선택 각 객체 구성을 고려하여 적절한 군집 수 선정 4. 비계층적 군집분석n개의 개체를 g개의 군집으로 나눌 수 있는 모든 가능한 방법을 점검해 최적화한 군집을 형성하는 것 K-평균 군집분석 주어진 데이터를 k개의 클러스터로 묶는 알고리즘 각 클러스터와 거리 차이 분산을 최소화하는 방식으로 동작 K-평균 군집분석 과정 원하는 군집 개수와 초기 값(seed)을 정해 seed 중심으로 군집 형성 각 데이터를 거리가 가장 가까운 seed가 있는 군집으로 분류 각 군집의 seed 값을 다시 계산 모든 개체가 군집으로 할당될 때까지 위 과정 반복 K-평균 군집분석 특징 거리 계산을 통해 군집화가 이루어지므로 연속형 변수에 활용 가능 K개의 초기 중심값은 임의 선택 가능하며 가급적이면 멀리 떨어지는 것이 바람직함 초기 중심값을 임의로 선택할 때, 일렬로 선택하면 군집이 혼합되지 않고 층으로 나눠질 수 있어 주의해야 함 초기 중심값 선정에 따라 결과가 달라질 수 있음 초기 중심으로부터 오차 제곱합을 최소화하는 방향으로 군집이 형성되는 탐욕적(greedy) 알고리즘이므로 안정된 군집은 보장하나 최적이라는 보장은 없음 장점 단점 알고리즘 단순, 수행 빠름 → 분석 방법 적용 용이, 계층적 군집분석에 비해 많은 양의 데이터를 다룰 수 있음, 내부 구조에 대한 사전정보 없이 의미 있는 자료구조 찾을 수 있음, 다양한 형태 데이터에 적용 가능 군집 수, 가중치와 거리 정의가 어려움, 사전에 주어진 목적이 없으므로 결과 해석이 어려움, 잡음이나 이상값의 영향을 많이 받음, 볼록한 형태가 아닌 군집이 존재할 경우에는 성능 떨어짐, 초기 군집 수 결정이 어려움 5. 혼합 분포 군집(mixture distribution clustering) 개요 모형 기반 군집 방법 데이터가 k개의 모수적 모형의 가중합으로 표현되는 모집단 모형으로부터 나왔다는 가정 하에서 모수와 함께 가중치를 자료로부터 추정하는 방법 사용 K개의 각 모형은 군집을 의미, 각 데이터는 추정된 k개의 모형 중 어느 모형으로부터 나왔을 확률이 높은지에 따라 군집 분류가 이루어짐 혼합모형에서 모수와 가중치의 추정(최대가능도 추정)에는 EM 알고리즘이 사용됨 혼합 분포모형으로 설명할 수 있는 데이터 형태 자료의 분포형태가 다봉형의 형태 교재 참고 EM(Expectation-Maximization) 알고리즘의 진행 과정 각 자료에 대해 Z의 조건부분포(어느 집단에 속할지에 관한)로부터 조건부 기댓값을 구할 수 있음 관측변수 X와 잠재변수 Z를 포함하는 (X,Z)에 대한 로그-가능도함수에 Z 대신 상수값인 Z의 조건부 기댓값을 대입하면 로그-가능도함수를 최대로 하는 모수를 쉽게 찾을 수 있음, (M-단계) 갱신된 모수 추정치에 위 과정을 반복하면 수렴하는 값을 얻게 되고 이는 최대 가능도 추정치로 사용될 수 있음 E-단계: 잠재변수 Z의 기대치 계산 M-단계: 잠재변수 Z의 기대치를 이용하여 파라미터 추정 혼합 분포 군집모형의 특징 K-평균군집 절차와 유사하지만 확률분포를 도입하여 군집 수행 군집을 몇 개의 모수로 표현할 수 있으며, 서로 다른 크기나 모양의 군집을 찾을 수 있음 EM 알고리즘을 이용한 모수 추정에서 데이터가 커지면 수렴에 시간이 걸릴 수 있음 군집 크기가 너무 작으면 추정 정도가 떨어지거나 어려울 수 있음 K-평균군집과 같이 이상치 자료에 민감 → 사전 조치 필요 6. SOM(Self Organizing Map) SOM 자가조직화지도 알고리즘은 코호넨에 의해 제시, 개발 → 코호넨 맵(Ko-honen Maps)이라고도 알려져 있음 SOM은 비지도 신경망으로 고차원의 데이터를 이해하기 쉬운 저차원의 뉴런으로 정렬하여 지도 형태로 형상화 형상화는 입력 변수의 위치 관계를 그대로 보존한다는 특징이 있음 → 실제 공간의 입력변수가 가까이 있으면 지도상에도 가까운 위치에 있음 구성 SOM 모델은 두 개의 인공신경망 층으로 구성되어 있음 입력층(Input layer, 입력벡터를 받는 층) 입력변수 개수와 뉴런 수가 동일하게 존재 입력층 자료는 학습을 통해 경쟁층에 정렬되는데, 이를 지도라 부름 입력층에 있는 각각의 뉴런은 경쟁층에 있는 각각의 뉴런과 완전 연결(fully connected)되어 있음 경쟁층(Competitive layer, 2차원 격자(grid)로 구성된 층) 입력백터 특성에 따라 백터가 한 점으로 클러스터링 되는 층 SOM은 경쟁 학습으로 각각 뉴런이 입력백터와 얼마나 가까운가를 계산하여 연결 강도(connection weight)를 반복적으로 재조정하여 학습 위 과정을 거치며 연결강도는 입력 패턴과 가장 유사한 경쟁층 뉴런이 승자가 됨 입력층 표본 백터에 가장 가까운 프로토타입 백터를 BMU(Best-Matching-Unit)라고 하며, 코호넨 승자 독점의 학습 규칙에 따라 위상학적 이웃(topological neighbors)에 대한 연결 강도를 조정 승자 독식 구조로 인해 경쟁층에는 승자 뉴런만이 나타나며, 승자와 유사한 연결 강도를 갖는 입력 패턴이 동일한 경쟁 뉴런으로 배열됨 특징 고차원의 데이터를 저차원의 지도 형태로 형상화 → 시각적으로 이해가 쉬움 입력변수의 위치 관계를 그대로 보존하기 때문에 실제 데이터가 유사하면 지도상에서 가깝게 표현 → 패턴 발견, 이미지 분석 등에서 뛰어난 성능을 보임 역전파(Back Propagation): 알고리즘 등을 이용하는 인공신경망과 달리 단 하나의 전방 패스를 사용함으로써 속도가 매우 빠름 → 실시간 학습처리를 할 수 있는 모형 SOM과 신경망 모형의 차이점 구분 신경망 모형 SOM 학습 방법 오차역전파법 경쟁학습방법 구성 입력층, 은닉층, 출력층 입력층, 2차원 격자 형태의 경쟁층 기계 학습 방법의 분류 지도학습(Supervised Learning) 비지도학습(Unsupervised Learning) ###7. 최신 군집분석 기법 교재 참고 6절. 연관분석1. 연관규칙 **연관규칙분석(Association Analysis)**의 개념 흔히 장바구니분석 또는 서열분석이라고 불림 기업 데이터베이스에서 상품의 구매, 서비스 등 일련의 거래 또는 사건 사이 규칙을 발견하기 위해 적용 장바구니 분석: 장바구니에 무엇이 같이 들어 있는지에 관한 분석 서열 분석: ‘A를 구매한 다음에 B를 구매한다’ 연관규칙의 형태 조건과 반응의 형태(if-then)로 이루어져 있음(Item set A) → (Item set B) If A then B: 만일 A가 일어나면 B가 일어난다. - 아메리카노를 마시는 손님 중 10%가 브라우니를 먹는다. - 샌드위치를 먹는 고객의 30%가 탄산수를 함께 마신다. 연관규칙의 측도 산업 특성에 따라 지지도, 신뢰도, 향상도 값을 잘 보고 규칙을 선택해야 함 지지도(support) 전체 거래 중 항목 A와 항목 B를 동시에 포함하는 거래의 비율 지지도 = P(A∩B) = $\\frac{A와 B 동시에 포함된 거래 수}{전체 거래 수}$ = $\\frac{A∩B}{전체}$ 신뢰도(confidence) 항목 A를 포함한 거래 중에서 항목 A와 항목 B가 같이 포함될 확률 → 연관성 정도 파악 가능 신뢰도 = $\\frac{P(A∩B)}{P(A)}$ = $\\frac{A와 B 동시에 포함된 거래 수}{A 포함하는 거래 수}$ = $\\frac{지지도}{P(A)}$ 향상도(Lift) A가 구매되지 않았을 때, 품목 B의 구매확률에 비해 A가 구매됐을 때 품목 B의 구매확률 증가 비 연관규칙 A → B는 품목 A와 품목 B의 구매가 서로 관련 없는 경우에 향상도가 1이 됨 향상도 = $\\frac{P(B|A)}{P(B)}$ = $\\frac{P(A∩B)}{P(A)P(B)}$ = $\\frac{A와 B 동시에 포함된 거래 수}{A를 포함하는 거래 수 X B를 포함하는 거래 수}$ = $\\frac{신뢰도}{P(B)}$ 연관규칙의 절차 최소 지지도보다 큰 집합만을 대상으로 높은 지지도를 갖는 품목 집합을 찾는 것 처음에는 5%로 잡고 규칙이 충분히 도출되는지 보고 다양하게 조절하여 시도 처음부터 너무 낮은 최소 지지도를 선정하는 것은 많은 리소스가 소모 절차 최소 지지도 결정 → 품목 중 최소 지지도 넘는 품목 분류 → 2가지 품목 집합 생성 → 반복 수행해 빈발품목 집함 찾기 연관규칙의 장단점 장점 단점 조건 반응으로 표현되는 연관선 분석 결과를 쉽게 이해할 수 있음(탐색적 방법), 강력한 비목적성 분석기법으로 분석 방향이나 목적이 특별히 없는 경우 목적변수가 없어 유용하게 활용, 사용이 편리한 분석 데이터 형태로 거래 내용에 관한 데이터를 변환 없이 그 자체로 이용할 수 있는 간단한 자료 구조를 가짐, 분석을 위한 계산이 간단함 품목수가 증가하면 분석에 필요한 계산은 기하급수적으로 늘어남,세분화한 품목을 갖고 연관성 규칙을 찾으면 의미 없는 분석이 될 수 있음, 거래량 적은 품목은 당연히 포함된 거래수가 적을 것이고 규칙 발견 시 제외하기가 쉬움 순차패턴(Sequence Analysis) 동시에 구매될 가능성이 큰 상품군을 찾는 연관성 분석에, 시간이라는 개념을 포함해 순차적으로 구매 가능성이 큰 상품군을 찾는 것 연관성분석에서의 데이터 형태에서 각각의 고객으로부터 발생한 구매시점에 대한 정보가 포함됨 2. 기존 연관성분석의 이슈 대용량 데이터에 관한 연관성 분석 불가능 시간이 많이 걸리거나 기존 시스템에서 실행 시, 시스템 다운 현상 발생 가능 3. 최근 연관성분성 동향 1세대 알고리즘인 Apriori나 2세대인 FP-Growth에서 발전하여 3세대의 FPV를 이용해 메모리를 효율적으로 사용 → SKU 레벨인 연관성분석을 성공적으로 적용 거래내역에 포함된 모든 품목 개수가 n개일 때, 품목의 전체집합에서 추출할 수 있는 품목 부분집합 개수는 2^n^-1(공집합 제외)개, 가능한 모든 연관규칙 개서는 3^n^ - 2^n+1^ + 1개 Aprirori: 모든 가능한 품목 부분집합 개수를 줄이는 방식으로 작동 FP-Growth: 거래내역 안에 포함된 품목 개수를 줄여 비교하는 횟수를 줄이는 방식으로 작동 Aprirori 알고리즘 빈발항목집합: 최소 지지도보다 큰 지지도 값을 갖는 품목 집합 모든 품목집합에 대한 지지도를 전부 계산하지 않고, 최소 지지도 이상의 빈발항목집합을 찾은 후 그것들에 대해서만 연관규칙을 계산하는 것 1994년에 발표된 알고리즘으로 구현과 이해가 쉬우나, 지지도 낮은 후보 집합 생성 시 아이템 개수가 많아지면 계산 복잡도가 증가하는 문제 발생 FP-Growth 알고리즘 후보 빈발항목집합을 생성하지 않고, FP-Tree(Frequent Pattern Tree)를 만든 후 분할정복 방식으로 Apriori 알고리즘보다 더 빠르게 빈발항목집합을 추출할 수 있는 방법 Apriori 알고리즘의 약점을 보안하기 위해 고안 → 데이터베이스 스캔 횟수가 작고 빠르게 분석 가능 4. 연관성분석 활용방안 장바구니 분석의 경우는 실시간 상품추천을 통한 교차판매에 응용 순차패턴 분석은 A를 구매한 사람인데 B를 구매하지 않은 경우, B를 추천하는 교차판매 캠페인에 사용 5. 연관성분석 예제 교재 참고","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ADsP","slug":"Study/ADsP","permalink":"https://ne-choi.github.io/categories/Study/ADsP/"}],"tags":[{"name":"ADsP","slug":"ADsP","permalink":"https://ne-choi.github.io/tags/ADsP/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"}],"author":"ne_choi"},{"title":"ADsP 자격증: Part03. 데이터 분석_1","slug":"Study/ADsP/Part03_데이터_분석_1","date":"2020-11-15T15:00:00.000Z","updated":"2021-01-20T03:39:04.060Z","comments":true,"path":"/2020/11/16/Study/ADsP/Part03_데이터_분석_1/","link":"","permalink":"https://ne-choi.github.io/2020/11/16/Study/ADsP/Part03_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_1/","excerpt":"","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터 분석 개요1절. 데이터 분석 기법의 이해1. 데이터 처리 데이터 분석 통계 기반이나, 통계지식과 복잡한 가정이 상대적으로 적은 실용적인 분야 활용 대기업은 데이터 웨어하우스(DW)와 데이터마트(DM)를 통해 분석 데이터를 가져와 사용 신규 시스템이나 DW에 포함되지 못한 자료는, 기존 운영 시스템(Legacy)나 스테이징 영역(staging area)과 ODS(Operational Data Store)에서 데이터를 가져와 DW에서 가져온 내용과 결합하여 활용 가능 단, 운영시스템에 직접 접근해 데이터를 활용하는 것은 매우 위험 → 스테이징 영역 데이터는 운영시스템에서 임시로 저장된 데이터기에 가급적 클린징 영역인 ODS에서 데이터 전처리를 하여 DW나 DM과 결합해 사용 최종 데이터 구조로 가공 데이터 마이닝 분류 분류값과 입력변수를 연관시켜 인구통계, 요약변수, 파생벽수 등 산출 정형화된 패턴 처리 비정형 데이터나 소셜 데이터는 정형화된 패턴으로 처리해야 함 비정형 데이터 DMBS에 저장됐다가 텍스트 마이닝을 거쳐 데이터 마트와 통합 관계형 데이터 DBMS에 저장되어 사회 신경망 분석을 거쳐 분석 결과 통계값이 마트와 통합되어 활용 2. 시각화(시각화 그래프) 가장 낮은 수준의 분석이지만, 제대로 사용하면 복잡한 분석보다도 효율적 대용량 데이터를 다루는 빅데이터 분석에서 시각화는 필수 탐색적 분석을 할 때, 시각화는 필수 SNA 분석(사회연결망 분석)을 할 때, 자주 활용 3. 공간분석(GIS) 공간분석(Spatial Analysis): 공간적 차원과 관련된 속성을 시각화하는 분석 지도 위에 관련 속성을 생성하고 크기, 모양, 선, 굵기 등으로 구분하여 인사이트를 얻음 4. 탐색적 자료 분석(EDA) 탐색적 분석 다양한 차원과 값을 조합하며 특이점이나 의미 있는 사실을 도출하여 분석의 최종 목적을 달성하는 과정 데이터의 특징과 내재하는 구조적 관계를 알아내기 위한 기법의 통칭 EDA의 4가지 주제 저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성 탐색적 분석 효율 예시-데이터 이해 단계, 변수 생성 단계, 변수 선택 단계에서 활용 5. 통계분석 통계 어떤 현상을 종합적으로 알아보기 쉽게 일정한 체계에 따라 숫자, 표, 그림 형태로 나타낸 것 기술통계(descriptive statistics) 모집단으로부터 표본을 추출하고 표본이 가진 정보를 쉽게 파악하도록 데이터를 정리하거나 요약하기 위해 하나의 숫자 또는 그래프 형태로 표현하는 절차 추측(추론)통계(inferential statistics) 모집단으로부터 추출된 표본의 표본통계량으로부터 모집단 특성인 모수에 관해 통계적으로 추론하는 절차 활용 분야 정부 경제정책 수립 / 평가 근거자료(통계청 실업률, 고용률, 물가지수) 농업(가뭄, 수해, 병충해에 강한 품종 개발 및 개량) 의학(치료 방법의 효과나 신약 개발을 위한 임상실험 결과 분석) 경영(제품 개발, 품질관리, 시장조사, 영업관리) 스포츠(선수 체질향상, 경기 분석, 전략 분석, 선수 평가, 기용) 6. 데이터 마이닝 데이터 마이닝 대표적인 고급 데이터 분석법 대용량 자료를 요약하고 미래 예측을 목표로 자료의 관계, 패턴, 규칙을 탐색하고 모형화 이전에 알려지지 않은 유용한 지식을 추출하는 분석 방법론 데이터베이스에서의 지식 탐색 데이터 웨어하우스에서 데이터 마트를 생성하면서 각 데이터 속성을 사전분석하여 지식을 얻는 방법 기계학습(machine learning) 인공지능의 한 분야 컴퓨터가 학습할 수 있도록 알고리즘과 기술을 개발하는 분야 인공신경망, 의사결정나무, 클러스터링, 베이지안 분류, SVM 등 패턴인식(pattern recognition) 원자료를 이용하여 사전지식, 패턴에서 추출된 통계 정보를 기반으로 자료 또는 패턴을 분류 장바구니 분석, 연관 규칙 활용 분야 데이터베이스 마케팅(고객 행동정보를 활용한 목표 마케팅, 고객 세분화, 장바구니 분석, 추천 시스템) 신용평가 및 조기경보시스템(금융기관에서 신용카드 발급, 보험, 대출 발생 시) 생물정보학(세포 유전자 분석으로 질병 진단과 치료법, 신약 개발) 텍스트마이닝(전자우편, SNS 등 디지털 텍스트 정보로 고객성향, 감정, 사회관계망 분석) 2장. R 프로그래밍 기초1절. R 소개1. 데이터 분석 도구의 현황 R의 탄생 오픈소스 프로그램으로 통계, 데이터 마이닝과 그래프를 위한 언어 최신 통계 분석과 마이닝 기능 제공 세계적인 사용자와 다양한 예제 공유 가능 패키지가 수시로 업데이트 됨 분석 도구 비교 SAS SPSS 오픈소스 R 프로그램 비용 유료, 고가 유료, 고가 오픈소스 설치 용량 대용량 대용량 모듈화로 간단함 다양한 모듈 지원 및 비용 별도 구매 별도 구매 오픈 소스 최근 알고리즘 및 기술 반양 느림 다소 느림 매우 빠름 학습자료 입수 편의성 유료 도서 위주 유료 도서 위주 공개 논문 및 자료 많음 질의용 공개 커뮤니티 NA NA 매우 활발 R의 특징 오픈소스 프로그램 사용자 커뮤니티에 도움 요청이 많음 많은 패키지가 수시 업데이트 그래픽 및 성능 프로그래밍, 그래픽 측명 등 사용 프로그램과 대등하거나 월등함 시스템 데이터 저장 방식 각 세션 사이마다 시스템에 데이터셋을 저장 → 매번 데이터 로딩 필요가 없음 명령어 스토리 저장 가능 모든 운영체제 윈도우, 맥, 리눅스 운영체제에서 사용 가능 표준 플랫폼 S 통계 언어 기반으로 구현 R/S 플랫폼은 통계전문가의 사실상 표준 플랫폼 객체지향 언어이며 함수형 언어 통계 기능뿐 아니라 일반 프로그래밍 언어처럼 자동화하거나 새로운 함수 생성 가능 객체지향 언어의 특징 SAS, SPSS 회귀 분석 시, 화면에 결과가 나와 추가 작업이 필요 R은 추정계수, 표준오차, 잔차 등 결괏값을 객체에 저장할 수 있어서 활용이 쉬움 함수형 언어의 특징 깔끔하고 단축된 코드 코드 실행이 빠름 단순한 코드로 디버깅 노력 감소 병렬 프로그래밍으로의 전환 용이 R 스튜디오 오픈소스이며 다양한 운영체계 지원 메모리에 변수가 어떻게 되어 있는지, 타입이 무엇인지를 볼 수 있음 스크립트 관리와 도큐먼테이션이 편리 코딩은 스크립트용 프로그래밍으로 어렵지 않게 자동화 가능 래틀(Rattle)은 GUI가 패키지와 긴밀하게 결합외어 있어 정해진 기능만 사용 가능 → 업그레이드가 제대로 되지 않으면 통합성에 문제 발생 R 기반 작업 환경 R 메모리: 64bit 유닉스- 무제한, x86 64bit- 128TB, 64bit 윈도우- 8TB 2절. R 기초 교재 참고 3절. 입력과 출력1. 데이터 분석 과정 분석자가 분석 목적에 맞는 방법론을 선택하여 얻은 결과를 해석하는 과정 INPUT → ANALYSIS → OUTPUT 2. R에서의 데이터 입력과 출력 R에서 다룰 수 있는 파일 타입 Tab-delimited text, Comma-separated text, Excel file, JSON file, HTML/XML file, Database, (other) Statistical SW’s file 4절. 데이터 구조와 데이터 프레임 11. 백터(Vector) 백터들은 동질적 한 백터의 모든 원소는 같은 자료형 또는 같은 모드(mode)를 가짐 백터는 위치로 인덱스 됨 V[2]는 V 백터의 2번째 원소 백터는 인덱스를 통해 여러 개 원소로 구성된 하위 백터를 반환할 수 있음 V[c(2,3)]은 V 백터의 2번째, 3번째 원소로 구성된 하위 백터 백터 원소들은 이름을 가질 수 있음 V &lt;- c(10,20,30); names(v) &lt;- c(&quot;Moe&quot;, &quot;Larry&quot;, &quot;Curly&quot;) v[&quot;Larry&quot;] Larry 20 2. 리스트(Lists) 리스트는 이질적 여러 자료형 원소가 포함될 수 있음 리스트는 위치로 인덱스 됨 L[[2]]는 L 리스트의 2번째 원소 리스트에서 하위 리스트 추출 가능 L[c(2,3)]은 L 리스트의 2번째, 3번째 원소로 이루어진 하위 리스트 리스트의 원소들은 이름을 가질 수 있음 L[[“Moe”]]와 L$Moe는 둘 다 “Moe”라는 이름의 원소를 지칭 3. R에서의 자료 형태(mode) 객체 예시 모드 숫자 3.1415 수치형(numeric) 숫자 백터 c(2,3,4,5,5) 수치형(numeric) 문자열 “Tom” 문자형(character) 문자열 백터 c(“Tom”,”Yoon”,”Kim”) 문자형(character) 요인 factor(c(“A”,”B”,”C”)) 수치형(numeric) 리스트 list(“Tom”,”Yoon”,”Kim”) 리스트(list) 데이터 프레임 data.frame(x=1:3, y=c(“Tom”,”Yoon”,”Kim”)) 리스트(list) 함수 print 함수(function) 4. 데이터 프레임(data frames) 강력하고 유연한 구조, SAS 데이터셋을 모방해서 만들어짐 데이터 프레임의 리스트 원소는 백터 또는 요인 백터와 요인은 데이터 프레임의 열 백터와 요인은 동일한 길이 데이터 프레임은 표 형태의 데이터 구조, 열별로 다른 데이터 형식을 가질 수 있음 열에는 이름이 있어야 함 데이터 프레임 원소 접근 방법 b[1]; b[&quot;empno&quot;] b[[i]]; b[[&quot;empno&quot;]] b$empno 5. 그밖의 데이터 구조 단일값(Scalars) R에서는 원소가 하나인 백터로 인식/처리pi length(pi) 행렬(Matrix) R에서는 차원을 가진 백터로 인식a &lt;- 1:9 dim(a) &lt;- c(3,3) a 배열(Arrays) 행렬에 3차원 또는 n차원까지 확장된 형태 주어진 백터에 더 많은 차원을 부여해 배열 생성b &lt;- 1:12 dim(b) &lt;- c(2,3,2) 요인(Factors) 백터처럼 생겼지만, R에서는 백터에 있는 고유값(unique key) 정보를 얻는데, 고유값들을 요인의 수준(level)이라고 함 요인의 주된 2가지 사용처: 범주형 변수, 집단 분류 6. 백터, 리스트, 행렬 다루기 행렬(Matrix)은 R에서 차원을 가진 백터이며, 텍스트마이닝과 소셜 네트워크 분석 등에 활용 재활용 규칙(Recycling Rule) 길이가 다른 두 백터 연산을 할 때, R은 짧은 백터의 처음으로 돌아가 연산이 끝날 때까지 원소를 재활용 3장. 데이터 마트1절. 데이터 변경 및 요약1. R reshape를 이용한 데이터 마트 개발 데이터 마트 데이터 웨어하우스와 사용자 사이의 중간층에 위치한 것 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스 데이터 마트 내 대부분의 데이터는 데이터 웨어하우스로부터 복제 또는 자체적으로 수집되거나 관계형/다차원 데이터 베이스를 이용해 구축 CRM 관련 업무 중 핵심: 고객 데이터 마트 구축 동일한 데이터셋 활용 시, 데이터 마트를 어떻게 구축하느냐에 분석 효과 차이를 만듦 요약변수 수집된 정보를 분석에 맞게 종합한 변수 가장 기본적인 변수로 총 구매 금액, 금액, 횟수, 구매여부 등 데이터 분석을 위해 만들어지는 변수 많은 모델을 공통으로 사용할 수 있어 재활용성이 높음 합계, 횟수와 같이 간단한 구조이므로 자동화하여 구측 가능 단점: 얼마 이상이면 구매하더라도 기준값 의미 해석이 애매할 수 있음 → 연속형 변수를 그룹핑해 사용하는 것이 좋음 파생변수 사용자(분석자)가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여한 변수 매우 주관적일 수 있으므로 논리적 타당성을 갖추어 개발해야 함 세분화, 고객행동 예측, 캠페인 반응 예측에 활용 파생변수는 상황에 따라 특정 상황에만 유의미하지 않고 대표성을 띄게 해야 함 reshape의 활용 reshape 패키지에는 melt()와 cast()라는 2개 핵심 함수가 있음 melt(): 쉬운 casting을 위해 적당한 형태로 만들어주는 함수 melt(data, id=…) cast(): 데이터를 원하는 형태로 계산, 변형하는 함수 cast(data, formula=… ~ variable, fun) 변수를 조합해 변수명을 만들고 변수를 시간, 상품 등 차원과 결합해 다양한 요약변수와 파생변수를 쉽게 생성하여 데이터 마트를 구성할 수 있게 함 2. sqldf를 이용한 데이터 분석 sqldf는 R에서 sql 명령어를 사용 가능하게 하는 패키지 SAS에서의 proc sql과 같은 역할을 하는 패키지 명령어 차이(sql, R) sql: select * from [data frame], R: sqldf(“select * from [data frame]”) sql: select * from [data frame] numrows 10, R: sqldf(“select * from [data frame] limit 10”) sql: select * from [data frame] where [col] = ‘char%’, R: sqldf(“select * from [data frame] where [col] like ‘char%’ “) 3. plyr을 이용한 데이터 분석 apply 함수에 기반해 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지 split - apply - combine: 데이터 분리, 처리, 결합 등 필수적인 처리 기능 제공 array data frame list nothing array aaply adply alply a_ply data frame daply ddply dlply d_ply list laply ldply llply l_ply n replicates raply rdply rlply r_ply function arguments maply mdply mlply m_ply 4. 데이터 테이블 data.table 패키지는 R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나 data.table은 큰 데이터를 탐색, 연산, 병합하는 데 유용 기존 data.frame 방식보다 월등히 빠른 속도 특정 column을 key 값으로 색인 지정 후, 데이터 처리 빠른 그루핑과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용(속도차 큼) 2절. 데이터 가공1. Data Exploration Data Exploration 데이터 분석을 위해 구성된 데이터 변수들의 상태를 파악 종류 head(데이터셋), tail(데이터셋) summary(데이터셋) 수치형변수: 최대값, 최소값, 평균, 1사분위수, 2사분위수(중앙값), 3사분위수 명목형변수: 명목값, 데이터 개수 2. 변수 중요도 변수 중요도 변수 선택법과 유사한 개념으로 모형을 생성하여 사용된 변수의 중요도를 살피는 과정 종류 klaR 패키지 특정 변수가 주어졌을 때, 클래스가 어떻게 분류되는지에 관한 에러율을 계산하고 그래픽으로 결과를 보여주는 기능 greedy.wilks(): 세분화를 위한 stepwise forward 변수 선택을 위한 패키지, 종속변수에 가장 영향력을 미치는 변수를 wilks lambda를 활용하여 변수 중요도 정리 (Wilk’s Lambda = 집단내분산/총분산) 3. 변수의 구간화 변수의 구간화 연속형 변수를 분석 목적에 맞게 활용하기 위해 구간화하여 모델링에 적용 일반적으로 10진수 단위로 구간화, 구간을 5개로 나누는 것이 보통이며 7개 이상의 구간을 만들지 않음 신용 평가 모형, 고객 세분화 같은 시스템에서 모형에 활용하는 각 변수를 구간화해서 구간별로 점수를 적용하는 스코어링 방식으로 활용 구간화 방법 binning 신용평가모형 개발에서 연속형 변수(부채비율 등)를 범주형 변수로 구간화하는데 자주 활용 의사결정나무 세분화 또는 예측에 활용되는 의사결정나무 모형을 사용해 입력변수 구간화 가능 동일한 변수를 여러 번의 분리 기준으로 사용 가능하기 때문에, 연속변수가 반복적으로 선택될 경우 각각 분리 기준값으로 연속형 변수를 구간화할 수 있음 3절. 기초 분석 및 데이터 관리1. 데이터 EDA(탐색적 자료 분석) 데이터 분석에 앞서 전체적으로 데이터 특징을 파악하고 다양한 각도로 데이터에 접근 summary()를 이용해 데이터의 기초통계량 확인 2. 결측값 인식 결측값은 NA, 99999999, ‘ ‘(공백), Not Answer 등으로 표현 결측값 자체에 의미가 있는 경우도 있음: 쇼핑몰 중 특정 거래 자체가 존재하지 않는 경우, 아주 부자이거나 아주 가난한 경우 정보를 잘 채우지 않음 결측값 처리는 전체 작업속도에 많은 영향을 줌 3. 결측값 처리 방법 단순 대치법(Single Imputation) completes analysis 결측값이 존재하는 레코드 삭제 평균 대치법(Mean Imputation) 관측 또는 실험을 통해 얻어진 데이터의 평균으로 대치 비조건부 평균 대치법: 관측 데이터 평균으로 대치 조건부 평균 대치법(regression imputation): 회귀분석을 활용한 대치법 단순확률 대치법(Single Stochastic Imputation) 평균 대치법에서 추정량 표준 오차의 과소 추정문제를 보완하고자 고안된 방법 Hot-deck 방법, nearest neighbor 방법 등 다중 대치법(Multiple Imputation) m번의 대치를 통해 m개의 가상적 완전 자료를 만드는 방법 1단계: 대치(imputation step), 2단계: 분석(Analysis step), 3단계: 결합(combination step) Amelia-time series cross sectional data set(여러 국가에서 매년 측정된 자료)에서 boostrapping based algorithm을 활용한 다중 대치법 4. R에서 결측값 처리 관련 함수 함수 내용 complete.cases() 데이터 내 레코드에 결측값 있으면 FALSE, 있으면 TRUE로 반환 is.na() 결측값을 NA로 인식하여 결측값 있으면 TRUE, 없으면 FALSE로 반환 DMwR 패키지의 centrallmputation() NA 값에 가운데 값(central value)으로 대치, 숫자는 중위수, 요인(factor)은 최빈값으로 대치 DMwR 패키지의 knnlmputation NA 값을 k 최근 이웃 분류 알고리즘을 사용하여 대치, k개 주변 이웃까지의 거리를 고려하여 가중 평균한 값 사용 Amelia 패키지의 amelia() time-series-cross-sectional data set에서 활용(랜덤포레스트 모델은 결측값 돈재할 경우 바로 에러 발생), ramdomForest 패키지의 rflmpute() 함수를 활용해 NA 결측값을 대치한 후 알고리즘에 적용 5. 이상값(Outlier) 인식과 처리 이상값이란? 의도하지 않게 잘못 입력한 경우 (Bad data) 의도하지 않게 입력되었으나 분석 목접에 부합되지 않아 제거해야 하는 경우 (Bad data) 의도하지 않은 현상이지만 분석에 포함해야 하는 경우 의도된 이상값(fraud, 불량)인 경우 이상값을 꼭 제거해야 하는 것은 아님 이상값 인식 방법 ESD(Extreme Studentized Deviation) 평균으로부터 3 표준편차 떨어진 값(각 0.15%) 기하평균 -2.5 x 표준편차 &lt; data &lt; 기하평균 +2.5 x 표준편차 사분위수 이용하여 제거하기(상자 그림의 outer fence 밖에 있는 값 제거) 이상값 정의: Q1 - 1.5(Q3 - Q1) &lt; data &lt; Q3 + 1.5(Q3 - Q1)을 벗어나는 데이터 극단값 절단(trimming) 방법 기하평균을 이용한 제거 geo_mean 하단, 상단 % 이용한 제거 10% 절단(상하위 5%에 해당되는 데이터 제거) 극단값 조정(winsorizing) 방법 상한값과 하한값을 벗어나는 값들을 상한, 하한값으로 바꾸어 활용","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ADsP","slug":"Study/ADsP","permalink":"https://ne-choi.github.io/categories/Study/ADsP/"}],"tags":[{"name":"ADsP","slug":"ADsP","permalink":"https://ne-choi.github.io/tags/ADsP/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"}],"author":"ne_choi"},{"title":"ADsP 자격증: Part02. 데이터 분석 기획","slug":"Study/ADsP/Part02_데이터_분석_기획","date":"2020-11-14T00:00:00.000Z","updated":"2021-01-20T03:39:04.024Z","comments":true,"path":"/2020/11/14/Study/ADsP/Part02_데이터_분석_기획/","link":"","permalink":"https://ne-choi.github.io/2020/11/14/Study/ADsP/Part02_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EB%B6%84%EC%84%9D_%EA%B8%B0%ED%9A%8D/","excerpt":"","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터 분석 기획의 이해1절. 분석기획 방향성 도출1. 분석기획의 특징 분석기획이란? 분석을 수행할 과제를 정의하고, 의도했던 결과를 도출하도록 관리할 방안을 사전에 계획하는 작업 데이터 사이언티스트의 역량 Math &amp; Statistics Information Technology Domain Knowledge 2. 분석 대상과 방법 분석 대상(What), 분석 방법(How) 따라 4가지로 나뉘어짐 Optimization(최적화): 분석 대상 &amp; 분석 방법 모두 앎 Solution(솔루션): 분석 대상 앎 &amp; 분석 방법 모름 Insight(통찰): 분석 대상 모름 &amp; 분석 방법 앎 Discovery(발견): 분석 대상 모름 &amp; 분석 방법 모름 3. 목표 시점별 분석 기획 방안 과제 중심적 접근 방식: 당면 과제를 빠르게 해결하는 방식 장기적인 마스터 플랜 방식: 지속적인 분석 내재화를 위한 방식 분석 기획에서는 문제해결을 위한 단기적 접근방식과 분석과제 정의를 위한 중장기적인 마스터 플랜 접근방식을 융합하는 것이 중요 목표 시점별 분석 기획 방안 당면한 분석 주제 해결(과제 단위) 지속적 분석 문화 내재화(마스터 플랜 단위) Speed &amp; Test 1차 목표 Accuracy &amp; Deploy Quick &amp; Win 과제 유형 Long Term View Problem Solving 접근 방식 Problem Definition 의미 있는 분석을 위해서는 분석 기술, IT 및 프로그래밍, 분석 주제에 관한 도메인 전문성, 의사 소통이 중요 분석대상 및 방식에 따른 다양한 분석 주제를 과제 단위 또는 마스터 플랜 단위로 도출할 수 있어야 함 4. 분석 기획 시 고려사항 가용 데이터(Available Data) 분석을 위한 데이터 확보가 우선적이며, 유형 분석이 선행되어야 함 데이터 유형에 따라 적용 가능한 솔루션 및 분석 방법이 다르기 때문 적절한 활용 방안과 유즈 케이스(Proper Business Use Case) 분석을 통해 가치가 창출될 수 있음 ‘바퀴를 재발명하지 마라’ 기존에 구현된 유사 분석 시나리오와 솔루션을 최대한 활용 장애요소에 대한 사전계획 수립(Low Barrier Of Execution 일회성 분석에 그치지 않고 조직의 역량으로 내재화 필요 충분하고 계속적인 교육 및 활용방안 등의 변화 관리가 고려되어야함 2절. 분석 방법론1. 분석 방법론 개요 데이터 분석이 기업 내 효과적으로 정착하기 위해서는 절차와 방법이 정리된 데이터 분석 방법론 수립 필요 프로젝트는 일정한 수준의 품질을 갖춘 산출물과 성공 가능성을 확보할 수 있어야 함 (개인의 역량이나 조직의 우연한 성공에 기인하면 안 됨) 상체한 절차(Procedures), 방법(Methods), 도구와 기법(Tools&amp;Techniques), 템플릿과 산출물(Templates&amp;Options)로 구성되어 어느 정도 지식으로 활용할 수 있어야 함 데이터 기반 의사결정의 필요성 경험과 감에 따른 의사결정 → 데이터 기반 의사결정 기업의 합리적 의사결정을 막는 장애요소 고정관념(Stereotype), 편향된 생각(Bias), 프레이밍 효과(Framing Effect, 문제 표현 방식에 따라 동일한 사건임에도 판단이나 선택이 달라지는 현상) 방법론의 생성 과정 암묵지 → (형식화) → 형식지 → (체계화) → 방법론 → (내재화) → 암묵지 방법론 적용 업무 특성에 따른 모델 폭포수 모델(Waterfall Model) 단계를 순차적으로 진행하는 방법, 이전 단계가 완료되어야 다음 단계 진행 가능 문제 발결 시 피드백 과정이 수행됨 기존 IT의 SW 개발 방식 프로토타임 모델(Prototype Model) 폭포수 모델 단점을 보환하기 위해 점진적으로 시스템을 개발해가는 접근 방식 고객 요구를 완전히 이해하지 못하거나 완벽한 요구 분석의 어려움을 해결하기 위한 방법 일부분을 우선 개발하여 사용자에게 제공 시험 사용 후, 사용자 요구를 분석하여 요구 정당성을 점검하고 개선 작업을 진행 나선형 모델(Spiral Model) 반복을 통해 점증적으로 개발하는 방법 처음 시도하는 프로젝트에는 용이하나, 관리 체계를 갖추지 못하면 복잡도 상승 방법론의 구성 단계 최상위 계층으로서 프로세스 그룹을 통해 완성된 단계별 산출물 생성, 각 단계는 기준선으로 설정되어 관리되어야 함, 버전관리 등으로 통제 → 단계별 완료 보고서 태스크 단계를 구성하는 단위 활동으로써 물리적 또는 논리적 단위로 품질검토의 항목이 됨 → 보고서 스탭 WBS(Work Breakdown Structure)의 워크 패키지에 해당, 입력자료/처리 및 도구/출력자료로 구성된 단위 프로세스 → 보고서 구성요소 2. KDD 분석 방법론 KDD(Knowledge Discovery in Databases) 프로파일링 기술 기반으로 데이터에서 통계적 패턴이나 지식을 찾는 데 활용할 수 있도록 정리한 데이터 마이닝 프로세스 데이터 마이닝, 기계학습, 인공지능, 패턴인식, 데이터 시각화 등에서 응용될 수 있는 구조를 가짐 KDD 분석 절차 [Data] → 1. Selection → [Target Data] → 2. Preprocessing → [Preprocessed Data] → 3. Transformation → [Transformed Data] → 4.Data Mining → [Patterns] → 5. Interpretation / Eveluation → [Knowledge] 데이터셋 선택(Selection) 데이터셋 선택에 앞서 분석 대상 비즈니스 도메인에 대한 이해와 프로젝트목표 설정이 필수 - 데이터베이스 또는 원시 데이터에서 분석에 필요한데이터를 선택하는 단계 - 데이터마이닝에 필요한 목표데이터(target data)를구성하여 분석에 활용 데이터 전처리(Preprocessing) 데이터셋을 정제하는 단계 잡음(Noise), 이상치(Outlier), 결측치(Missing Value)를 식별하고 제거하거나 의미 있는 데이터로 재처리 전처리 단계에서 추가로 요구되는 데이터셋이 필요한 경우, 데이터 선택 프로세스를 재실행 데이터 변환(Transformation) 데이터 전처리 과정을 통해 정제된 데이터에 분석 목적에 맞는 변수 생성, 선택 데이터 차원을 축소하여 효율적으로 데이터 마이닝 하도록 변경하는 단계 학습용 데이터(training data)와 검증용 데이터(test data)로 데이터를 분리하는 단계 데이터 마이닝(Data Mining) 학습용 데이터를 이용하여 분석 목적에 맞는 데이터 마이닝 기법을 선택하고 적절한 알고리즘으로 데이터 마이닝 작업을 실행 필요에 따라 데이터 전처리와 데이터 변환 프로세스를 추가로 실행 데이터 마이닝 결과 평가(Interpretation / Evaluation) 데이터 마이닝 결과에 대한 해석과 평가, 분석 목적과의 일치성 확인 데이터 마이닝을 통해 발견한 지식을 업무에 활용하기 위한 방안 마련 단계 필요에 따라 데이터 선택 프로세스에서 데이터 마이닝 프로세스를 반복 수행 3. CRISP-DM 분석 방법론 CRISP-DM(Cross Industy Standard Process for Data Mining) 5개 업체가 주도: Daimler-Chrysler, SPSS, NCR, Teradata, OHRA 계층적 프로세스 모델로써 4개 레벨로 구성됨 CRISP-DM의 4레벨 구조 Phases(단계) → Generic Tasks(일반화 태스크) → Specialized Task(세분화 태스크) → Process Instances(프로세스 실행) CRISP-DM의 6단계 프로세스 각 단계는 단방향으로 구성되지 않고, 단계 간 피드백을 통해 단계별 완성도를 높이게 되어 있음 단계 내용 수행업무 1. 업무 이해 프로젝트 목적과 요구사항을 이해, 도메인 지식을 데이터 분석을 위한 문제 정의로 변경하고 초기 프로젝트 계획을 수립 업무 목적 파악, 상황 파악, 데이터 마이닝 목표 설정, 프로젝트 계획 수립 2. 데이터 이해 데이터 수집하고 데이터 속성 이해, 데이터 품질 문제점 식별, 숨겨진 인사이트 발견 초기 데이터 수집, 데이터 기술 분석, 데이터 탐색, 데이터 품질 확인 3. 데이터 준비 분석을 위해 수진된 데이터에서 분석기법에 적합한 데이터 편성 초기 데이터 수집, 데이터 기술 분석, 데이터 탐색, 데이터 품질 확인 4. 모델링 다양한 모델링 기법과 알고리즘 선택, 파라미터 최적화, 데이터셋이 추가로 필요한 경우 준비 단계 반복 수행, 테스트용 데이터셋을 평가해 모델의 과적합 문제 확인 모델링 기법 선택, 모델 테스트 계획 설계, 모델 작성, 모델 평가 5. 평가 모델링 결과가 프로젝트 목적에 부합하는지 평가, 데이터 마이닝 결과를 최종적으로 수용할 것인지 판단 분석 결과 평가, 모델링 과정 평가, 모델 적용성 평가 6. 전개 모델을 실 업무에 적용하기 위한 계획 수립, 유지보수 계획 마련(생명주기 고려 필요) 전개 계획 수립, 모니터링/유지보수 계획 수립, 프로젝트 종료 보고서 작성, 프로젝트 리뷰 4. KDD vs CRISP-DM KDD CRISP-DM 분석대상 비즈니스 이해 업무 이해 데이터셋 선택 / 데이터 전처리 데이터의 이해 데이터 변환 데이터 준비 데이터 마이닝 모델링 데이터 마이닝 결과 평가 평가 데이터 마이닝 활용 전개 5. 빅데이터 분석 방법론 빅데이터 분석의 계층적 프로세스 단계(Phase): 프로세스 그룹을 통해 완성된 단계별 산출물 생성 태스크(Task): 각 단계는 여러 개의 태스크로 구성, 단계를 구성하는 단위 활동이며 물리/논리적 단위로 품질 검토 항목이 될 수 있음 스텝(Step): WBS의 워크 패키지에 해당되고 입력자료, 처리 및 도구, 출력자료로 구성된 단위 프로세스 빅데이터 분석 방법론 5단계 분석 기획(Planning): 비즈니스 도메인과 문제점 인식, 분석 계획 및 프로젝트 수행계획을 수립하는 단계 데이터 준비(Preparing): 비즈니스 요구사항과 데이터 분석데 필요한 원천 데이터를 정의하고 준비하는 단계 데이터 분석(Analyzing): 원천 데이터를 분석용 데이터셋으로 편성하고 분석 기법과 알고리즘으로 데이터를 분석하는 단계, 추가 데이터가 필요할 경우 준비 단계로 피드백하여 두 단계 반복 진행 시스템 구현(Developing): 분석 기획에 맞는 모델 도출, 운영 중인 가동 시스템에 적용하거나 시스템 개발을 위한 사전 검증 평가 및 전개(Lesson Learned): 프로젝트 성과를 평가하고 정리, 모델 발전 계획을 수립하여 차기 분석 기획으로 전달 3절. 분석 과제 발굴1. 분석 과제 발굴 방법론 개요 과제 정의서 형태로 도출 하향식 접근 방법과 상향식 접근 방법이 있음 최적의 의사 결정은 두 접근 방식이 상호 보완일 때 가능 디자인 사고: 상향식 접근의 발산 단계, 하향식 접근의 수렴 단계를 반복적으로 수행하여 분석 가치를 높임 하향식 접근법(Top Down Approach) 현황 분석을 통해 기회나 문제 탐색 → 문제 정의 → 해결방안 탐색 데이터 분석의 타당성 평가를 거쳐 분석 과제를 도출하는 과정으로 구성 1단계. 문제 탐색 문제를 해결함으로 나타나는 가치에 중점 비즈니스 모델 기반 문제 탐색: 비즈니스 모델 캔버스 업무(Operation), 제품(Product), 고객(Customer), 규제와 감사(Regulation &amp; Audit), 지원 인프라(IT &amp; Human Resource) 분석 기회 발굴 범위 확장 거시적 관점: 사회, 기술, 경제, 환경, 정치 경쟁자 확대: 대체제, 경쟁자, 신규 진입자 시장 니즈 탐색: 고객, 채널, 영향자들 역량의 재해석: 내부역량, 파트너 네트워크 외부 참조 모델 기반 문제탐색 Quick &amp; Easy 방식으로 빠르게 도출 데이터 분석을 통한 인사이트 도출 지속적 조사와 데이터 분석을 통한 가치 발굴 사례를 정리하여 풀(Pool)로 만들면 좋음 분석 유즈 케이스(Analytics Use Case) 빠짐 없이 도출한 분석 기회를 구체적인 과제로 만들기 전에 분석 유즈 케이스로 표기하는 것이 필요 2단계. 문제 정의(Problem Definition) 식별된 비즈니스 문제를 데이터 문제로 변환하여 정의하는 단계 (How ?) 데이터 분석 문제 정의 및 요구사항: 분석 수행 당사자뿐 아니라 최종 사용자(End User) 관점에서 이루어져야 함 데이터 정의 및 기법 발굴을 용이하게 하기 위해 정확히 분석의 관점에서 문제를 재정의할 필요가 있음 3단계. 해결 방안 탐색(Solution Search) 정의된 데이터 분석 문제 해결을 위해 다양한 방안 모색 기존 정보시스템의 단순한 보완으로 분석 가능한지 고려 엑셀 등 간단한 도구로 분석 가능한지 고려 하둡 등 분산병렬처리를 활용한 빅데이터 분석 도구로 보다 체계적이고 심도 있는 방안 고려 분석 역량이 없을 경우, 교육이나 전문인력 채용으로 역량을 확보하거나 전문 업체를 활용 4단계. 타당성 검토(Feasibility Study) 경제적 타당성: 비용 대비 편익 분석 관점으로 접근 데이터 및 기술적 타당성 상향식 접근법(Bottom up Approach) 다양한 원천 데이터를 대상으로 분석하여 가치 있는 모든 문제를 도출하는 과정 하향식 접근법의 한계를 극복하기 위한 분석 방법론 단계별 접근법은 문제 구조가 분명하고, 문제 해결이 데이터 분석가 및 의사결정자에게 주어져 있음을 가정 → 솔루션 도출에는 유리하나 새로운 문제 탐색에는 한계 디자인 사고 접근법을 통해 전통적인 분석적 사고 한계를 극복해야 함 Why가 아닌 사물을 그대로 인식하는 What 관점으로 보아야 함 데이터 그 자체를 관찰하고 행동하여 대상을 잘 이해하는 방식의 접근법 Empathize → Define → Ideate → Prototype → Test 비지도 학습과 지도 학습 비지도 학습(Unsupervised Learning) 일반적으로 상향식 접근방식의 데이터 분석은 비지도 학습 방법으로 수행 데이터 자체의 결합, 연관성, 유사성을 중심으로 데이터 상태를 표현하는 것 예) 장바구니 분석, 군집 분석, 기술 통계 및 프로파일링 지도 학습(Supervised Learning) 명확한 목적 하에 데이터 분석을 실시하는 것 분류, 추측, 예측, 최적화를 통해 사용자 주도 하 분석을 실시하고 지식을 도출하려는 목적 상관관계 분석, 연관 분석을 통해 다양한 문제를 해결 시행 착오를 통한 문제 해결 프로토타이핑 접근법 요구사항이나 데이터 규정이 어렵고, 데이터 소스를 명확히 파악하기 어려운 상황에서 일단 분석하고 결과를 보면서 반복적으로 개선해 나가는 방식 완벽하지는 못하지만, 신속하게 해결책이나 모형을 제시함으로써 이를 바탕으로 문제를 명확히 인식하고 필요한 데이터를 식별하여 구체화하게 하는 상향식 접근 방식 가설 생성 → 디자인 실험 → 실제 환경 테스트 → 인사이트 도출 및 가설 확인 빅데이터 분석 환경에서 프로토타이핑의 필요성 문제 인식 수준: 문제 정의가 불명확하거나 새로운 문제일 경우 문제 이해와 구체화에 도움 필요 데이터 존재 여부의 불확실성: 데이터를 어떻게 찾을 것인지 사용자와 분석가 간 반복적인 협의 과정 필요 데이터 사용 목적 가변성: 기존 데이터 정의를 재검토하여 데이터 사용 목적과 범위 확대 가능 분석과제 정의: 분석별 필요한 소스 데이터, 분석 방법, 데이터 입수, 분석 난이도, 분석 수행주기, 분석 결과 검증 오너십, 상세 분석 과정 정의 분석 데이터 소스: 내/외부 비구조적인 데이터와 소셜 미디어 및 오픈 데이터까지 범위 확장하여 고려하고 분석 방법 또한 상세하게 정의 4절. 분석 프로젝트 관리 방안1. 분석과제 관리를 위한 5가지 주요 영역 Data Size 분석하고자 하는 데이터 양 고려 Data Complexity 초기 데이터 확보와 통합뿐 아니라 해당 데이터에 적용될 수 있는 분석 모델 선정 등의 사전 고려 필요 Speed 시나리오 측면에서의 속도 고려 필요 프로젝트 수행 시 분석 모델의 성능 및 속도를 고려한 개발/테스트 Analytic Complexity 분선 모델의 정확도와 복잡도는 트레이드 오프 관계 분석 모델이 복잡할수록 정확도는 올라가지만 해석이 어려워짐 해석이 가능하면서도 정확도를 올릴 수 있는 최적모델을 찾아야 함 Accuracy &amp; Precision 정확도: 모델과 실제 값 사이 차이가 적음을 의미 일관성: 모델을 반복했을 때의 편차의 수준 활용 측면에서는 정확도가, 안정성 측면에서는 일관성이 중요 2장. 분석 마스터 플랜1절. 마스터 플랜 수립 프레임 워크1. 분석 마스터 플랜 수립 프레임 워크 마스터 플랜 수립 개요 우선 순위 고려 요소: 전략적 중요도, 비즈니스 성과/ROI, 실행 용이성 적용 범위/방식 고려 요소: 업무 내재화 적용 수준, 분석 데이터 적용 수준, 기술 적용 수준 수행 과제 도출 및 우선순위 평가 일반적인 IT 프로젝트의 우선순위 평가 예시 전략적 중요도: 전략적 필요성, 시급성 실행 용이성: 투자 용이성, 기술 용이성 ROI 관점의 빅데이터 핵심 특징 3V(난이도): 크기(Volume), 다양성(variety), 속도(Velocity) → 투자비용 요소 4V(시급성): 3V + 가치(Value) → 비즈니스 효과 2절. 분석 거버넌스 체계 수립1. 거버넌스 체계 구성 요소 분석 기획 및 관리를 수행하는 조직(Organization) 과제 기획 및 운영 프로세스(Process) 분석 관련 시스템(System) 데이터(Data) 분석 관련 교육 및 마인드 육성 체계(Human Resource) 2. 데이터 분석 수준 진단 분석 준비도(Readiness): 분석 업무, 분석 인력 및 조직, 분석 기법, 분석 데이터, 분석 문화, 분석 인프라 분석 성숙도(Maturity): 도입 &gt; 활용 &gt; 확산 &gt; 최적화(비즈니스, 조직 및 역량, IT) 분석 준비도 진단 과정 내용 분석업무 파악 발생 사실 분석, 예측 분석, 시뮬레이션 분석, 최적화 분석, 분석 업무 정기적 개선 인력 및 조직 분석 전문가 직무, 분석 전문가 교육 훈련, 관리자의 기본적 분석 능력, 전사 분석업무 총괄 조직, 경영진의 분석 업무 이해 분석 기법 업무별 적합한 분석 기법, 분석 업무 도입 방법론, 분석 기법 라이브러리, 분석 기법 효과성 평가, 분석 기법 정기적 개선 분석 데이터 데이터 충분성 / 신뢰성 / 적시성, 비구조적 데이터 관리, 외부 데이터 활용 체계, 기준 데이터 관리(MDM) IT 인프라 운영 시스템 데이터 통합, EAI/ETL 등 데이터 유통 체계, 분석 전용 서버 및 스토리지, 빅데이터 분석 환경, 통계 분석 환경, 비쥬얼 분석 환경 분석 성숙도 모델 조직의 성숙도 평가 도구: CMMI(Capability Maturity Model Integration) 성숙도 수준 분류: 도입 단계 → 활용 단계 → 확산 단계 → 최적화 단계 분석 성숙도 진단 분류: 비즈니스 부문, 조직/역량 부문, IT 부문 분석 관점에서의 사분면 분석 정착형(준비도 낮음, 성숙도 높음) 확산형(준비도 높음, 성숙도 높음) 준비형(준비도 낮음, 성숙도 낮음) 도입형(준비도 높음, 성숙도 낮음) 3. 분석 지원 인프라 방안 수립 분석 과제 단위별로 별도 분석 시스템을 구축하면, 관리 복잡 &amp; 비용 증대 문제가 발생 분석 마스터 플랜 기획 단계에서부터 확장성을 고려한 플랫폼 구조 도입이 필요 플랫폼 구조: 공동 기능, 중앙 집중적 데이터 관리, 시스템 간 인터페이스 최소화 플랫폼 단순한 분석 응용프로그램뿐 아니라, 분석 서비스를 위한 응용프로그램이 실행될 수 있는 기초를 이루는 컴퓨터 시스템 일반적으로 하드웨어에 탑재되어 데이터 분석에 필요한 프로그래밍 환경과 실행 및 서비스 환경을 제공 분석 플랫폼이 구성된 경우, 개별적인 분석 시스템 추가 대신 서비스를 추가하는 방식으로 확장성을 높일 수 있음 4. 데이터 거버넌스 체계 수립 데이터 거버넌스 전사 차원 데이터에 대해 관리 체계 수립, 프레임워크 및 저장소 구축 마스터 데이터(Master Data), 메타 데이터(Meta Data), 데이터 사전(Data Dictionary) 데이터 거버넌스 체계를 구축함으로써 데이터의 가용성, 유용성, 통합성, 보안성, 안정성 확보 가능 독자적 수행도 가능하나, 전사 차원의 IT 거버넌스나 EA(Enterprise Architecture)의 구성요소로 구축되는 경우도 있음 빅데이터의 효율적인 관리, 다양한 데이터 관리 체계, 데이터 최적화, 정보 보호, 데이터 생명주기 관리, 데이터 카테고리별 관리 책임자 지정 등 포함 데이터 거버넌스 구성 3요소 원칙(Principle) 데이터를 유지/관리하기 위한 지침과 가이드 보안, 품질 기준, 변경 관리 조직(Organization) 데이터 관리 조직의 역할과 책임 데이터 관리자, 데이터베이스 관리자, 데이터 아키텍트 프로세스(Process) 데이터 관리 위한 활동과 체계 작업 절차, 모니터링 활동, 측정 활동 데이터 거버넌스 체계 데이터 표준화 업무: 데이터 표준 용어 설정, 명명 규칙 수립, 메타 데이터 구축, 데이터 사전 구축 데이터 표준 용어는 표준 단어사전, 표준 도메인사전, 표준 코드 등으로 구성 (점검 프로세스 포함 필요) 명명 규칙은 필요 시 언어별로 작성되어 매핑 상태를 유지해야 함 데이터 관리 체계 표준 데이터를 포함한 메타 데이터와 데이터 사전 관리 원칙 수립 → 데이터 정합성 및 활용 효율성을 위해 수립된 원칙에 근거해 상세 프로세스를 만들고 담당자와 조직을 상세히 준비 데이터 생명주기 관리 방안(Data Life Cycle Management) 수립 필요 데이터 저장소 관리(Repository) 메타 데이터 및 표준 데이터 관리를 위한 전사 차원 저장소 워크플로우 및 관리용 응용 소프트웨어를 지원하고 관리 대상 시스템과의 인터페이스를 통제가 이뤄져야 함 데이터 구조 변경에 따른 사전 영향 평가 수행 필요 → 효율적 활용을 위해 표준화 활동 데이터 거버넌스 체계 구축 후, 표준 준수 여부를 주기적으로 점검하고 모니터링 실시 5. 데이터 조직 및 인력방안 수립 분석을 위한 3가지 조직 구조 집중 구조 전사 분석업무를 별도 분석 전담 조직에서 담당 전략적 중요도에 따라 분석조직이 우선순위 정하여 진행 현업 업무부서의 분석업무와 이중화/이원화 가능성이 높음 기능 구조 일반적인 분석 수행 구조 별도 분석조직이 없고 해당 업무 부서에서 분석 수행 전사적 핵심 분석이 어려우며, 과거 실적에 국한된 분석이 수행될 가능성이 높음 분산 구조 분석조직 인력을 현업부서로 직접 배치하여 분석 업무 수행 전사 차원 우선순위 수행 분석결과에 따른 신속한 Action 가능 베스트 프랙티스 공유 가능 부서 분석업무와 역할 분담을 명확히 해야함 → 업무 과다 이원화 가능성 분석 조직 인력 구성 분석 조직(DSCoE: Data Science Center of Excellence) 비즈니스 인력, IT 기술 인력, 분석 전문 인력, 변화 관리 인력, 교육 담당 인력 6. 분석 과제 관리 프로세스 수립 과제 관리 프로세스 과제 발굴: 분석 idea 발굴 → 분석 과제 후보 제안 → 분석 과제 확정 과제 수행: → 탐구성 → 분석 과제 실행 → 분석 과제 진행 관리 → 결과 공유 및 개선","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ADsP","slug":"Study/ADsP","permalink":"https://ne-choi.github.io/categories/Study/ADsP/"}],"tags":[{"name":"ADsP","slug":"ADsP","permalink":"https://ne-choi.github.io/tags/ADsP/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"}],"author":"ne-choi"},{"title":"빅데이터 분석과 R 프로그래밍 1: Ⅲ. R 데이터구조","slug":"Study/Postech/빅데이터분석R/Ⅲ_R_데이터구조","date":"2020-11-13T00:00:00.000Z","updated":"2021-02-01T00:41:50.772Z","comments":true,"path":"/2020/11/13/Study/Postech/빅데이터분석R/Ⅲ_R_데이터구조/","link":"","permalink":"https://ne-choi.github.io/2020/11/13/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A2_R_%EB%8D%B0%EC%9D%B4%ED%84%B0%EA%B5%AC%EC%A1%B0/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅰ 과정입니다. 1. R 데이터 생성파일 불러들이기 csv brain &lt;- read.csv(“week3_2/brain2210.csv”, header=T) xls *.xls 파일인 경우, 데이터를 csv(comma separated value)로 저장한 다음 read.csv 함수를 사용해 r로 불러들이는 게 편리 readxl로도 가능함 txt 파일을 부를 이름 &lt;- read.table(file = &quot;파일명.txt, na = &quot; &quot;, header = TRUE) 데이터 저장 폴더 데이터와 프로그램 저장 폴더 지정 폴더 이름은 영문으로 생성할 것 r 프로그램의 기본 경로 지정하는 명령어 # change working directory #set working directory setwd(&quot;경로&quot;) # check the current working directory getwd() 데이터와 변수 이름attach를 사용하면 정의된 파일 이름 입력 없이 바로 항목 조회가 가능 # attach 사용 attach(brain) # 원래대로라면 table(brain$sex) ## ## f m ## 77 108 # attach를 쓰고 나면 table(sex) ## sex ## f m ## 77 108 2. R 데이터 활용 Ⅰ데이터 추출_subset subset(데이터 이름, 조건) 예제 1. brain 데이터에서 female만 있는 subset 데이터 생성 brainf &lt;- subset(brain, sex = &quot;f&quot;) mean(brainf$wt) ## [1] 1206.822 예제 2. brain 데이터에서 wt &lt; 1300 이하인 데이터 생성 # subset with wt &lt; 1300 brain1300 &lt;- subset(brain, brain$wt &lt; 1300) # same subset of brain1300 # brain1300 &lt;- subset(brain, !brain@wt =&gt; 1300) summary(brain1300) ## wt sex ## Min. : 915 Length:138 ## 1st Qu.:1074 Class :character ## Median :1155 Mode :character ## Mean :1145 ## 3rd Qu.:1230 ## Max. :1289 # subset with female # brainf &lt;- subset(brain, sex = &quot;f&quot;) brainf &lt;- subset(brain, sex == &quot;f&quot;) mean(brainf$wt) ## [1] 1117.169 sd(brainf$wt) ## [1] 98.97094 # subset with male brainm &lt;- subset(brain, sex == &quot;m&quot;) mean(brainm$wt) ## [1] 1270.741 sd(brainm$wt) ## [1] 129.22 요약통계치 (그룹별)_aggregate aggregate(변수~그룹, 데이터, 함수) # &#39;aggregate&#39; for statistics by group aggregate(wt~sex, data = brain, FUN = mean) ## sex wt ## 1 f 1117.169 ## 2 m 1270.741 aggregate(wt~sex, data = brain, FUN = sd) ## sex wt ## 1 f 98.97094 ## 2 m 129.21997 추출한 데이터의 활용 (그룹별 히스토그램) # histogram for female and male # 2*2 multiple plot par(mfrow=c(2,2)) brainf&lt;-subset(brain,brain$sex==&#39;f&#39;) hist(brainf$wt, breaks = 12,col = &quot;green&quot;,cex=0.7, main=&quot;Histogram (Female)&quot; ,xlab=&quot;brain weight&quot;) # subset with male brainm&lt;-subset(brain,brain$sex==&#39;m&#39;) hist(brainm$wt, breaks = 12,col = &quot;orange&quot;, main=&quot;Histogram with (Male)&quot; , xlab=&quot;brain weight&quot;) 추출한 데이터의 활용 # histogram with same scale hist(brainf$wt, breaks = 12,col = &quot;green&quot;,cex=0.7, main=&quot;Histogram with Normal Curve (Female)&quot; , xlim=c(900,1700),ylim=c(0,25), xlab=&quot;brain weight&quot;) hist(brainm$wt, breaks = 12,col = &quot;orange&quot;, main=&quot;Histogram with Normal Curve (Male)&quot; , xlim=c(900,1700), ylim=c(0,25),xlab=&quot;brain weight&quot;) csv로 내보내기 # plot margin # par(mar=c(2,2,2,2)) # export csv file - write out to csv file write.table(brainf,file=&quot;week3_2/brainf.csv&quot;, row.names = FALSE, sep=&quot;,&quot;, na=&quot; &quot;) write.csv(brainf,file=&quot;week3_2/brainf.csv&quot;, row.names = FALSE) # export txt file write.table(brainm, file=&quot;week3_2/brainm.txt&quot;, row.names = FALSE, na=&quot; &quot;) 퀴즈 brain1000 &lt;- subset(brain, brain$wt &lt; 1000) brain1000 table(brain1000) 3. R 데이터 활용 Ⅱdplyr 패키지library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union car &lt;- read.csv(&quot;week3_3/autompg.csv&quot;) head(car) ## mpg cyl disp hp wt accler year origin carname ## 1 18 8 307 17 3504 12.0 70 1 chevrolet chevelle malibu ## 2 15 8 350 35 3693 11.5 70 1 buick skylark 320 ## 3 18 8 318 29 3436 11.0 70 1 plymouth satellite ## 4 16 8 304 29 3433 12.0 70 1 amc rebel sst ## 5 17 8 302 24 3449 10.5 70 1 ford torino ## 6 15 8 429 42 4341 10.0 70 1 ford galaxie 500 데이터 구조 파악하기 # 데이터의 수와 변수는? dim(car) ## [1] 398 9 # 데이터 수: 398개, 변수: 9개 # 데이터 전체 주고 파악하기: str 함수 str(car) ## &#39;data.frame&#39;: 398 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : int 8 8 8 8 8 8 8 8 8 8 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 17 35 29 29 24 42 47 46 48 40 ... ## $ wt : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ accler : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ carname: chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... # num: 실수, int: 정수 # 데이터 요약하기: summary 함수 summary(car) ## mpg cyl disp hp wt ## Min. : 9.00 Min. :3.000 Min. : 68.0 Min. : 1.00 Min. :1613 ## 1st Qu.:17.50 1st Qu.:4.000 1st Qu.:104.2 1st Qu.:26.00 1st Qu.:2224 ## Median :23.00 Median :4.000 Median :148.5 Median :60.50 Median :2804 ## Mean :23.51 Mean :5.455 Mean :193.4 Mean :51.39 Mean :2970 ## 3rd Qu.:29.00 3rd Qu.:8.000 3rd Qu.:262.0 3rd Qu.:79.00 3rd Qu.:3608 ## Max. :46.60 Max. :8.000 Max. :455.0 Max. :94.00 Max. :5140 ## accler year origin carname ## Min. : 8.00 Min. :70.00 Min. :1.000 Length:398 ## 1st Qu.:13.82 1st Qu.:73.00 1st Qu.:1.000 Class :character ## Median :15.50 Median :76.00 Median :1.000 Mode :character ## Mean :15.57 Mean :76.01 Mean :1.573 ## 3rd Qu.:17.18 3rd Qu.:79.00 3rd Qu.:2.000 ## Max. :24.80 Max. :82.00 Max. :3.000 # 데이터 요약통계치(빈도 구하기): table 함수 attach(car) # attach를 쓰면 변수에 이름을 안 써도 됨 ## The following object is masked from brain: ## ## wt table(origin) ## origin ## 1 2 3 ## 249 70 79 table(year) ## year ## 70 71 72 73 74 75 76 77 78 79 80 81 82 ## 29 28 28 40 27 30 34 28 36 29 29 29 31 # 데이터 요약통계치 (평균, 표준편차): mean mean(mpg) ## [1] 23.51457 mean(hp) ## [1] 51.38945 mean(wt) ## [1] 2970.425 4. 데이터 핸들링dplyr 활용dplyr: 전처리 과정을 하기 위한 편리한 기능 library(dplyr) attach(car) ## The following objects are masked from car (pos = 3): ## ## accler, carname, cyl, disp, hp, mpg, origin, wt, year ## The following object is masked from brain: ## ## wt str(car) ## &#39;data.frame&#39;: 398 obs. of 9 variables: ## $ mpg : num 18 15 18 16 17 15 14 14 14 15 ... ## $ cyl : int 8 8 8 8 8 8 8 8 8 8 ... ## $ disp : num 307 350 318 304 302 429 454 440 455 390 ... ## $ hp : num 17 35 29 29 24 42 47 46 48 40 ... ## $ wt : int 3504 3693 3436 3433 3449 4341 4354 4312 4425 3850 ... ## $ accler : num 12 11.5 11 12 10.5 10 9 8.5 10 8.5 ... ## $ year : int 70 70 70 70 70 70 70 70 70 70 ... ## $ origin : int 1 1 1 1 1 1 1 1 1 1 ... ## $ carname: chr &quot;chevrolet chevelle malibu&quot; &quot;buick skylark 320&quot; &quot;plymouth satellite&quot; &quot;amc rebel sst&quot; ... 변수 추출: selectcar 데이터에서 mpg, hp 변수만 추출 # Data handling usin &quot;dplyr&quot; # 1 subset data: selecting a few variables set1 &lt;- select(car, mpg, hp) head(set1) ## mpg hp ## 1 18 17 ## 2 15 35 ## 3 18 29 ## 4 16 29 ## 5 17 24 ## 6 15 42 car 데이터에서 mpg로 시작하는 변수를 제외 # 2 subset data: drop variables with - set2 &lt;- select(car, -starts_with(&quot;mpg&quot;)) head(set2) ## cyl disp hp wt accler year origin carname ## 1 8 307 17 3504 12.0 70 1 chevrolet chevelle malibu ## 2 8 350 35 3693 11.5 70 1 buick skylark 320 ## 3 8 318 29 3436 11.0 70 1 plymouth satellite ## 4 8 304 29 3433 12.0 70 1 amc rebel sst ## 5 8 302 24 3449 10.5 70 1 ford torino ## 6 8 429 42 4341 10.0 70 1 ford galaxie 500 데이터 추출: filter 조건에 맞는 데이터 추출: filter(데이터, 변수 조건, …) car 데이터에서 mpg가 30보다 큰 행 추출 # 3 subset data: filter mpg &gt; 50 set3 &lt;- filter(car, mpg &gt; 30) head(set3) ## mpg cyl disp hp wt accler year origin carname ## 1 31 4 71 62 1773 19.0 71 3 toyota corolla 1200 ## 2 35 4 72 66 1613 18.0 71 3 datsun 1200 ## 3 31 4 79 64 1950 19.0 74 3 datsun b210 ## 4 32 4 71 62 1836 21.0 74 3 toyota corolla 1200 ## 5 31 4 76 53 1649 16.5 74 3 toyota corona ## 6 32 4 83 58 2003 19.0 74 3 datsun 710 변수 생성: mutate mutate(새로운 변수 이름 = 기존 변수 활용) %&gt;% 파이프 연산자: 연산자 사용하여 연결 # create a derived variable set4 &lt;- car %&gt;% filter(!is.na(mpg)) %&gt;% mutate(mpg_km = mpg*1.609) head(set4) ## mpg cyl disp hp wt accler year origin carname mpg_km ## 1 18 8 307 17 3504 12.0 70 1 chevrolet chevelle malibu 28.962 ## 2 15 8 350 35 3693 11.5 70 1 buick skylark 320 24.135 ## 3 18 8 318 29 3436 11.0 70 1 plymouth satellite 28.962 ## 4 16 8 304 29 3433 12.0 70 1 amc rebel sst 25.744 ## 5 17 8 302 24 3449 10.5 70 1 ford torino 27.353 ## 6 15 8 429 42 4341 10.0 70 1 ford galaxie 500 24.135 # filter: car 데이터 mpg열의 NA가 아닌 모든 데이터 추출 # mutate: 기존 mpg열을 사용해 새로운 mpg_km열 생성 데이터 요약 통계치(평균) summarize(mean(변수이름)) # mean and standard deviation car %&gt;% summarize(mean(mpg), mean(hp), mean(wt)) ## mean(mpg) mean(hp) mean(wt) ## 1 23.51457 51.38945 2970.425 # 몇 개 변수의 평균값 한 번에 구하기 select(car, 1:6) %&gt;% colMeans() # 데이터를 열로 재구성하여 평균값 구함 ## mpg cyl disp hp wt accler ## 23.514573 5.454774 193.425879 51.389447 2970.424623 15.568090 백터화 요약치: summarize_all(FUN) 열추출하여 기술통계치 구하고 요약치 보기 a1 &lt;- select(car, 1:6) %&gt;% summarize_all(mean) a2 &lt;- select(car, 1:6) %&gt;% summarize_all(sd) a3 &lt;- select(car, 1:6) %&gt;% summarize_all(min) a4 &lt;- select(car, 1:6) %&gt;% summarize_all(max) table1 &lt;- data.frame(rbind(a1,a2,a3,a4)) rownames(table1) &lt;- c(&quot;mean&quot;, &quot;sd&quot;, &quot;min&quot;, &quot;max&quot;) # data.frame을 tbl_df로 전환시켰으므로 data.frame으로 원상복귀하여 행 이름을 바꿈 table1 ## mpg cyl disp hp wt accler ## mean 23.514573 5.454774 193.4259 51.38945 2970.4246 15.568090 ## sd 7.815984 1.701004 104.2698 29.93236 846.8418 2.757689 ## min 9.000000 3.000000 68.0000 1.00000 1613.0000 8.000000 ## max 46.600000 8.000000 455.0000 94.00000 5140.0000 24.800000 그룹별 통계량: group_by group_by(변수), summarize(__=FUN()) 그룹별 요약통계량 구하기 # summary statistics by group variable car %&gt;% group_by(cyl) %&gt;% summarize(mean_mpg = mean(mpg), na.rm = TRUE) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 5 x 3 ## cyl mean_mpg na.rm ## &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 3 20.6 TRUE ## 2 4 29.3 TRUE ## 3 5 27.4 TRUE ## 4 6 20.0 TRUE ## 5 8 15.0 TRUE # group_by: car 데이터의 cyl열을 그룹으로 묶음 # summarize: cyl그룹의 mpg 평균을 구함 # na.rm = TURE: 통계 분석 시 결측값을 제외 퀴즈 height &lt;- c(165, 170, 155, 185) weight &lt;- c(55, 65, 50, 110) gender &lt;- c(&quot;Female&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;) df &lt;- data.frame(height, weight, gender) df ## height weight gender ## 1 165 55 Female ## 2 170 65 Male ## 3 155 50 Female ## 4 185 110 Male df %&gt;% group_by(height) %&gt;% summarize(result = mean(gender)) ## Warning in mean.default(gender): argument is not numeric or logical: returning ## NA ## Warning in mean.default(gender): argument is not numeric or logical: returning ## NA ## Warning in mean.default(gender): argument is not numeric or logical: returning ## NA ## Warning in mean.default(gender): argument is not numeric or logical: returning ## NA ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 4 x 2 ## height result ## &lt;dbl&gt; &lt;dbl&gt; ## 1 155 NA ## 2 165 NA ## 3 170 NA ## 4 185 NA summarize(group_by(df, gender), result=mean(height)) ## `summarise()` ungrouping output (override with `.groups` argument) ## # A tibble: 2 x 2 ## gender result ## &lt;chr&gt; &lt;dbl&gt; ## 1 Female 160 ## 2 Male 178. filter(df, gender = c(&quot;Male&quot;, &quot;Female&quot;)) %&gt;% summarize(result = mean()) ## Error: Problem with `filter()` input `..1`. ## x Input `..1` is named. ## i This usually means that you&#39;ve used `=` instead of `==`. ## i Did you mean `gender == c(&quot;Male&quot;, &quot;Female&quot;)`? df %&gt;% select(height) %&gt;% summarize(result = mean(gender)) ## Warning in mean.default(gender): argument is not numeric or logical: returning ## NA ## result ## 1 NA","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"dplyr","slug":"dplyr","permalink":"https://ne-choi.github.io/tags/dplyr/"},{"name":"read csv","slug":"read-csv","permalink":"https://ne-choi.github.io/tags/read-csv/"}],"author":"ne-choi"},{"title":"ADsP 자격증: Part01. 데이터 이해","slug":"Study/ADsP/Part01_데이터_이해","date":"2020-11-12T00:00:00.000Z","updated":"2021-01-20T03:39:16.974Z","comments":true,"path":"/2020/11/12/Study/ADsP/Part01_데이터_이해/","link":"","permalink":"https://ne-choi.github.io/2020/11/12/Study/ADsP/Part01_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%9D%B4%ED%95%B4/","excerpt":"","text":"해당 자료는 ADsP 데이터분석 준전문가 2020 완전 개정판 요약본으로 저작권은 DATA EDU에 있습니다. 1장. 데이터의 이해1절. 데이터와 정보1. 데이터의 정의와 특성 데이터의 정의 데이터는 1646년 영군 문헌에 처음 등장, 라틴어인 dare(주다)의 과거분사형으로 주어진 것이라는 의미 데이터는 추론과 추정의 근거를 이루는 사실 단순한 객체로서의 가치뿐 아니라 다른 객체와의 상호관계 속에서 가치를 가짐 데이터의 특성 구분 특성 존재적 특성 객관적 사실(fact, raw material) 당위적 특성 추론·예측·전망·추정을 위한 근거(basis) 2. 데이터의 유형 구분 형태 예 특징 정성적 데이터(qualitative data) 언어, 문자 등 회사 매출 증가 저장·검색·분석에 많은 비용 소모 정량적 데이터(quantitative data) 수치, 도형, 기호 등 나이, 몸무게, 주가 등 정형화된 데이터로 비용 소모 적음 3. 지식경영의 핵심 이슈 데이터는 지식경영의 핵심 이슈인 암묵지(tacit knowledge)와 형식지(explicit knowledge)의 상호 작용에 있어 중요한 역할을 함 구분 의미 예 특징 상호작용 암묵지 학습/경험으로 개인에 체화되어 있지만 드러나지 않는 지식 김장, 자전거 타기 사회적으로 중요하지만 공유 어려움 공통화, 내면화 형식지 문서/매뉴얼처럼 형상화된 지식 교과서, 비디오, DB 전달과 공유 용이 표출화, 연결화 암묵지: 개인에게 축적된 내면화(internalization)된 지식 → 조직의 지식으로 공통화(socialization) 형식지: 언어, 기호, 숫자로 표출화(externalization)된 지식 → 개인의 지식으로 연결화(combination) 4. 데이터와 정보의 관계 DIKW의 정의 구분 내용 데이터(data) 개별 데이터 자체로는 의미가 중요하지 않은 객관적 사실 정보(information) 데이터 가공, 처리와 데이터 간 연관관계 속에서 의미가 도출된 것 지식(knowledge) 도출된 정보를 구조화해 유의미한 정보 분류, 개인 경험과 결합시켜 고유의 지식으로 내재화된 것 지혜(wisdon) 지식의 축적과 아이디어가 결합된 창의적 산물 DIKW 피라미드 지혜: 근본 원리에 대한 깊은 이해를 바탕으로 도출되는 창의적 아이디어 i.e. A마트의 다른 상품도 B마트보다 저렴할 것이다. 지식: 상호 연결된 정보 패턴을 이해하여 이를 토대로 예측한 결과물 i.e. 상대적으로 저렴한 A마트에서 연필을 사야겠다. 정보: 데이터 가공 및 상관관계를 이해하여 패턴을 인식하고 의미를 부여한 데이터 i.e. A마트에서 파는 연필이 더 저렴하다. 데이터: 존재형식을 불문하고, 타 데이터와 상관관계 있는 가공 전의 순수한 수치나 기호 : 연필은 A마트에서는 100원에, B마트에서는 200원에 판매된다. 2절. 데이터베이스 정의와 특징1. 용어의 연혁 1950년대: 미국, 군비상황 집중 관리를 위해 컴퓨터 도서관 설립 → 데이터의 기지(base)라는 뜻의 데이터베이스 탄생 1975년: 미국의 CAC가 KORSTIC을 통해 서비스되며 우리나라에서 데이터베이스 이용 1980년대 중반: 국내 데이터베이스 관련 기술의 연구 및 개발 2. 데이터베이스 정의 1차 개념 확대: 정형데이터 관리 2차 개념 확대: 빅데이터의 출현으로 비정형데이터 포함 3. 데이터베이스의 특징 데이터베이스의 일반적 특징 데이터베이스 특징 설명 통합된(integrated) 데이터 동일한 내용의 데이터가 중복되지 않음, 데이터 중복은 관리상 부작용 초래 저장된(stored) 데이터 컴퓨터가 접근할 수 있는 저장 매체에 저장, 데이터베이스는 컴퓨터 기술 바탕 공용(shared) 데이터 여러 사용자가 서로 다른 목적으로 데이터 이용, 대용량화 &amp; 복잡한 구조 변화되는(changable) 데이터 데이터베이스에 저장된 내용은 데이터베이스의 현 시점의 상태를 나타냄, 항상 변화하면서도 현재의 정확한 데이터 유지 필요 데이터베이스의 다양한 측면에서의 특징 정보 축적 및 전달 기계가독성: 일정 형식에 따라 컴퓨터 등 정보처리기기가 읽고 쓸 수 있음 검색가독성: 다양한 방법으로 필요한 정보 검색 원격조작성: 정보통신망을 통해 원거리에서도 즉시 온라인 이용 정보 이용 이용자의 정보 요구에 맞게 다양한 정보를 신속하게 획득 원하는 정보를 정확하고 경제적으로 찾아낼 수 있음 정보 관리 정보를 일정한 질서와 구조에 따라 정리, 저장, 검색, 관리할 수 있도록 하여 방대한 양의 정보를 체계적으로 축적 새로운 내용의 추가 또는 갱신이 용이 정보 기술 발전 정보처리, 검색·관리 소프트웨어, 관련된 하드웨어, 네트워크기술 발전 견인 경제·산업 정보 인프라적 특성 3절. 데이터베이스의 활용 기업내부 데이터베이스 1980년대 기업내부 데이터베이스 OLTP(On-Line Transaction Processing) 호스트 컴퓨터와 온라인으로 접속된 여러 단말간 처리 형태의 하나 호스트 컴퓨터가 데이터베이스를 엑세스하고, 처리 결과를 바로 돌려보내는 형태 주문입력시스템 재고관리시스템 등 현업의 대부분 업무가 이 성격을 띔 OLAP(On-Line Analytical Processing) 다양한 비즈니스 관점에서 쉽고 빠르게 다차원적 데이터 접근하여 의사 결정에 활용할 수 있는 정보를 얻게 하는 기술 OLTP에서 처리된 트랜잭션 데이터로 다양한 분석 실행 OLTP가 데이터 갱신 위주라면, OLAP는 데이터 조회 위주 (비교 보기 p.69) 2000년대 기업내부 데이터베이스 CRM(Customer Relationship Management) SCM(Supply Chain Management) 분야별 내부 데이터베이스 제조: ERP(Enterprise Resource Planning), BI(Business Intelligence), CRM, RTE(Real-Time Enterprise) 금융: EAI(Enterprise Application Integration), EDW(Enterprise Data Warehouse) 유통: KMS(Knowledge Management System), RFID(RF, Radio Frequency) 사회기반구조로서의 데이터베이스 EDI(Electronic Data Interchange), VAN(Value Added Network), CALS(Commerce At Light Speed) 2장. 데이터의 가치와 미래1절. 빅데이터의 이해1. 빅데이터의 이해 관점에 따른 빅데이터의 정의 좁은 범위의 정의: 3V로 요약되는 데이터 자체의 특성 변화에 초점 중간 범위의 정의: 데이터 자체뿐 아니라 처리, 분석의 기술적 변화까지 포함 넓은 관점에서의 정의: 인재, 조직 변화까지 포함 가트너 그룹 더그 래니의 3V 양(Volume): 데이터 규모 측면 → 센싱 데이터, 비정형데이터 다양성(Variety): 데이터 유형과 소스 측면 → 정형, 비정형데이터(영상, 사진) 속도(Velocity): 대이터 수집과 처리 측면 → 원하는 데이터 추출 및 분석 속도 PLUS, 가치(Value), 시각화(Visualization), 정확성(Veracity) 빅데이터 정의의 범주 및 효과 데이터 변화: 규모, 형태, 속도 기술 변화: 데이터 처리, 저장, 분석기술 및 아키텍처, 클라우드 컴퓨팅 활용 인재, 조직 변화: Data Scientist 같은 새로운 인재 필요, 데이터 중심 조직 2. 출현 배경과 변화없던 것이 새로 등장한 것이 아니라, 기존의 것에서 변화된 것 - 3가지 출현 배경 출현 배경 내용 산업계 고객 데이터 축적 데이터에 숨은 가치를 발굴해 새로운 성장동력원 확보 학계 거대 데이터 활용, 과학 확산 거대 데이터를 다루는 학문이 많아지면서 필요한 기술 아키텍처 및 통계 도구 발전 기술 발전 관련 기술의 발달 디지털화, 저장 기술 발달, 인터넷 보급, 모바일 혁명, 클라우드 컴퓨팅 3. 빅데이터의 기능 비유 비유 대상 내용 산업혁명의 석탄, 철 제조업 ~ 서비스 분야 생산성을 끌어올려 사회 전반에 혁명적 변화를 가져올 것 21세기의 원유 경제 성장에 필요한 정보를 제공하여 산업 생산성을 한 단계 향상 시키고 기존에 없던 새 범주 산업을 만들어 낼 것 렌즈 현미경이 생물학 발전에 미친 영향만큼 데이터가 산업 발전에 영향을 미칠 것 플랫폼 공동 활용 목적으로 구축된 유무형의 구조물로써 다양한 서드파티 비즈니스에 활용되며 플랫폼 역할을 할 것 4. 빅데이터가 만드는 본질적 변화 과거에서 현재로 사전 처리 → 사후처리 표본조사 → 전수조사 질 → 양 인과관계 → 상관관계 2절. 빅데이터의 가치와 영향1. 빅데이터의 가치 빅데이터 가치 산정이 어려운 이유 데이터 활용방식: 재사용, 재조합, 다목적용 데이터 개발 새로운 가치 창출: 기존에 없던 가치를 창출하여 가치 측정이 어려움 분석 기술 발전: 현재는 가치 없는 데이터가 추후 분석 기법 등장으로 큰 가치를 지닐 수 있음 2. 빅데이터의 영향 빅데이터가 미치는 영향 기업: 혁신, 경쟁력 제고, 생산성 향상 → 소비자 행동 분석, 시장 변동 예측 정부: 환경 탐색, 상황 분석, 미래 대응 → 기상, 인구 이동, 법제 데이터 등 수집 개인: 목적에 따른 활용 → 개인 인지도 향상에도 빅데이터가 활용 3절. 비즈니스 모델1. 빅데이터 활용 사례 기업 구글: 사용자 로그 데이터를 활용한 검색엔진 개발, 기존 페이지랭크 알고리즘 혁신으로 검색 서비스 개선 월마트: 고객 구매패턴을 분석해 상품 진열에 활용 정부 실시간 교통정보 수집, 기후 정보, 지질 활동, 소방 서비스 등 국가 안전 확보를 위해 실시간 모니터링 개인 정치인: 선거 승리를 위해 사회관계망 분석 → 유세 지역 선정, 해당 지역 유권자에게 영향을 줄 수 있는 내용을 선정해 효과적인 선거 활동 가수: 팬들의 음악 청취 기록 분석을 통해 실제 공연에서 부를 노래 순서 선정 2. 빅데이터 활용 기본 테크닉 테크닉 종류와 예시 연관규칙학습 변인들 간 주목할 상관관계 확인 커피를 구매하는 사람이 탄산음료를 더 많이 사는가? 유형분석 문서 분류 또는 조직과 팀을 특성에 따라 분류할 때 이 사용자는 어떤 특성을 가진 집단에 속하는가? 유전자 알고리즘 최적화 필요한 문제의 해결책을 선택, 돌연변이 같은 매커니즘으로 점진적으로 진화(evolve)시키는 법 최대의 시청률을 얻으려면 어떤 프로그램을 어떤 시간대에 방송해야 하는가? 기계학습 훈련 데이터로부터 학습한 알려진 특성을 활용해 예측하는 방법 기존 시청 기록을 바탕으로, 시청자가 현재 보유한 영화 중 어떤 것을 가장 보고 싶어 할까? 회귀분석 독립변수를 조작함에 따라, 종속변수가 어떻게 변하는지를 보며 두 변인 관계를 파악 구매자의 나이가 구매 차량의 타입에 어떤 영향을 미치는가? 감정분석 특정 주제에 관해 말하거나 글 쓴 사람의 감정 분석 새로운 환불 정책에 대한 고객 평가는? 소셜네트워크(사회관계망)분석 특정인과 다른 사람이 몇 촌 정도 관계인가를 파악, 영향력 있는 사람을 찾을 때 사용 고객들 간 관계망은 어떻게 구성되어 있나? 4절. 위기 요인과 통제 방안1. 빅데이터 시대의 위기 요인 사생활 침해 내용: 개인정보 포함된 데이터가 목적 외에 사용될 경우 사생활 침해 + 사회적 위협으로 변형될 수 있음 예시: 여행사실을 트위트한 사람 집을 강도가 노리는 사례 → 익명화의 기술 발전 필요 책임 원칙 훼손 내용: 분석대상이 되는 사람들이 예측 알고리즘의 희생양이 될 가능성 증가 예시: 범죄 예측 프로그램에 의해 범행을 저지르기 전 체포, 신용도와 무관하게 대출 거절 데이터 오용 내용: 빅데이터으로 한 예측은 항상 맞을 수 없음 예시: 적군 사망자 수를 전쟁 진척 상황 지표로 사용했으나, 적군 사망자 수가 과장되어 보고 2. 위기 요인에 따른 통제 방안 동의에서 책임으로 개인정보 제공자의 동의 → 개인정보 사용자의 책임 결과 기반 책임 원칙 고수 책임원칙 훼손 위기 요인에 대한 통제 방안 예측 자료에 의한 불이익을 당할 가능성을 최소화하는 장치 마련 필요 알고리즘 접근 허용 데이터 오용 위기요소에 대한 대응책 → 예측 알고리즘의 부당함을 반증할 수 있는 방법 5절. 미래의 빅데이터1. 빅데이터 활용 3요소 기본 3요소 데이터: 모든 것을 데이터화(Datafication)하는 추세로, 목적없이 축적된 데이터를 통한 창의적인 분석이 가능 기술: 대용량 데이터를 빠르게 처리하기 위한 알고리즘의 진화, 스스로 학습하고 데이터를 처리할 수 있는 인공지능 기술 출현 인력: 빅데이터 처리 위한 데이터 사이언티스트와 알고리즈미스트의 역할 → 빅데이터의 다각적 분석을 통한 인사이트 도출이 중요해짐 3장. 가치 창조를 위한 데이터 사이언스와 전략 인사이트1절. 빅데이터 분석과 전략 인사이트1. 빅데이터 열풍과 회의론빅데이터 회의론은 실제 빅데이터 분석에서 찾을 수 있는 가치를 발굴하기도 전에 사전에 활용 자체를 차단해 버릴 수 있음 2. 빅데이터 회의론의 원인 및 진단 투자효과를 못 거둔 부정적 학습효과 → 과거 CRM 빅데이터 성공사례 중, 기존 분석 프로젝트를 포함한 것이 많음 3. ’Big’이 핵심이 아님 빅데이터 분석 가치 크기 이슈가 아니라, 어떤 시각과 통찰을 얻을 수 있는지가 중요 4. 전략적 통찰이 없는 분석의 함정5. 일차원적 분석 vs 가치기반 분석 산업별 분석 애플리케이션 산업 일차원적 분석 애플리케이션 금융 서비스 신용점수 산정, 사기 탐지, 가격 책정, 프로그램트레이딩, 클레임분석, 고객수익성분석 소매업 판촉, 매대 관리, 수요 예측, 재고 보충, 가격 및 제조 최적화 제조업 공급사슬 최적화, 수요 예측, 재고 보충, 보증서 분석, 맞춤형 상품 개발, 신상품 개발 운송업 일정 관리, 노선 배정, 수익 관리 헬스케어 약품 거래, 예비 진단, 질병 관리 병원 가격 책정, 고객 로열티, 수익 관리 에너지 트레이딩, 공급/수요 예측 커뮤니케이션 가격 계획 최적화, 고객 보유, 수요 예측, 생산능력 계획, 네트워크 최적화, 고객 수익성 관리 서비스 콜센터 직원 관리, 서비스-수익 사슬 관리 정부 사기 탐지, 사례 관리, 범죄 방지, 수익 최적화 온라인 웹 매트릭스, 사이트 설계 고객 추천 모든사업 성과관리 일차원적 분석의 문제점은 환경변화와 같은 큰 변화에 대응하기 어렵고, 새로운 기회를 포착하기 어렵다는 것 전략도출 가치기반 분석 해당 사업에 중요한 기회 발굴, 주요 경영진의 지원 얻기 가능 분석의 활용 범위를 더 넓고 전략적으로 변화시키는 것 필요 차별화를 위한 전략적 인사이트를 주는 가치기반 분석단계로 나아가야 함 2절. 전략 인사이트 도출에 필요한 역량1. 데이터 사이언스 데이터사이언스는 데이터로부터 의미 있는 정보를 추출해내는 학문 비즈니스 성과를 좌우하는 핵심 이슈에 답하고, 사업의 성과를 견인할 수 있어야 함 2. 데이터 사이언스의 구성요소 데이터 사이언스의 영역 Analytics: 수학, 확률모델, 머신러닝, 분석학, 패턴 인식과 학습, 불확실성 모델링 IT: 시그널 프로세싱, 프로그래밍, 데이터 엔지니어링, 데이터 웨어하우스, 고성능 컴퓨팅 비즈니스 분석: 커뮤니케이션, 프레젠테이션, 스토리텔링, 시각화 3. 데이터 사이언티스트 요구 역량 Hard Skill 빅데이터에 관한 이론적 지식 분석 기술의 숙련 Soft skill 통찰력 있는 분석 설득력 있는 전달 다분야간 협력 4. 데이터 사이언스: 과학과 인문의 교차로스토리텔링, 커뮤니케이션, 창의력, 열정, 직관력, 비판적 시각, 대화능력 등의 인문학적 요소가 필요 5. 전략적 통찰력과 인문학의 외부 환경 측면에서 본 인문학 열풍 이유 외부환경의 변화 내용 예시 컨버전스 → 디버전스 단순세계화에서 복잡한 세계화로 변화 규모의 경제, 세계화, 표준화, 이성화 → 복잡한 세계, 다양성, 관계, 연결성, 창조성 생산 → 서비스 비즈니스 중심이 제품 생산에서 서비스로 이동 고장나지 않는 제품 → 뛰어난 서비스 생산 → 시장 창조 공급자 중심 기술 경쟁에서 무형자산 경쟁으로 변화 생산 기술 중심, 기술 중심 투자 → 패러다임에 근거한 시장 창조, 현지 사회와 문화에 관한 지식 3절. 빅데이터, 데이터 사이언스의 미래1. 가치 패러다임 변화 과거: Digitalization, 아날로그 세상을 디지털화하는지가 가치 창출 원천 현재: Connection, 디지털화된 정보가 연결되기 시작하면서 효과적인 연결을 찾는 것이 성공 요인 미래: Agency, 복잡한 연결을 효과적이고 믿을 수 있게 관리하는 것이 중요 2. 데이터 사이언스의 한계 한계 분석과정에서는 가정 등 인간의 해석이 개입되는 단계가 반드시 존재 분석결과를 해석하는 사람에 따라 다른 결과가 도출 정량적인 분석이라고 할지라도 결국 가정에 근거 추가. 최신 빅데이터 상식1. DMBS와 SQL DMBS(Data Base Management System) 데이터베이스를 관리하여 응용프로그램들이 데이터베이스를 공유하며 사용하는 환경을 제공하는 소프트웨어 데이터베이스를 구축하는 틀, 데이터 검색, 저장 기능 등 제공 대표 시스템: 오라클, 인포믹스, 액세스 데이터베이스 관리 시스템의 종류 관계형 DBMS 데이터를 column과 row를 이루는 하나 이상의 테이블/관계로 정리 Primary key가 각 row를 식별 row는 레코드나 튜플로 불림 일반적으로 각 테이블/관계는 하나의 엔티티 타입(고객이나 제품과 같은)을 대표 객체지향 DMBS 일반적으로 사용되는 테이블 기반의 관계형DB와 다르게 정보를 ‘객체’ 형태로 표현하는 데이터베이스 모델 네트워크 DMBS 레코드들이 노드로, 레코드들 사이 관계가 간선으로 표현되는 그래프를 기반으로 하는 데이터베이스 모델 계층형 DMBS 트리 구조를 기반으로 하는 계층 데이터베이스 모델 SQL(Structured Query Language) 데이터베이스에 접근할 수 있는 데이터베이스의 하부 언어 단순한 질의 기능뿐 아니라 데이터의 완전한 정의와 조작 기능을 갖춤 테이블 단위로 연산 수행, 영어 문장과 비슷한 구문으로 사용하기 쉬움 2. Data 관련 기술 개인정보 비식별 기술 데이터 셋에서 개인을 식별할 수 있는 요소를 전부/일부 삭제하거나 다른 값으로 대체하는 기술 비식별 기술 내용 예시 데이터 마스킹 데이터 길이, 유형, 형식 유지한채, 새로운 데이터를 익명으로 생상 홍길동, 35세, 서울 거주, 한국대 재학 → 홍–, 35세, 서울 거주, –대학 재학 가명 처리 개인정보 주체 이름을 변경하는 기술, 변경 규칙이 노출되지 않아야 함 홍길동, 35세, 서울 거주, 한국대 재학 → 임꺽정, 30대, 서울 거주, 국내대 재학 총계처리 데이터의 총합값을 보임 임꺽정 180cm, 홍길동 170cm → 물리학과 학생 키 합: 350cm, 평균키 175cm 데이터값 삭제 필요 없는 값 또는 개인식별에 중요한 값을 삭제, 날짜 정보는 연단위 처리 홍길동, 35세, 서울 거주, 한국대 졸업 → 35세, 서울 거주, 주민번호 901206-1234567 → 90년대 생, 남자 데이터 범주화 데이터 값을 범주 값으로 변환하여 값 숨김 홍길동, 35세 → 홍씨, 30~40세 무결성과 레이크 데이터 무결성(Data integrity) 데이터 변경/수정 시 제한을 두어 데이터의 정확성을 보증 유형: 개체 무결성(Entity integrity), 참조 무결성(Referential integrity), 범위 무결성(Domain integrity) 데이터 레이크(Data Lake) 수많은 정보 속에서 의미 있는 내용을 찾기 위해 방식 상관 없이 데이터를 저장하는 시스템 대용량의 정형 및 비정형 데이터 저장, 접근이 쉬운 대규모의 저장소 주요 플랫폼: Apache Hadoop, Teredata Integrated Big Data Platform 1700 3. 빅데이터 분석 기술 Hadoop 여러 컴퓨터를 하나인 것처럼 묶어 대용량 데이터를 처리하는 기술 분산파일 시스템(HDFS)을 통해 대용량 파일을 저장할 수 있는 기능 제공 하둡 에코시스템으로 하둡의 부족한 기능 보완 Apache Spark 실시간 분산형 컴퓨팅 플랫폼 스칼라로 작성되었으나, 스칼라, 자바, R, 파이썬, API 지원 In-Memory 방식으로 하둡에 비해 처리속도가 빠름 Smart Factory 공장 내 설비와 기계에 사물인터넷이 설치 → 공정 데이터가 실시간으로 수집, 데이터에 기반한 의사결정 Machine Learning &amp; Deep Learning 머신러닝: 인공지능 연구 분야 중 하나, 인간의 학습 능력과 같은 기능을 컴퓨터에서 실현 딥러닝: 컴퓨터가 데이터를 이용해 스스로 합습하도록 인공신경망(Artificial Neural Natwork) 등 기술로 구축한 기계 학습 기술 4. 기타 데이터의 유형 &lt;table&gt; &lt;colgroup&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;col style=&quot;width: 33%&quot; /&gt; &lt;/colgroup&gt; &lt;thead&gt; &lt;tr class=&quot;header&quot;&gt; &lt;th&gt;유형&lt;/th&gt; &lt;th&gt;내용&lt;/th&gt; &lt;th&gt;예시&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr class=&quot;odd&quot;&gt; &lt;td&gt;정형데이터&lt;/td&gt; &lt;td&gt;형태(고정된 필드) 존재, 연산 가능, 주로 관계형 데이터베이스에 저장, 데이터 수집 난이도 낮고 형식이 정해져 처리 쉬움&lt;/td&gt; &lt;td&gt;관계형 데이터베이스, 스프레드시트, CSV&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;even&quot;&gt; &lt;td&gt;반정형데이터&lt;/td&gt; &lt;td&gt;형태(스키마, 메타데이터) 존재, 연산 불가능, 주로 파일로 저장, 보통 API 형태로 제공되어 데이터 처리 기술(파싱) 필요&lt;/td&gt; &lt;td&gt;XML. HTML, JSON, 로그형태(웹로그, 센서데이터)&lt;/td&gt; &lt;/tr&gt; &lt;tr class=&quot;odd&quot;&gt; &lt;td&gt;비정형데이터&lt;/td&gt; &lt;td&gt;형태 없음, 연산 불가능, 주로 NoSQL에 저장, 데이터 수집 난이도 높음, 텍스트 마이닝 혹은 파일일 경우 데이터 형태로 파싱이 필요해 수집 데이터 처리가 어려움&lt;/td&gt; &lt;td&gt;소셜데이터(트위터, 페이스북), (영상, 이미지, 음성, 텍스트)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; 참고: XML은 Extensible Markup Language의 약자로 다목적 마크업 언어를 이용, 인터넷에 연결된 시스템끼리 데이터를 쉽게 주고 받을 수 있게 함(HTML 한계를 극복할 목적으로 만들어짐)","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"ADsP","slug":"Study/ADsP","permalink":"https://ne-choi.github.io/categories/Study/ADsP/"}],"tags":[{"name":"ADsP","slug":"ADsP","permalink":"https://ne-choi.github.io/tags/ADsP/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"}],"author":"ne-choi"},{"title":"빅데이터 분석과 R 프로그래밍 1: Ⅱ. 벡터, 행렬의 연산 및 함수","slug":"Study/Postech/빅데이터분석R/Ⅱ_벡터,행렬의_연산_및_함수","date":"2020-11-11T00:00:00.000Z","updated":"2021-02-01T00:41:22.108Z","comments":true,"path":"/2020/11/11/Study/Postech/빅데이터분석R/Ⅱ_벡터,행렬의_연산_및_함수/","link":"","permalink":"https://ne-choi.github.io/2020/11/11/Study/Postech/%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9DR/%E2%85%A1_%EB%B2%A1%ED%84%B0,%ED%96%89%EB%A0%AC%EC%9D%98_%EC%97%B0%EC%82%B0_%EB%B0%8F_%ED%95%A8%EC%88%98/","excerpt":"","text":"POSTECH에서 제공하는 MOOC 중, 빅데이터분석과 R프로그래밍 Ⅰ 과정입니다. 1. 벡터 및 행렬 생성벡터 생성 벡터 seq 함수 사용 (sequence) 0부터 10까지, 20개의 값을 생성y1 &lt;- seq(0, 10, length=20) y1 [1] 0.0000000 0.5263158 1.0526316 1.5789474 2.1052632 2.6315789[7] 3.1578947 3.6842105 4.2105263 4.7368421 5.2631579 5.7894737[13] 6.3157895 6.8421053 7.3684211 7.8947368 8.4210526 8.9473684[19] 9.4736842 10.00000000부터 10까지, 0.5씩 간격을 두고 값을 생성y2 &lt;- seq(0, 10, by = 0.5) y2 [1] 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 6.5 7.0[16] 7.5 8.0 8.5 9.0 9.5 10.0 rep 함수 사용 (replication) 1부터 4까지 두 번을 반복하여 생성z1 &lt;- rep(1:4, 2) z1 [1] 1 2 3 4 1 2 3 41부터 5까지 다섯 번을 반복하여 생성z2 &lt;- rep(1:2, 5) z2 [1] 1 2 1 2 1 2 1 2 1 2 백터 결합 cbind: column bind (열 기준으로 결합) x &lt;- c(1, 3, 5, 7, 9) c1 &lt;- c(2, 4, 6, 8, 10) c2 &lt;- cbind(x, c1) c2 x c1[1,] 1 2[2,] 3 4[3,] 5 6[4,] 7 8[5,] 9 10 rbind: row bind (행으로 결합) c3 &lt;- rbind(x, c1) c3 [,1] [,2] [,3] [,4] [,5]x 1 3 5 7 9c1 2 4 6 8 10 행렬의 생성 matrix 함수 tow row matrix with 1 to 10m1 &lt;- matrix(1:10, nrow=2) #number of row m1 [,1] [,2] [,3] [,4] [,5][1,] 1 3 5 7 9[2,] 2 4 6 8 10three columns matrix with 1:6m2 &lt;- matrix(1:6, ncol=3) m2 [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6matrix filled by rows, defalut: filled by columnsm3 &lt;- matrix(1:6, nrow=2, byrow=T) m3 [,1] [,2] [,3][1,] 1 2 3[2,] 4 5 6 고차원 행렬_array 함수 higher order of arraya1 &lt;- array(c(1:18), dim=c(3,3,2)) # 3 by 3 행렬을 2개 만듦 a1 , , 1[,1] [,2] [,3][1,] 1 4 7[2,] 2 5 8[3,] 3 6 9, , 2[,1] [,2] [,3][1,] 10 13 16[2,] 11 14 17[3,] 12 15 182. 벡터 생성과 이름 주기=========================== 벡터 생성 및 벡터 이름 주기 (0,1) 값을 갖는 벡터 gender에 0=female, 1=male 값 부여 gender &lt;- c(0,1) names(gender) &lt;- c(&quot;female&quot;, &quot;male&quot;) gender female male0 1 factor 변수로 정의 gender 변수에 (0,1)입력된 경우 -&gt; gender를 factor 변수로 정의 필요 gender 변수는 factor 변수로 인식하지 못함: (0,1)로 입력되었으므로 define as a factor variableis.factor(gender) [1] FALSE as.factor(변수이름): 어떤 변수를 factor 변수로 정의할 때, gender 변수는 factor 변수로 정의 -&gt; is.factor(gender)로 확인하면 factor 변수로 정의된 것 확인 가능 gender &lt;- as.factor(gender) is.factor(gender) [1] TRUE 범주형 변수 생성_factor 사용 size라는 변수 생성: (S, M, L, XL) 값을 갖는 범주형 변수(factor) 생성 size &lt;- c(&quot;S&quot;, &quot;M&quot;, &quot;L&quot;, &quot;XL&quot;) define size as a factor (categorical variable)size_factor &lt;- factor(size) size_factor [1] S M L XLLevels: L M S XL 순서를 정의한 factor 생성 size_factor1 &lt;- factor(size, ordered = TRUE, levels = c(&quot;S&quot;, &quot;M&quot;, &quot;L&quot;, &quot;XL&quot;)) size_factor [1] S M L XLLevels: L M S XL 행렬 생성하고 차원 알아보기 x &lt;- matrix(rnorm(12), nrow=4) x [,1] [,2] [,3][1,] -0.003729970 1.4663220 0.04381181[2,] 1.147397993 1.2467755 -0.48528371[3,] -0.002165277 -0.4026005 -1.08181700[4,] -0.972551871 0.5998451 1.70662398차원 확인하기dim(x) [1] 4 3 행렬의 속성 행렬 x는 (4*3), x가 data frame은 아님 generate matrix form normalx &lt;- matrix(rnorm(12), nrow=4) x [,1] [,2] [,3][1,] -0.71917921 0.1306288 1.3220113[2,] 0.67408016 -0.3067689 -0.5009565[3,] 0.59011810 0.4158816 0.5064173[4,] 0.03118919 -0.7860212 -0.6759638check dimension of xdim(x) [1] 4 3data frameis.data.frame(x) [1] FALSEmatrix x is not a data frame as.data.frame(x)는 x를 데이터로 인식 defime x as a data framex &lt;- as.data.frame(x) then x is a data frameis.data.frame(x) [1] TRUE3. 백터와 행렬의 연산==================== 기본 연산 기호 2^3 [1] 84**3 [1] 647%%2 [1] 17%/%2 [1] 3 행렬의 연산 - 참고 전치행렬(transpose) 구하기 (t) 전치행렬은 행과 열을 바꾼 행렬 m2는 (2 * 3)행렬, tm2는 (3 * 2) 행렬 m2 &lt;- matrix(1:6, ncol=3) m2 [,1] [,2] [,3][1,] 1 3 5[2,] 2 4 6transpose of m2tm2 &lt;- t(m2) tm2 [,1] [,2][1,] 1 2[2,] 3 4[3,] 5 6 determinant(행렬식) 구하기 (det) determinant 식 |A| = $\\frac{|a b|}{[c d]}$ = ad = bc d1 &lt;- matrix(1:4, nrow=2, byrow=T) # row부터 채워라 d1 [,1] [,2][1,] 1 2[2,] 3 4det(d1) [1] -2 역행렬(inverse) 구하기 (solve) d1 = $\\frac{[1 2]}{3 4]}$, inverse(d1) = fra**c[ − 2.01.0][1.5 − 0.5] inverse of matrixd1_inv &lt;- solve(d1) d1_inv [,1] [,2][1,] -2.0 1.0[2,] 1.5 -0.5d1 * inv(dq) = identity matrixd1 %*% d1_inv [,1] [,2][1,] 1 1.110223e-16[2,] 0 1.000000e+00: d1*d1의 역행렬 = 단위행렬(대각행렬이 1인 행렬) 역행렬을 이용한 방정식 해 구하기 (solve) 3x + 2y = 8, x + y = 2 방정식의 해를 구하기 위해 a(행렬)과 b(벡터)를 생성 solve equation3x + 2y = 8, x + y =2matrix a, ba &lt;- matrix(c(3,1,2,1), nrow=2, ncol=2) b &lt;- matrix(c(8,2), nrow=2, ncol=1) a [,1] [,2][1,] 3 2[2,] 1 1b [,1][1,] 8[2,] 2: solve 함수를 이용해 x와 y의 해를 찾음 solve(a,b) [,1][1,] 4[2,] -2 고유치(eigenvalue)와 고유벡터(eigenvector) example for eigen value and eigen vectoralready centered matrixx &lt;- matrix(c(-3, -2, 0, 1, 2, 2, -3, -3, 0, 2, 2, 2, 5, 7, 4, 0, -5, -11), nrow=6, ncol=3) x [,1] [,2] [,3][1,] -3 -3 5[2,] -2 -3 7[3,] 0 0 4[4,] 1 2 0[5,] 2 2 -5[6,] 2 2 -11dim(x) [1] 6 3eigen value and eigen vectore1 &lt;- eigen(t(x)%*%x) e1 eigen() decomposition$values[1] 273.546962 13.845220 0.607818$vectors[,1] [,2] [,3][1,] -0.2525343 0.5487321 0.79694382[2,] -0.2841664 0.7452586 -0.60319073[3,] 0.9249194 0.3787911 0.032272114. 간단한 함수생성 및 루프(for, while)================ 함수 생성 squre 함수 생성 create a simple functionsqure functionsqure &lt;- function(x)&#123; return(x*x) # 제곱값 제공 &#125; squre(9) [1] 81squre(1:3) #1부터 3까지 각 squre 값 제공하라 [1] 1 4 9 dif 함수 생성 dif &lt;- function(x,y)&#123; return(x-y) &#125; dif(20,10) [1] 10 rootdif 함수 생성 rootdif &lt;- function(x,y)&#123; return(sqrt(x-y)) &#125; rootdif(20,10) [1] 3.162278 기존 함수의 코드 보기_round 함수 round 함수: 소수점 자리 조정하는 함수round(5.14846) [1] 5round(5.14846, 2) [1] 5.15to see the function “round”round function (x, digits = 0) .Primitive(“round”)plusround(rootdif(20,10)) [1] 3round(rootdif(20,10),2) [1] 3.16루프문 루프문 (for) for을 사용한 루프 1 for 1 to 10if remainder = 1 when deviding by 3then go to next number$$for(i in 1:10)&#123; if(i%%3 == 1)&#123; next() &#125; print(i) &#125; [1] 2[1] 3[1] 5[1] 6[1] 8[1] 91부터 10까지의 숫자 반복3으로 나누었을 때 내머지가 1인 경우 next(다음 숫자로 넘어감)넘어가지 않은 경우 print(i)를 통해 결과 반환 for을 사용한 루프 2 i = 1 ~ 10 1, 2, 3 … 더해가며 프린트하는데, i &gt; 5 보다 크면 수행(print(i))을 멈춤for loop example 2stop loop after i &gt; 5%%for (i in 1:10)&#123; i &lt;- i + 1 print(i) if (i &gt; 5)&#123; # stop loop after i &gt; 5 break &#125; &#125; [1] 2[1] 3[1] 4[1] 5[1] 6 while을 사용한 루프 y가 5보다 적을 때는 {expression} 부분 수행while loopwhile (condition) {expression}y = 0 while(y &lt; 5)&#123;print(y &lt;- y+1)&#125; [1] 1[1] 2[1] 3[1] 4[1] 5","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Vector","slug":"Vector","permalink":"https://ne-choi.github.io/tags/Vector/"},{"name":"Array","slug":"Array","permalink":"https://ne-choi.github.io/tags/Array/"}],"author":"ne-choi"},{"title":"Pandas 10분 완성(10 Minutes to Pandas)","slug":"Study/Python/Pandas_10분_완성","date":"2020-11-10T00:00:00.000Z","updated":"2021-01-20T04:00:59.832Z","comments":true,"path":"/2020/11/10/Study/Python/Pandas_10분_완성/","link":"","permalink":"https://ne-choi.github.io/2020/11/10/Study/Python/Pandas_10%EB%B6%84_%EC%99%84%EC%84%B1/","excerpt":"","text":"Pandas 10분 완성(10 Minutes to Pandas) 필사 본 자료의 저작권은 BSD-3-Clause이며, 데잇걸즈2가 번역한 Pandas 10분 완성을 필사한 자료임. 목차 Object Creation (객체 생성) Viewing Data (데이터 확인하기) Selection (선택) Missing Data (결측치) Operation (연산) Merge (병합) Grouping (그룹화) Reshaping (변형) Time Series (시계열) Categoricals (범주화) Plotting (그래프) Getting Data In / Out (데이터 입 / 출력) Gotchas (잡았다!) # 패키지 불러오기 import pandas as pd import numpy as np import matplotlib.pyplot as plt 1. Object Creation- 참고: 데이터 구조 소개 섹션 Pansdas는 값을 가지고 있는 리스트를 통해 Series를 만들고, 정수로 만들어진 인덱스를 기본값으로 불러온다. s = pd.Series([1, 3, 5, np.nan, 6, 8]) s 0 1.0 1 3.0 2 5.0 3 NaN 4 6.0 5 8.0 dtype: float64 datetime 인덱스와 레이블이 있는 열을 가진 numpy 배열을 전달하여 데이터프레임을 만든다. dates = pd.date_range('20130101', periods = 6) dates DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;, &#39;2013-01-05&#39;, &#39;2013-01-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df = pd.DataFrame(np.random.randn(6, 4), index = dates, columns = list('ABCD')) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-01 -1.612642 -0.202385 1.369361 0.354048 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 2013-01-04 -0.748149 0.417369 0.823899 1.043707 2013-01-05 -0.420492 -0.175005 -0.021870 -1.980740 2013-01-06 1.494445 -0.582027 1.053088 0.574172 Series와 같은 것으로 변환될 수 있는 객체들의 dict로 구성된 데이터프레임을 만든다. df2 = pd.DataFrame(&#123;'A': 1., 'B': pd.Timestamp('20130102'), 'C': pd.Series(1, index = list(range(4)), dtype = 'float32'), 'D': np.array([3] * 4, dtype = 'int32'), 'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]), 'F': 'foo'&#125;) df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D E F 0 1.0 2013-01-02 1.0 3 test foo 1 1.0 2013-01-02 1.0 3 train foo 2 1.0 2013-01-02 1.0 3 test foo 3 1.0 2013-01-02 1.0 3 train foo df2.dtypes A float64 B datetime64[ns] C float32 D int32 E category F object dtype: object 2. Viewing Data- 참고: Basic Section 데이터프레임의 가장 윗줄과 마지막 줄을 확인하고 싶을 때 사용하는 방법 알아보기. # 괄호() 안에 숫자를 넣으면 (숫자)줄을 불러오고 넣지 않으면 기본값인 5개를 불러옴 df.tail(3) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-04 -0.748149 0.417369 0.823899 1.043707 2013-01-05 -0.420492 -0.175005 -0.021870 -1.980740 2013-01-06 1.494445 -0.582027 1.053088 0.574172 df.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-01 -1.612642 -0.202385 1.369361 0.354048 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 2013-01-04 -0.748149 0.417369 0.823899 1.043707 2013-01-05 -0.420492 -0.175005 -0.021870 -1.980740 index, column, numpy 데이터 세부 정보를 알아보자. df.index DatetimeIndex([&#39;2013-01-01&#39;, &#39;2013-01-02&#39;, &#39;2013-01-03&#39;, &#39;2013-01-04&#39;, &#39;2013-01-05&#39;, &#39;2013-01-06&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;D&#39;) df.columns Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;], dtype=&#39;object&#39;) df.values array([[-1.61264215, -0.2023848 , 1.36936106, 0.35404822], [-0.41459705, 0.74983681, 0.09588737, -0.74053093], [ 0.60731344, 0.78256437, 0.14000027, 0.89485905], [-0.74814932, 0.41736876, 0.82389876, 1.04370685], [-0.42049172, -0.17500487, -0.02187012, -1.98074049], [ 1.49444451, -0.58202719, 1.0530883 , 0.57417186]]) df.describe() # 데이터의 대략적인 통계적 정보 요약을 보여줌 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D count 6.000000 6.000000 6.000000 6.000000 mean -0.182354 0.165059 0.576728 0.024252 std 1.087357 0.564931 0.582501 1.167331 min -1.612642 -0.582027 -0.021870 -1.980740 25% -0.666235 -0.195540 0.106916 -0.466886 50% -0.417544 0.121182 0.481950 0.464110 75% 0.351836 0.666720 0.995791 0.814687 max 1.494445 0.782564 1.369361 1.043707 df.T # 데이터 전치 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 2013-01-01 2013-01-02 2013-01-03 2013-01-04 2013-01-05 2013-01-06 A -1.612642 -0.414597 0.607313 -0.748149 -0.420492 1.494445 B -0.202385 0.749837 0.782564 0.417369 -0.175005 -0.582027 C 1.369361 0.095887 0.140000 0.823899 -0.021870 1.053088 D 0.354048 -0.740531 0.894859 1.043707 -1.980740 0.574172 df.sort_index(axis = 1, ascending = False) # 축별로 정리 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; D C B A 2013-01-01 0.354048 1.369361 -0.202385 -1.612642 2013-01-02 -0.740531 0.095887 0.749837 -0.414597 2013-01-03 0.894859 0.140000 0.782564 0.607313 2013-01-04 1.043707 0.823899 0.417369 -0.748149 2013-01-05 -1.980740 -0.021870 -0.175005 -0.420492 2013-01-06 0.574172 1.053088 -0.582027 1.494445 df.sort_values(by = 'B') # 값별로 정렬 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-06 1.494445 -0.582027 1.053088 0.574172 2013-01-01 -1.612642 -0.202385 1.369361 0.354048 2013-01-05 -0.420492 -0.175005 -0.021870 -1.980740 2013-01-04 -0.748149 0.417369 0.823899 1.043707 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 3. Selection 주석: Pandas에 최적화된 데이터 접근 방법인 .at, .iat, .loc, .iloc 사용 - 참고: 데이터 인덱싱 및 선택, 다중 인덱싱/심화 인덱싱 - Getting(데이터 얻기)df.A와 동일한 Series를 생성하는 단일 열을 선택한다. df['A'] 2013-01-01 -1.612642 2013-01-02 -0.414597 2013-01-03 0.607313 2013-01-04 -0.748149 2013-01-05 -0.420492 2013-01-06 1.494445 Freq: D, Name: A, dtype: float64 df[0:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-01 -1.612642 -0.202385 1.369361 0.354048 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 df['20130102':'20130104'] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 2013-01-04 -0.748149 0.417369 0.823899 1.043707 - Selection by Label- 참고: Label을 통한 선택 # 라벨을 사용하여 횡단면 얻기 df.loc[dates[0]] A -1.612642 B -0.202385 C 1.369361 D 0.354048 Name: 2013-01-01 00:00:00, dtype: float64 # 라벨을 사용하여 여러 축(의 데이터) 얻기 df.loc[:, ['A', 'B']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B 2013-01-01 -1.612642 -0.202385 2013-01-02 -0.414597 0.749837 2013-01-03 0.607313 0.782564 2013-01-04 -0.748149 0.417369 2013-01-05 -0.420492 -0.175005 2013-01-06 1.494445 -0.582027 # 양쪽 종단점을 포함한 라벨 슬라이싱 보기 df.loc['20130102':'20130104', ['A', 'B']] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B 2013-01-02 -0.414597 0.749837 2013-01-03 0.607313 0.782564 2013-01-04 -0.748149 0.417369 # 반환되는 객체의 차원 줄이기 df.loc['20130102', ['A', 'B']] A -0.414597 B 0.749837 Name: 2013-01-02 00:00:00, dtype: float64 # 스킬라 값 얻기 df.loc[dates[0], 'A'] -1.6126421545697252 # cf. 스킬라 값 더 빠르게 구하는 법 df.at[dates[0], 'A'] -1.6126421545697252 - Selection by Position- 참고: 위치로 선택하기 # 넘겨 받은 정수 위치를 기준으로 선택 df.iloc[3] A -0.748149 B 0.417369 C 0.823899 D 1.043707 Name: 2013-01-04 00:00:00, dtype: float64 # 정수로 표기된 슬라이스를 통해, numpy / python과 유사하게 작동 df.iloc[3:5, 0:2] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B 2013-01-04 -0.748149 0.417369 2013-01-05 -0.420492 -0.175005 # 정수로 표기된 위치값 리스트를 통해, numpy / python 스타일과 유사해짐 df.iloc[[1, 2, 4], [0, 2]] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A C 2013-01-02 -0.414597 0.095887 2013-01-03 0.607313 0.140000 2013-01-05 -0.420492 -0.021870 # 명시적으로 행을 나누고자 하는 경우 df.iloc[1:3, :] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 2013-01-03 0.607313 0.782564 0.140000 0.894859 # 명시적으로 열을 나누고자 하는 경우 df.iloc[:, 1:3] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; B C 2013-01-01 -0.202385 1.369361 2013-01-02 0.749837 0.095887 2013-01-03 0.782564 0.140000 2013-01-04 0.417369 0.823899 2013-01-05 -0.175005 -0.021870 2013-01-06 -0.582027 1.053088 # 명시적으로 (특정한) 값을 얻고자 하는 경우 df.iloc[1, 1] 0.7498368136559216 # 스칼라 값을 빠르게 얻는 방법 df.iat[1, 1] 0.7498368136559216 - Boolean Indexing데이터를 선택하기 위해 단일 열 값을 사용한다. df[df.A > 0] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-03 0.607313 0.782564 0.140000 0.894859 2013-01-06 1.494445 -0.582027 1.053088 0.574172 # Boolean 조건을 충족하는 데이터프레임에서 값 선택 df[df > 0] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 2013-01-01 NaN NaN 1.369361 0.354048 2013-01-02 NaN 0.749837 0.095887 NaN 2013-01-03 0.607313 0.782564 0.140000 0.894859 2013-01-04 NaN 0.417369 0.823899 1.043707 2013-01-05 NaN NaN NaN NaN 2013-01-06 1.494445 NaN 1.053088 0.574172 # 필터링을 위한 메소드 isin()을 사용 df2 = df.copy() df2['E'] = ['one', 'one', 'two', 'three', 'four', 'three'] df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D E 2013-01-01 -1.612642 -0.202385 1.369361 0.354048 one 2013-01-02 -0.414597 0.749837 0.095887 -0.740531 one 2013-01-03 0.607313 0.782564 0.140000 0.894859 two 2013-01-04 -0.748149 0.417369 0.823899 1.043707 three 2013-01-05 -0.420492 -0.175005 -0.021870 -1.980740 four 2013-01-06 1.494445 -0.582027 1.053088 0.574172 three df2[df2['E'].isin(['two', 'four'])] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D E 2013-01-03 0.607313 0.782564 0.14000 0.894859 two 2013-01-05 -0.420492 -0.175005 -0.02187 -1.980740 four - Setting새 열을 설정하면 데이터가 인덱스별로 자동정렬 된다. s1 = pd.Series([1, 2, 3, 4, 5, 6], index = pd.date_range('20130102', periods=6)) s1 2013-01-02 1 2013-01-03 2 2013-01-04 3 2013-01-05 4 2013-01-06 5 2013-01-07 6 Freq: D, dtype: int64 df['F'] = s1 # 라벨에 의해 값 설정 df.at[dates[0], 'A'] = 0 # 위치에 의해 값 설정 df.iat[0, 1] = 0 # Numpy 배열을 사용한 할당에 의해 값 설정 df.loc[:, 'D'] = np.array([5] * len(df)) # 위 설정대로 작동한 결과 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F 2013-01-01 0.000000 0.000000 1.369361 5 NaN 2013-01-02 -0.414597 0.749837 0.095887 5 1.0 2013-01-03 0.607313 0.782564 0.140000 5 2.0 2013-01-04 -0.748149 0.417369 0.823899 5 3.0 2013-01-05 -0.420492 -0.175005 -0.021870 5 4.0 2013-01-06 1.494445 -0.582027 1.053088 5 5.0 # where 연산 설정 df2 = df.copy() df2[df2 > 0] = -df2 df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F 2013-01-01 0.000000 0.000000 -1.369361 -5 NaN 2013-01-02 -0.414597 -0.749837 -0.095887 -5 -1.0 2013-01-03 -0.607313 -0.782564 -0.140000 -5 -2.0 2013-01-04 -0.748149 -0.417369 -0.823899 -5 -3.0 2013-01-05 -0.420492 -0.175005 -0.021870 -5 -4.0 2013-01-06 -1.494445 -0.582027 -1.053088 -5 -5.0 4. Missing DataPandas는 결측치를 표현하기 위해, 주로 np.nan 값을 사용한다. (기본 설정값이나 계산에는 포함되지 않음)- 참고: Missing data section Reindexing으로 지정된 축 상의 인덱스를 변경/추가/삭제할 수 있다. Reindexing은 데이터의 복사본을 반환한다. df1 = df.reindex(index = dates[0:4], columns = list(df.columns) + ['E']) df1.loc[dates[0]:dates[1], 'E'] = 1 df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F E 2013-01-01 0.000000 0.000000 1.369361 5 NaN 1.0 2013-01-02 -0.414597 0.749837 0.095887 5 1.0 1.0 2013-01-03 0.607313 0.782564 0.140000 5 2.0 NaN 2013-01-04 -0.748149 0.417369 0.823899 5 3.0 NaN # 결측치를 가지고 있는 행 지우기 df1.dropna(how = 'any') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F E 2013-01-02 -0.414597 0.749837 0.095887 5 1.0 1.0 # 결측치 채워 넣기 df1.fillna(value = 5) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F E 2013-01-01 0.000000 0.000000 1.369361 5 5.0 1.0 2013-01-02 -0.414597 0.749837 0.095887 5 1.0 1.0 2013-01-03 0.607313 0.782564 0.140000 5 2.0 5.0 2013-01-04 -0.748149 0.417369 0.823899 5 3.0 5.0 # NAN 값에 boolean을 통한 표식 pd.isna(df1) # 데이터프레임의 모든 값이 boolean 형태로 표시되게 하며, nan인 값만 True가 표시되게 하는 함수 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F E 2013-01-01 False False False False True False 2013-01-02 False False False False False False 2013-01-03 False False False False False True 2013-01-04 False False False False False True 5. Operation (연산)- 참고: 이진(Binary) 연산의 기본 섹션 - Stats통계: 일반적으로 결측치를 제외한 후 연산된다. # 기술통계 수행 df.mean() A 0.086420 B 0.198790 C 0.576728 D 5.000000 F 3.000000 dtype: float64 # 다른 축에서 동일한 연산 수행 df.mean(1) 2013-01-01 1.592340 2013-01-02 1.286225 2013-01-03 1.705976 2013-01-04 1.698624 2013-01-05 1.676527 2013-01-06 2.393101 Freq: D, dtype: float64 # 정렬이 필요하며, 차원이 다른 객체로 연산 (pandas는 지정된 차원을 따라 자동으로 브로드캐스팅됨) # broadcast: nympy에서 유래, n차원이나 스칼라 값으로 연산 수행 시 도출되는 결과의 규칙을 설명하는 것 s = pd.Series([1, 3, 5, np.nan, 6, 8], index = dates).shift(2) s 2013-01-01 NaN 2013-01-02 NaN 2013-01-03 1.0 2013-01-04 3.0 2013-01-05 5.0 2013-01-06 NaN Freq: D, dtype: float64 df.sub(s, axis = 'index') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F 2013-01-01 NaN NaN NaN NaN NaN 2013-01-02 NaN NaN NaN NaN NaN 2013-01-03 -0.392687 -0.217436 -0.860000 4.0 1.0 2013-01-04 -3.748149 -2.582631 -2.176101 2.0 0.0 2013-01-05 -5.420492 -5.175005 -5.021870 0.0 -1.0 2013-01-06 NaN NaN NaN NaN NaN - Apply# 데이터에 함수 적용 df.apply(np.cumsum) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D F 2013-01-01 0.000000 0.000000 1.369361 5 NaN 2013-01-02 -0.414597 0.749837 1.465248 10 1.0 2013-01-03 0.192716 1.532401 1.605249 15 3.0 2013-01-04 -0.555433 1.949770 2.429147 20 6.0 2013-01-05 -0.975925 1.774765 2.407277 25 10.0 2013-01-06 0.518520 1.192738 3.460366 30 15.0 df.apply(lambda x: x.max() - x.min()) A 2.242594 B 1.364592 C 1.391231 D 0.000000 F 4.000000 dtype: float64 - Histogramming- 참고: Histogramming and Discretization(히스토그래밍과 이산화) s = pd.Series(np.random.randint(0, 7, size = 10)) s 0 6 1 6 2 4 3 3 4 6 5 3 6 0 7 1 8 0 9 4 dtype: int64 s.value_counts &lt;bound method IndexOpsMixin.value_counts of 0 6 1 6 2 4 3 3 4 6 5 3 6 0 7 1 8 0 9 4 dtype: int64&gt; - String MethodsSeries는 문자열 처리 메소드 모음(set)을 가지고 있다.이 모음은 배열의 각 요소를 쉽게 조작하도록 만드는 문자열의 속성에 포함되어 있다.문자열의 패턴 일치 확인은 기본적으로 정규 표현식을 사용하며, 몇몇 경우에는 항상 정규 표현식을 사용한다. - 참고: 벡터화된 문자열 메소드 s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan, 'CABA', 'dog', 'cat']) s.str.lower() 0 a 1 b 2 c 3 aaba 4 baca 5 NaN 6 caba 7 dog 8 cat dtype: object 6. Merge- Concat (연결)결합(join)/병합(merge) 형태의 연산에 관한 인덱스, 관계 대수 기능을 위한 다양한 형태의 논리를 포함한 Series, 데이터프레임, Panel 객체를 손쉽게 결합하는 기능이 있다. - 참고: Merging # concat()으로 pandas 객체 연결 df = pd.DataFrame(np.random.randn(10, 4)) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 3 0 1.044225 0.155778 0.674068 -1.489455 1 0.504468 -2.412972 -0.541338 0.556083 2 0.849690 0.618393 -0.587040 0.065025 3 -0.112398 0.415087 -0.452262 1.626640 4 1.043760 -1.345565 -0.534134 -0.112001 5 1.280222 1.533708 0.054365 0.290299 6 0.476762 1.399581 0.342671 -0.624159 7 0.231877 0.835411 -0.527813 0.502120 8 0.268321 -0.991597 0.900198 2.113147 9 -0.403591 -0.531963 -1.762530 -2.067926 # break it into pieces pieces = [df[:3], df[3:7], df[7:]] pd.concat(pieces) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; 0 1 2 3 0 1.044225 0.155778 0.674068 -1.489455 1 0.504468 -2.412972 -0.541338 0.556083 2 0.849690 0.618393 -0.587040 0.065025 3 -0.112398 0.415087 -0.452262 1.626640 4 1.043760 -1.345565 -0.534134 -0.112001 5 1.280222 1.533708 0.054365 0.290299 6 0.476762 1.399581 0.342671 -0.624159 7 0.231877 0.835411 -0.527813 0.502120 8 0.268321 -0.991597 0.900198 2.113147 9 -0.403591 -0.531963 -1.762530 -2.067926 - JoinSQL 방식으로 병합한다.- 참고: 데이터베이스 스타일 결합 left = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'lval': [1, 2]&#125;) right = pd.DataFrame(&#123;'key': ['foo', 'foo'], 'rval': [4, 5]&#125;) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key lval 0 foo 1 1 foo 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key rval 0 foo 4 1 foo 5 pd.merge(left, right, on = 'key') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key lval rval 0 foo 1 4 1 foo 1 5 2 foo 2 4 3 foo 2 5 # 다른 예시 left = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'lval': [1, 2]&#125;) right = pd.DataFrame(&#123;'key': ['foo', 'bar'], 'rval': [4, 5]&#125;) left .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key lval 0 foo 1 1 bar 2 right .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key rval 0 foo 4 1 bar 5 pd.merge(left, right, on = 'key') .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; key lval rval 0 foo 1 4 1 bar 2 5 - Append (추가)데이터프레임에 행을 추가한다.- 참고: Appending df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D']) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 0 -1.293826 0.735671 1.108050 -0.175206 1 -1.949366 1.468593 1.386484 0.963284 2 -0.423707 0.157079 0.185440 0.435985 3 -0.478659 1.293856 -0.515603 0.200678 4 0.096275 1.907146 -0.771668 -0.622949 5 0.557493 0.096811 -0.901813 -0.458602 6 -0.072528 -2.372909 -1.428934 1.458320 7 -0.199425 -0.455105 0.771140 0.506667 s = df.iloc[3] df.append(s, ignore_index=True) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 0 -1.293826 0.735671 1.108050 -0.175206 1 -1.949366 1.468593 1.386484 0.963284 2 -0.423707 0.157079 0.185440 0.435985 3 -0.478659 1.293856 -0.515603 0.200678 4 0.096275 1.907146 -0.771668 -0.622949 5 0.557493 0.096811 -0.901813 -0.458602 6 -0.072528 -2.372909 -1.428934 1.458320 7 -0.199425 -0.455105 0.771140 0.506667 8 -0.478659 1.293856 -0.515603 0.200678 7. Grouping그룹화는 다음 단계 중 하나 이상을 포함하는 과정을 말한다. 몇몇 기준에 따라 여러 그룹으로 데이터를 분할(splitting) 각 그룹에 독립적으로 함수를 적용(applying) 결과물을 하나의 데이터 구조로 결합(combining) - 참고: 그룹화 df = pd.DataFrame( &#123; 'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'], 'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'], 'C': np.random.randn(8), 'D': np.random.randn(8) &#125;) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D 0 foo one 0.024720 -1.293299 1 bar one 1.510969 0.509977 2 foo two 0.342461 -0.811380 3 bar three 0.227221 0.717127 4 foo two 0.089665 0.040704 5 bar two -0.336139 1.132600 6 foo one 0.301063 -0.133439 7 foo three 0.167332 0.789836 # 생성된 데이터프레임을 그룹화한 후, 각 그룹에 sum() 함수 적용 df.groupby('A').sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; C D A bar 1.402051 2.359705 foo 0.925241 -1.407579 # 여러 열을 기준으로 그룹화하면 계층적 인덱스가 형성, 여기에도 sum 함수 적용 가능 df.groupby(['A', 'B']).sum() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; C D A B bar one 1.510969 0.509977 three 0.227221 0.717127 two -0.336139 1.132600 foo one 0.325783 -1.426738 three 0.167332 0.789836 two 0.432126 -0.770676 8. Reshaping- 참고: 계층적 인덱싱, 변형 - Stacktuples = list(zip(*[['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux'], ['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two']])) index = pd.MultiIndex.from_tuples(tuples, names = ['first', 'second']) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=['A', 'B']) df2 = df[:4] df2 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B first second bar one 0.164873 -1.599363 two 0.262695 0.130688 baz one 1.062799 1.995716 two -1.389387 -1.260636 # stack() 메소드는 데이터프레임 열들의 계층을 압축 stacked = df2.stack() stacked first second bar one A 0.164873 B -1.599363 two A 0.262695 B 0.130688 baz one A 1.062799 B 1.995716 two A -1.389387 B -1.260636 dtype: float64 # Stack된 데이터프레임 또는 MultiIndex를 인덱스로 사용하는 Series인 경우, # stack()의 역연산은 unstack()이며 기본적으로 마지막 계층을 unstack함 stacked.unstack() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B first second bar one 0.164873 -1.599363 two 0.262695 0.130688 baz one 1.062799 1.995716 two -1.389387 -1.260636 stacked.unstack(1) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; second one two first bar A 0.164873 0.262695 B -1.599363 0.130688 baz A 1.062799 -1.389387 B 1.995716 -1.260636 stacked.unstack(0) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; first bar baz second one A 0.164873 1.062799 B -1.599363 1.995716 two A 0.262695 -1.389387 B 0.130688 -1.260636 - Pivot Tables- 참고: 피봇 테이블 df = pd.DataFrame(&#123;'A': ['one', 'one', 'two', 'three'] * 3, 'B': ['A', 'B', 'C'] * 4, 'C': ['foo', 'foo', 'foo', 'bar', 'bar', 'bar'] * 2, 'D': np.random.randn(12), 'E': np.random.randn(12)&#125;) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; A B C D E 0 one A foo -0.335507 0.728239 1 one B foo -0.251893 1.290054 2 two C foo 0.590448 0.226098 3 three A bar 1.207361 0.915646 4 one B bar 0.296306 -1.981152 5 one C bar 0.166610 -0.312002 6 two A foo 1.505633 -0.791708 7 three B foo -1.978302 -1.671514 8 one C foo 1.263163 -2.340603 9 one A bar -0.028701 -1.374556 10 two B bar 0.657087 0.272042 11 three C bar 0.351417 1.455815 # 피봇 테이블 생성 pd.pivot_table(df, values='D', index=['A', 'B'], columns=['C']) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; C bar foo A B one A -0.028701 -0.335507 B 0.296306 -0.251893 C 0.166610 1.263163 three A 1.207361 NaN B NaN -1.978302 C 0.351417 NaN two A NaN 1.505633 B 0.657087 NaN C NaN 0.590448 9. Time SeriesPandas는 자주 일어나는 변환(ex. 5분마다 일어나는 데이터의 2차 데이터 변환) 사이에 수행하는 리샘플링 연산을 위해 간단하고 강력하고 효율적인 함수를 제공한다.재무(금융) 응용에서 매우 일반적이나 이에 국한되지는 않는다.- 참고: 시계열 rng = pd.date_range('1/1/2012', periods=100, freq='S') ts = pd.Series(np.random.randint(0, 500, len(rng)), index=rng) ts.resample('5Min').sum() 2012-01-01 25049 Freq: 5T, dtype: int64 # 시간대 표현 rng = pd.date_range('3/6/2012 00:00', periods=5, freq='D') ts = pd.Series(np.random.randn(len(rng)), rng) ts 2012-03-06 0.099699 2012-03-07 0.235671 2012-03-08 0.636022 2012-03-09 -1.993396 2012-03-10 -0.113581 Freq: D, dtype: float64 ts_utc = ts.tz_localize('UTC') ts_utc 2012-03-06 00:00:00+00:00 0.099699 2012-03-07 00:00:00+00:00 0.235671 2012-03-08 00:00:00+00:00 0.636022 2012-03-09 00:00:00+00:00 -1.993396 2012-03-10 00:00:00+00:00 -0.113581 Freq: D, dtype: float64 # 다른 시간대로 변환 ts_utc.tz_convert('US/Eastern') 2012-03-05 19:00:00-05:00 0.099699 2012-03-06 19:00:00-05:00 0.235671 2012-03-07 19:00:00-05:00 0.636022 2012-03-08 19:00:00-05:00 -1.993396 2012-03-09 19:00:00-05:00 -0.113581 Freq: D, dtype: float64 # 시간 표현 ↔ 기간 표현 변환 rng = pd.date_range('1/1/2012', periods=5, freq='M') ts = pd.Series(np.random.randn(len(rng)), index=rng) ts 2012-01-31 -1.161855 2012-02-29 0.262081 2012-03-31 0.238179 2012-04-30 -1.160233 2012-05-31 0.816160 Freq: M, dtype: float64 ps = ts.to_period() ps 2012-01 -1.161855 2012-02 0.262081 2012-03 0.238179 2012-04 -1.160233 2012-05 0.816160 Freq: M, dtype: float64 ps.to_timestamp() 2012-01-01 -1.161855 2012-02-01 0.262081 2012-03-01 0.238179 2012-04-01 -1.160233 2012-05-01 0.816160 Freq: MS, dtype: float64 기간 ↔ 시간 변환은 편리한 산술 기능을 사용할 수 있도록 만든다.11월에 끝나는 연말 결산의 분기별 빈도를, 분기말 익월의 월말일 오전 9시로 변환해보자. prng = pd.period_range('1990Q1', '2000Q4', freq='Q-NOV') ts = pd.Series(np.random.randn(len(prng)), prng) ts.index = (prng.asfreq('M', 'e') + 1).asfreq('H', 'S') ts.head() 1990-03-01 00:00 0.671614 1990-06-01 00:00 -0.141739 1990-09-01 00:00 0.070749 1990-12-01 00:00 -0.769261 1991-03-01 00:00 -0.432595 Freq: H, dtype: float64 10. CategoricalsPandas는 데이터프레임 내에 범주형 데이터를 포함할 수 있다.- 참고: 범주형 소개, API 문서 df = pd.DataFrame(&#123;\"id\":[1,2,3,4,5,6], \"raw_grade\":['a', 'b', 'b', 'a', 'a', 'e']&#125;) 가공하지 않은 성적을 범주형 데이터로 변환한다. df[\"grade\"] = df[\"raw_grade\"].astype(\"category\") df[\"grade\"] 0 a 1 b 2 b 3 a 4 a 5 e Name: grade, dtype: category Categories (3, object): [&#39;a&#39;, &#39;b&#39;, &#39;e&#39;] 범주에 더 의미있는 이름을 붙이자. (Series.cat.categories로 할당하는 것이 적합) df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"] 범주 순서를 바꾸고 동시에 누락된 범주를 추가한다. (Series.cat에 속하는 메소드는 기본적으로 새로운 Series를 반환) df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"]) df[\"grade\"] 0 very good 1 good 2 good 3 very good 4 very good 5 very bad Name: grade, dtype: category Categories (5, object): [&#39;very bad&#39;, &#39;bad&#39;, &#39;medium&#39;, &#39;good&#39;, &#39;very good&#39;] 정렬은 사전 순서가 아니라, 해당 범주에서 지정된 순서대로 배열한다. df.sort_values(by=\"grade\") .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; id raw_grade grade 5 6 e very bad 1 2 b good 2 3 b good 0 1 a very good 3 4 a very good 4 5 a very good 범주의 열을 기준으로 그룹화하면 빈 범주도 표시된다. df.groupby(\"grade\").size() grade very bad 1 bad 0 medium 0 good 2 very good 3 dtype: int64 11. Plotting- 참고: Plotting ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', periods=1000)) ts = ts.cumsum() ts.plot &lt;pandas.plotting._core.PlotAccessor object at 0x7f4670f2ae80&gt; 데이터프레임에서 plot() 메소드는 라벨이 존재하는 모든 열을 그릴 때 편리하다. df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, columns=['A', 'B', 'C', 'D']) df = df.cumsum() plt.figure(); df.plot(); plt.legend(loc='best') --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-97-bc219ebae1fe&gt; in &lt;module&gt;() ----&gt; 1 df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, 2 columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;]) 3 df = df.cumsum() 4 plt.figure(); df.plot(); plt.legend(loc=&#39;best&#39;) /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __getattr__(self, name) 5137 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5138 return self[name] -&gt; 5139 return object.__getattribute__(self, name) 5140 5141 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;DataFrame&#39; object has no attribute &#39;DataFrame&#39; 12. Getting Data In / OutCSVcsv 파일에 쓴다.df.to_csv(‘foo.csv’) csv 파일을 읽는다.pd.read_csv(‘foo.csv’) HDF5HDF5 Store에 쓴다.df.to_hdf(‘foo.h5’,’df’)HDF5 Store에서 읽는다.pd.read_hdf(‘foo.h5’,’df’) Excel엑셀 파일에 쓴다.df.to_excel(‘foo.xlsx’, sheet_name=’Sheet1’)엑셀 파일을 읽는다.pd.read_excel(‘foo.xlsx’, ‘Sheet1’, index_col=None, na_values=[‘NA’]) 13. Gotchas연산 수행 시, 다음과 같은 예외 상황이 나타날 수 있다. if pd.Series([False, True, False]): print(\"I was true\") --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-104-06fa23a4b3e2&gt; in &lt;module&gt;() ----&gt; 1 if pd.Series([False, True, False]): 2 print(&quot;I was true&quot;) /usr/local/lib/python3.6/dist-packages/pandas/core/generic.py in __getattr__(self, name) 5137 if self._info_axis._can_hold_identifiers_and_holds_name(name): 5138 return self[name] -&gt; 5139 return object.__getattribute__(self, name) 5140 5141 def __setattr__(self, name: str, value) -&gt; None: AttributeError: &#39;DataFrame&#39; object has no attribute &#39;Series&#39; 이러한 경우에는 any(), all(), empty 등을 사용해서 무엇을 원하는지를 선택해주어야 한다. if pd.Series([False, True, False]) is not None: print(\"I was not None\")","categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Python","slug":"Study/Python","permalink":"https://ne-choi.github.io/categories/Study/Python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://ne-choi.github.io/tags/python/"},{"name":"10 Minutes to Pandas","slug":"10-Minutes-to-Pandas","permalink":"https://ne-choi.github.io/tags/10-Minutes-to-Pandas/"},{"name":"pandas","slug":"pandas","permalink":"https://ne-choi.github.io/tags/pandas/"},{"name":"numpy","slug":"numpy","permalink":"https://ne-choi.github.io/tags/numpy/"},{"name":"matplotlib","slug":"matplotlib","permalink":"https://ne-choi.github.io/tags/matplotlib/"}],"author":"ne-choi"},{"title":"Python: Lists, Tuples, and Dictionary","slug":"ch3_1&2_lists_tuples_and_dictionary","date":"2020-11-09T00:00:00.000Z","updated":"2021-01-20T03:24:43.310Z","comments":true,"path":"/2020/11/09/ch3_1&2_lists_tuples_and_dictionary/","link":"","permalink":"https://ne-choi.github.io/2020/11/09/ch3_1&2_lists_tuples_and_dictionary/","excerpt":"","text":"Lists, Tuples, and Dictionary* 출처: dschloe.github.io Lists List는 [] 형태로 정의 List는 정렬(ordered)됨 List는 다양한 형태의 데이터 유형을 담을 수 있음 List 안에 있는 값에는 인덱스로 접근 List 안에 또 List를 담을 수 있음 List는 변하기 쉬움 List는 동적임 1) List는 정렬(ordered)됨# 파이썬의 기초 자료형 alphabet = [[\"a\", \"b\"], [\"c\", \"d\"]] print(alphabet) # List 자료형은 Matplolib 시각화에, Numpy 기초 자료(행렬, 딥러닝 연산에 자주 활용)로 활용됨 [[&#39;a&#39;, &#39;b&#39;], [&#39;c&#39;, &#39;d&#39;]] # a1 = a2? a1 = [1, 2, 3, 4] a2 = [2, 3, 4, 1] a1 == a2 # False가 나온 이유? 순서가 다르면 서로 다른 자료 False 2) 다양한 형태의 List 리스트 하나에 여러 형태의 자료를 담을 수 있다 multi_values = [11.1, 'foo', 3, 5, True] print(multi_values) [11.1, &#39;foo&#39;, 3, 5, True] float # 하나의 함수이자 클래스, List에 담을 수도 있음 float len # 값의 길이, 값이 몇 개인지 표시, 하나의 함수지만 List에 담을 수 있음 len(multi_values) 5 # 사용자 정의 함수, List에 담을 수 있음 def temp(): pass temp &lt;function __main__.temp()&gt; # 리스트에 다양한 형태 담아보기 fun_list = [float, temp, len] print(fun_list) [&lt;class &#39;float&#39;&gt;, &lt;function temp at 0x000001D5446B4700&gt;, &lt;built-in function len&gt;] 3) List 접근 방법_index 순서는 0부터 시작 역순으로 할 경우 -1부터 시작 alphabet_temp = [\"A\", \"B\", \"C\", \"D\", \"E\"] # python에서 순서는 '0'부터 시작 alphabet_temp[0] &#39;A&#39; # Negative List # python에서 거꾸로 지시하는 순서는 ''-1'부터 시작 alphabet_temp[-1] &#39;E&#39; *** 슬라이싱** ‘:’ 기호를 이용해 연속한 데이터를 슬라이싱해서 추출 temp[M:N]: M번째부터 N 미만까지 값을 가져옴 i.g temp[0:5]를 해야 알파벳 5개를 모두 가져올 수 있음 # alphabet 슬라이싱으로 추출해보기 alphabet_temp[2:4] [&#39;C&#39;, &#39;D&#39;] alphabet_temp[0:5] [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;] 4) List 안에 List 담기 List 안에 다양한 데이터를 넣을 수 있는데, 여기에는 List도 포함됨 x = ['a', ['bb', ['ccc', 'ddd'], 'ee', 'ff'], 'g', ['hh', 'ii'], 'j'] x [&#39;a&#39;, [&#39;bb&#39;, [&#39;ccc&#39;, &#39;ddd&#39;], &#39;ee&#39;, &#39;ff&#39;], &#39;g&#39;, [&#39;hh&#39;, &#39;ii&#39;], &#39;j&#39;] # 인덱스로 접근해보기 x[0] &#39;a&#39; x[1] [&#39;bb&#39;, [&#39;ccc&#39;, &#39;ddd&#39;], &#39;ee&#39;, &#39;ff&#39;] x[1][0] # x의 2번째 list 중 1번째 값 불러오기 &#39;bb&#39; x[1][1][0] # x의 2번째 list 중, 2번째 list의 1번째 값 불러오기 &#39;ccc&#39; x[3][0] # x의 5번째 list 중, 1번째 값 불러오기 &#39;hh&#39; 5) List는 변하기 쉬움 Mutable message = \"string immutable\" message[0] &#39;s&#39; # 문자열은 바로 치환되지 않아 에러가 나타남 = Immutable message[0] = 'p' print(message) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-45-e59a77b7a4e6&gt; in &lt;module&gt; 1 # 문자열은 바로 치환되지 않아 에러가 나타남 = Immutable ----&gt; 2 message[0] = &#39;p&#39; 3 print(message) TypeError: &#39;str&#39; object does not support item assignment # List의 경우 살펴보기 a = ['foo', 'bar', 'baz', 'qux', 'quux', 'corge'] a[1] &#39;bar&#39; a[1]=10 a [&#39;foo&#39;, 10, &#39;baz&#39;, &#39;qux&#39;, &#39;quux&#39;, &#39;corge&#39;] string에서 바뀌지 않던 것이 list에서는 쉽게 바뀜 쉽게 바뀌는 list의 성질을 Mutable이라고 표현(Tuple은 immutable) 6) List는 동적임 Mutable이라는 뜻 자체가 매우 동적임을 나타냄 다양한 형태로 작업이 가능함 temp = ['A', 'B', 'C', 'D', 'E', 'F'] temp[3:3] = [1, 2, 3] temp [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, 1, 2, 3, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;] # list에 값 추가하기 temp += [1000] temp [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, 1, 2, 3, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, 1000] # list에서 값 삭제하기 temp[3:6] = [] temp [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;, 1000] del temp[6] temp [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;] Tuples Lists와 비슷하지만 세부 특성이 다름 List는 []로 표현하나, Tuple은 ()라고 표현 Tuple은 immutable: 처음부터 바꾸는 것 외에는 방법이 없음 temp = ('A', 'B', 'C', 'D', 'E', 'F') temp (&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;) # immutable이기 때문에 바뀌지 않음 temp[2] = \"Cat\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-75-266ec228a7d8&gt; in &lt;module&gt; 1 # immutable이기 때문에 바뀌지 않음 ----&gt; 2 temp[2] = &quot;Cat&quot; TypeError: &#39;tuple&#39; object does not support item assignment 1) Tuples을 사용하는 이유? 데이터 분석에서 접근하면, 특정 데이터가 변하지 않도록 방지하는 기법 프로그래밍적으로 Tuple이 List보다 연산 속도가 빠름 Dictionary도 문자열과 같은 immutable type을 요구 Tuples은 Packing &amp; Unpacking 기법이 강력함(일종의 여행용 가방) t = (\"foo\", \"bar\", \"hey\", \"yeah\") t (&#39;foo&#39;, &#39;bar&#39;, &#39;hey&#39;, &#39;yeah&#39;) Unpacking: 각각의 element를 다른 이름으로 저장해보기 (s1, s2, s3, s4) = t s2 &#39;bar&#39; 이때, 반드시 좌우 값이 동일하게 작동해야 함 만약 좌우 값이 일치하지 않으면 에러가 발생 # 코드를 통해 확인 (a1, a2, a3) = t --------------------------------------------------------------------------- ValueError Traceback (most recent call last) &lt;ipython-input-89-76bce831b49d&gt; in &lt;module&gt; ----&gt; 1 (a1, a2, a3) = t ValueError: too many values to unpack (expected 3) 2) swap 스왑은 맞바꾸다의 뜻 Tuple에서는 매우 쉽게 swap이 가능 a1 = \"chloe\" v1 = \"loves\" a2 = \"evan\" temp = a1, v1, a2 # Tuple 형태로 표시됨 temp (&#39;chloe&#39;, &#39;loves&#39;, &#39;evan&#39;) a1, a2 = a2, a1 temp = a1, v1, a2 # Tuple 형태로 표시 temp (&#39;evan&#39;, &#39;loves&#39;, &#39;chloe&#39;) Dictionary 영어 원뜻 그대로 사전이라는 개념 Dictionary는 {}로 표현 List와 유사한 부분이 많음 Mutable이라는 뜻을 포함 다른 Dictionary, List 등을 포함할 수 있음 List와의 차이 List에서 각 원소에 접근하기 위해 index를 사용 Dictionary에서는 각 원소에 접근하기 위해 key를 사용 Rename할 때 자주 사용 1) Dictionary 정의 Dictionary의 기본적인 문법d = &#123; &lt;키>: &lt;값>, &lt;키>: &lt;값>, . . . &lt;키>: &lt;값>, &#125; # 국내 야구팀을 Dictionary 형태로 만들기 kbo = &#123; \"인천\" : \"SK\", \"수원\" : \"KT\", \"광주\" : \"기아\" &#125; print(kbo) &#123;&#39;인천&#39;: &#39;SK&#39;, &#39;수원&#39;: &#39;KT&#39;, &#39;광주&#39;: &#39;기아&#39;&#125; 2) Dictionary 접근법 indexing이 아닌 key 값으로 접근 가능 print(kbo[1]) # index로 추출할 수 없음 --------------------------------------------------------------------------- KeyError Traceback (most recent call last) &lt;ipython-input-81-8f1aa6e7486b&gt; in &lt;module&gt; ----&gt; 1 print(kbo[1]) # index로 추출할 수 없음 KeyError: 1 # Dictionary는 key 값을 넣어줘야 호출할 수 있음 print(kbo[\"수원\"]) KT # Dictionary에 값 추가하기 kbo[\"대구\"] = \"삼성\" print(kbo) &#123;&#39;인천&#39;: &#39;SK&#39;, &#39;수원&#39;: &#39;KT&#39;, &#39;광주&#39;: &#39;기아&#39;, &#39;대구&#39;: &#39;삼성&#39;&#125; # Dictionary에서 값 삭제하기 del kbo[\"대구\"] kbo &#123;&#39;인천&#39;: &#39;SK&#39;, &#39;수원&#39;: &#39;KT&#39;, &#39;광주&#39;: &#39;기아&#39;&#125; 3) Dictionary Using Integer 이전까지 접근한 것은 문자를 이요해 접근 Integer로 활용하는 방법은? dic = &#123;0: 'a', 1: 'b', 2: 'c', 3: 'd'&#125; dic &#123;0: &#39;a&#39;, 1: &#39;b&#39;, 2: &#39;c&#39;, 3: &#39;d&#39;&#125; print(dic[0]) print(dic[1]) a b *** 주의할 점** dic는 list가 아님 list에서 할 수 있던, sclicing이나 append를 사용할 수 없음 # 에러 1: dictionary에는 append라는 속성값이 없음 dic.append(\"e\") --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) &lt;ipython-input-101-ea0dea029ae5&gt; in &lt;module&gt; 1 # 에러 1: dictionary에는 append라는 속성값이 없음 ----&gt; 2 dic.append(&quot;e&quot;) AttributeError: &#39;dict&#39; object has no attribute &#39;append&#39; # 에러 2: slice 작동하지 않음 dic[0:2] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) &lt;ipython-input-103-4e3c8d53df26&gt; in &lt;module&gt; 1 # 에러 2: slice 작동하지 않음 ----&gt; 2 dic[0:2] TypeError: unhashable type: &#39;slice&#39; 4) Dictionary와 관련된 함수 소개 Dictionary와 함께 사용하면 좋은 함수들 d.clear()dic = &#123;'A': 1, 'B': 2, 'C': 3&#125; dic &#123;&#39;A&#39;: 1, &#39;B&#39;: 2, &#39;C&#39;: 3&#125; # clear() 함수 적용 뒤 결괏값 확인 dic.clear() dic &#123;&#125; d.get() key 값을 활용해서 value 값을 가져오는 함수 dic = &#123;'A': 1, 'B': 2, 'C': 3&#125; print(dic.get('B')) print(dic.get('Z')) 2 None .get(‘B’)에 해당하는 Dictionary값이 존재하기 때문에 2를 반환 .get(‘Z’)에 해당하는 Dictionary값이 존재하지 않기 때문에 None을 반환 # None 반환보다 특정 숫자나 문자로 출력하고 싶은 경우 print(dic.get('z', 0)) print(dic.get('z', \"없음\")) 0 없음 d.keys() Dictionary는 key와 value로 구성되어 있는데, keys()의 뜻은 현재 구성된 Dictionary에서 keys()를 dict_keys 형태로 반환 이때, list로 변환하려면 list()를 활용하면 됨 dic = &#123;'A': 1, 'B': 2, 'C': 3&#125; print(dic.keys()) print(list(dic.keys())) dict_keys([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;] d.values() keys()를 활용하여 key를 반환한 것처럼, values()를 활용하여 value 진행 가능 dic = &#123;'A': 1, 'B': 2, 'C': 3&#125; print(dic.values()) print(list(dic.values())) dict_values([1, 2, 3]) [1, 2, 3]","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://ne-choi.github.io/tags/python/"},{"name":"lists vs tuples","slug":"lists-vs-tuples","permalink":"https://ne-choi.github.io/tags/lists-vs-tuples/"},{"name":"dictionary","slug":"dictionary","permalink":"https://ne-choi.github.io/tags/dictionary/"},{"name":"list","slug":"list","permalink":"https://ne-choi.github.io/tags/list/"},{"name":"tuples","slug":"tuples","permalink":"https://ne-choi.github.io/tags/tuples/"}],"author":"ne-choi"},{"title":"Unzip multiples files in Python","slug":"Tips/How to unzip multiple zip files at once","date":"2020-11-05T00:00:00.000Z","updated":"2021-01-27T01:28:11.323Z","comments":true,"path":"/2020/11/05/Tips/How to unzip multiple zip files at once/","link":"","permalink":"https://ne-choi.github.io/2020/11/05/Tips/How%20to%20unzip%20multiple%20zip%20files%20at%20once/","excerpt":"","text":"How to unzip multiples zip files at once Using Linux command keyword: ubuntu open zip files When we download kaggle data directly, sometimes we look zip files. So I will unzip this files on Google Colab using a Linux command. First, I will show unzip each file. Then, I’ll do unzip all zip files in one directory at once. Unzip a zip file!unzip your_file_name.zip If you work on Google Colab, don’t forget ! you have to match path: using %cd Unzip all zip files at once!unzip *.zip` If you work on Google Colab, don’t forget ! you have to match path: using %cd","categories":[{"name":"Tips","slug":"Tips","permalink":"https://ne-choi.github.io/categories/Tips/"},{"name":"Programming","slug":"Tips/Programming","permalink":"https://ne-choi.github.io/categories/Tips/Programming/"}],"tags":[{"name":"kaggle","slug":"kaggle","permalink":"https://ne-choi.github.io/tags/kaggle/"},{"name":"unzip","slug":"unzip","permalink":"https://ne-choi.github.io/tags/unzip/"},{"name":"unzip multiple files","slug":"unzip-multiple-files","permalink":"https://ne-choi.github.io/tags/unzip-multiple-files/"}],"author":"ne-choi"},{"title":"Practice Kaggle Data_Titanic","slug":"titanic","date":"2020-11-04T00:00:00.000Z","updated":"2020-11-14T15:18:46.498Z","comments":true,"path":"/2020/11/04/titanic/","link":"","permalink":"https://ne-choi.github.io/2020/11/04/titanic/","excerpt":"","text":"I. Practice Kaggle Data 구글 드라이브 연동 Kaggle API 설치 Kaggle Token 다운로드 Titanic 데이터 불러오기 1. 구글 드라이브 연동Google Colab을 시작하면 항상 드라이브 연동을 해야 한다. from google.colab import drive # 패키지 불러오기 from os.path import join ROOT = \"/content/drive\" # 드라이브 기본 경로 print(ROOT) # print content of ROOT (Optional) drive.mount(ROOT) # 드라이브 기본 경로 Mount MY_GOOGLE_DRIVE_PATH = 'My Drive/Colab Notebooks/Python/python/practice' # 프로젝트 경로 PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH) # 프로젝트 경로 print(PROJECT_PATH) /content/drive Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True). /content/drive/My Drive/Colab Notebooks/Python/python/practice 아래 코드 실행 시, 에러가 없다면 데이터를 불러오면 된다. %cd \"&#123;PROJECT_PATH&#125;\" /content/drive/My Drive/Colab Notebooks/Python/python/practice 2. Kaggle API 설치Google Colab에서 Kaggle API를 불러오는 소스코드를 실행한다. !pip install kaggle # Google Colab에서 설치할 때는 ! 필요 Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.9) Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1) Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1) Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1) Requirement already satisfied: slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (0.0.1) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-&gt;kaggle) (2.10) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify-&gt;kaggle) (1.3) 3. Kaggle Token 다운로드Kaggle에서 API Token을 다운로드한다.[Kaggle] - [My Account] - [API] - [Create New API Token]을 누르면 kaggle.json 파일이 다운로드 된다.파일을 바탕화면에 옮긴 뒤, 아래 코드를 실행한다. from google.colab import files uploaded = files.upload() for fn in uploaded.keys(): print('uploaded file \"&#123;name&#125;\" with length &#123;length&#125; bytes'.format( name=fn, length=len(uploaded[fn]))) # kaggle.json을 아래 폴더로 옮긴 뒤, file을 사용할 수 있도록 권한을 부여한다. !mkdir -p ~/.kaggle/ &amp;&amp; mv kaggle.json ~/.kaggle/ &amp;&amp; chmod 600 ~/.kaggle/kaggle.json Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json uploaded file &quot;kaggle.json&quot; with length 62 bytes 아래 코드를 실행했을 때, 에러 메시지가 없으면 json 파일이 성공적으로 업로드 되었다는 뜻이다. ls -1ha ~/.kaggle/kaggle.json /root/.kaggle/kaggle.json 4. Kaggle 데이터 불러오기Kaggle competition list를 불러온다. !kaggle competitions list Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) ref deadline category reward teamCount userHasEntered --------------------------------------------- ------------------- --------------- --------- --------- -------------- contradictory-my-dear-watson 2030-07-01 23:59:00 Getting Started Prizes 134 False gan-getting-started 2030-07-01 23:59:00 Getting Started Prizes 185 False tpu-getting-started 2030-06-03 23:59:00 Getting Started Knowledge 315 False digit-recognizer 2030-01-01 00:00:00 Getting Started Knowledge 2356 False titanic 2030-01-01 00:00:00 Getting Started Knowledge 18058 True house-prices-advanced-regression-techniques 2030-01-01 00:00:00 Getting Started Knowledge 4536 True connectx 2030-01-01 00:00:00 Getting Started Knowledge 390 False nlp-getting-started 2030-01-01 00:00:00 Getting Started Knowledge 1184 False rock-paper-scissors 2021-02-01 23:59:00 Playground Prizes 152 False riiid-test-answer-prediction 2021-01-07 23:59:00 Featured $100,000 1466 False nfl-big-data-bowl-2021 2021-01-05 23:59:00 Analytics $100,000 0 False competitive-data-science-predict-future-sales 2020-12-31 23:59:00 Playground Kudos 9343 False halite-iv-playground-edition 2020-12-31 23:59:00 Playground Knowledge 43 False predict-volcanic-eruptions-ingv-oe 2020-12-28 23:59:00 Playground Swag 193 False hashcode-drone-delivery 2020-12-14 23:59:00 Playground Knowledge 79 False cdp-unlocking-climate-solutions 2020-12-02 23:59:00 Analytics $91,000 0 False lish-moa 2020-11-30 23:59:00 Research $30,000 3395 False google-football 2020-11-30 23:59:00 Featured $6,000 916 False conways-reverse-game-of-life-2020 2020-11-30 23:59:00 Playground Swag 131 False lyft-motion-prediction-autonomous-vehicles 2020-11-25 23:59:00 Featured $30,000 778 False 코드 실행 시 나오는 대회 목록에서 원하는 대회의 데이터셋을 불러온다. # 실습: 타이타닉 데이터 불러오기 !kaggle competitions download -c titanic Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.9 / client 1.5.4) Downloading train.csv to /content/drive/My Drive/Colab Notebooks/Python/python/practice/data 0% 0.00/59.8k [00:00&lt;?, ?B/s] 100% 59.8k/59.8k [00:00&lt;00:00, 8.18MB/s] Downloading gender_submission.csv to /content/drive/My Drive/Colab Notebooks/Python/python/practice/data 0% 0.00/3.18k [00:00&lt;?, ?B/s] 100% 3.18k/3.18k [00:00&lt;00:00, 443kB/s] Downloading test.csv to /content/drive/My Drive/Colab Notebooks/Python/python/practice/data 0% 0.00/28.0k [00:00&lt;?, ?B/s] 100% 28.0k/28.0k [00:00&lt;00:00, 3.97MB/s] !ls # ls: 리눅스 명령어, 경로 내 모든 데이터 파일을 보여줌 gender_submission.csv test.csv train.csv 총 3개의 데이터가 다운로드 되었다. gender_submission.csv test.csv train.csv II. Kaggle Data 실습_Titanic 데이터 살펴보기 Kaggle Code 필사 1. 데이터 살펴보기아래 코드를 실행하여 EDA 필수 패키지를 설치한다. import pandas as pd # 데이터 가공, 변환 import pandas_profiling # 보고서 기능 import numpy as np # 수치 연산&amp;배열, 행렬 import matplotlib as mpl # 시각화 import matplotlib.pyplot as plt # 시각화 from matplotlib.pyplot import figure # 시각화 import seaborn as sns from IPython.core.display import display, HTML from pandas_profiling import ProfileReport %matplotlib inline import matplotlib.pylab as plt plt.rcParams[\"figure.figsize\"] = (14,4) plt.rcParams['lines.linewidth'] = 2 plt.rcParams['lines.color'] = 'r' plt.rcParams['axes.grid'] = True (1) 데이터 수집 gender_submission.csv test.csv train.csv gender = pd.read_csv('data/gender_submission.csv') train = pd.read_csv('data/train.csv') test = pd.read_csv('data/test.csv') print(\"data import is done\") data import is done (2) 데이터 확인Kaggle 데이터를 불러와서 가장 먼저 확인해야 할 것은 데이터셋의 크기다. 변수의 개수 Numeric 변수 &amp; Categorical 변수의 개수 등 파악 cf) 보통 test 데이터의 변수 개수가 train 변수 개수보다 하나 적음 gender.shape, train.shape, test.shape ((418, 2), (891, 12), (418, 11)) # train 데이터의 상위 5개 데이터만 확인해보기 display(train.head()) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S Numerical 변수와 Categorical 변수를 구분한다. numeric_features 구분 numeric_features = train.select_dtypes(include=[np.number]) print(numeric_features.columns) print(\"The total number of numeric features are: \", len(numeric_features.columns)) numeric_features = test.select_dtypes(include=[np.number]) print(numeric_features.columns) print(\"The total number of numeric features are: \", len(numeric_features.columns)) Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;], dtype=&#39;object&#39;) The total number of numeric features are: 7 Index([&#39;PassengerId&#39;, &#39;Pclass&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Fare&#39;], dtype=&#39;object&#39;) The total number of numeric features are: 6 train의 numeric 데이터는 7개, test의 numeric 데이터는 6개이다. numeric_features 제외한 나머지 변수 추출 categorical_features = train.select_dtypes(exclude=[np.number]) print(categorical_features.columns) print(\"The total number of non numeric features are: \", len(categorical_features.columns)) categorical_features = test.select_dtypes(exclude=[np.number]) print(categorical_features.columns) print(\"The total number of non numeric features are: \", len(categorical_features.columns)) Index([&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;) The total number of non numeric features are: 5 Index([&#39;Name&#39;, &#39;Sex&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;], dtype=&#39;object&#39;) The total number of non numeric features are: 5 train의 numeric 아닌 데이터는 5개, test의 numeric 아닌 데이터는 5개이다. 2. Kaggle Code 필사- 필사 자료: EDA to Prediction(DieTanic) part1: Exploratory Data Analysis(EDA) Analysis of the features Finding any relations or trends considering multiple features # 사용 함수 import import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns plt.style.use('fivethirtyeight') import warnings warnings.filterwarnings('ignore') %matplotlib inline train.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th &#123; vertical-align: top; &#125; .dataframe thead th &#123; text-align: right; &#125; PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked 0 1 0 3 Braund, Mr. Owen Harris male 22.0 1 0 A/5 21171 7.2500 NaN S 1 2 1 1 Cumings, Mrs. John Bradley (Florence Briggs Th... female 38.0 1 0 PC 17599 71.2833 C85 C 2 3 1 3 Heikkinen, Miss. Laina female 26.0 0 0 STON/O2. 3101282 7.9250 NaN S 3 4 1 1 Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0 1 0 113803 53.1000 C123 S 4 5 0 3 Allen, Mr. William Henry male 35.0 0 0 373450 8.0500 NaN S train.isnull().sum() # checking for total null values PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 The AGE, Cabin and Embarked have null values. How many Survived? f, ax = plt.subplots(1, 2, figsize = (18, 8)) train['Survived'].value_counts().plot.pie(explode = [0, 0.1], autopct = '%1.1f%%', ax = ax[0], shadow = True) ax[0].set_title('Survived') # 왼쪽 그래프 제목 ax[0].set_ylabel('') sns.countplot('Survived', data=train, ax=ax[1]) ax[1].set_title('Survived') plt.show() It is evident that not many passengers survived the accident. Out of 891 passengers in traing set, only around 350 survived i.e Only 38.4% of the total training set survived the crash. We need to dig down more to get better insights from the data and see which categories of the passengers did survive and who didn’t. We will try to chek the survival rate by using the different features of the dataset. Some of the features being Sex, Port Of Embarcation, Age, etc. First let us understand the different types of features. *** Types Of Features** Categorical Features:A categorical variable is one that has two or more categories and each value in that feature can be categorised by them. For example, gender is a categorical variable having two categories(male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables(명목형 변수). Categorical Features in the dataset: Sex, Embarked Ordinal Features:An ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: if we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable. Ordinal(순위) Features in the dataset: PClass Continuous Feature:A feature is said to be continous if it can take values between any two points or between the minimun or maximum values in the features column. Coutinuous Features in the dataset: Age *** Analysing The Features** Sex -&gt; Catagorical Feature train.groupby(['Sex', 'Survived'])['Survived'].count() Sex Survived female 0 81 1 233 male 0 468 1 109 Name: Survived, dtype: int64 f, ax = plt.subplots(1, 2, figsize = (18, 8)) train[['Sex', 'Survived']].groupby(['Sex']).mean().plot.bar(ax = ax[0]) ax[0].set_title('Survived vs Sex') sns.countplot('Sex', hue='Survived', data=train, ax = ax[1]) ax[1].set_title('Sex: Survived vs Dead') plt.show() This looks interesting. The number of men on the ship is lot more than the number of women. Still the number of women saved is almost twice the number of males saved. The survival rates for a women on the ship is around 75% while that for men in around 18-19%. This looks to be a very important feature for modeling. But is it the best? Let’s check other features. *** Pclass -&gt; Ordinal Feature** # 클래스별 사망자/생존자 수 pd.crosstab(train.Pclass, train.Survived, margins=True).style.background_gradient(cmap = 'summer_r') #T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col0,#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col1,#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col2{ background-color: #ffff66; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col1{ background-color: #cee666; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col2{ background-color: #f4fa66; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col0{ background-color: #f6fa66; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col0{ background-color: #60b066; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col1{ background-color: #dfef66; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col2{ background-color: #90c866; color: #000000; }#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col0,#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col1,#T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col2{ background-color: #008066; color: #f1f1f1; } Survived 0 1 All Pclass &lt;tr&gt; &lt;th id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002level0_row0&quot; class=&quot;row_heading level0 row0&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col0&quot; class=&quot;data row0 col0&quot; &gt;80&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col1&quot; class=&quot;data row0 col1&quot; &gt;136&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row0_col2&quot; class=&quot;data row0 col2&quot; &gt;216&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002level0_row1&quot; class=&quot;row_heading level0 row1&quot; &gt;2&lt;/th&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col0&quot; class=&quot;data row1 col0&quot; &gt;97&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col1&quot; class=&quot;data row1 col1&quot; &gt;87&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row1_col2&quot; class=&quot;data row1 col2&quot; &gt;184&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002level0_row2&quot; class=&quot;row_heading level0 row2&quot; &gt;3&lt;/th&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col0&quot; class=&quot;data row2 col0&quot; &gt;372&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col1&quot; class=&quot;data row2 col1&quot; &gt;119&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row2_col2&quot; class=&quot;data row2 col2&quot; &gt;491&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002level0_row3&quot; class=&quot;row_heading level0 row3&quot; &gt;All&lt;/th&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col0&quot; class=&quot;data row3 col0&quot; &gt;549&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col1&quot; class=&quot;data row3 col1&quot; &gt;342&lt;/td&gt; &lt;td id=&quot;T_8a5ee43e_1e4d_11eb_98db_0242ac1c0002row3_col2&quot; class=&quot;data row3 col2&quot; &gt;891&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; f, ax=plt.subplots(1, 2, figsize = (18, 8)) train['Pclass'].value_counts().plot.bar(color = ['#CD7F32', '#FFDF00', '#D3D3D3'], ax = ax[0]) ax[0].set_title('Number Of Passengers By Pclass') ax[0].set_ylabel('Count') sns.countplot('Pclass', hue='Survived', data=train, ax = ax[1]) ax[1].set_title('Pclass:Survived vs Dead') plt.show() People say Money Can’t Buy Everything. But we can clearly see that Passengers Of Pclass 1 were given a very high priority while rescue. Even thouugh the number of Passengers in Pclass 3 were a lot higher, still the number of survival from them is very low, somewhere around 25%. For Pclass 1% survived is around 63% while for Pclass 2 is around 48%. So money and status matters. Such a materialistic world. Let’s dive in little bit more and check for other interesting observations. Let’s check survival rate with Sex and Pclass Together. # Sex, Pclss 기준으로 Survived 인원 보기 pd.crosstab([train.Sex, train.Survived], train.Pclass, margins=True).style.background_gradient(cmap='summer_r') #T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col0,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col1,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col3,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col2{ background-color: #ffff66; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col2,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col2{ background-color: #f1f866; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col0{ background-color: #96cb66; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col1{ background-color: #a3d166; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col3{ background-color: #cfe766; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col0{ background-color: #a7d366; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col1,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col3{ background-color: #85c266; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col2{ background-color: #6eb666; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col0{ background-color: #cde666; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col1{ background-color: #f0f866; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col3{ background-color: #f7fb66; color: #000000; }#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col0,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col1,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col2,#T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col3{ background-color: #008066; color: #f1f1f1; } Pclass 1 2 3 All Sex Survived &lt;tr&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level0_row0&quot; class=&quot;row_heading level0 row0&quot; rowspan=2&gt;female&lt;/th&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level1_row0&quot; class=&quot;row_heading level1 row0&quot; &gt;0&lt;/th&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col0&quot; class=&quot;data row0 col0&quot; &gt;3&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col1&quot; class=&quot;data row0 col1&quot; &gt;6&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col2&quot; class=&quot;data row0 col2&quot; &gt;72&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row0_col3&quot; class=&quot;data row0 col3&quot; &gt;81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level1_row1&quot; class=&quot;row_heading level1 row1&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col0&quot; class=&quot;data row1 col0&quot; &gt;91&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col1&quot; class=&quot;data row1 col1&quot; &gt;70&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col2&quot; class=&quot;data row1 col2&quot; &gt;72&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row1_col3&quot; class=&quot;data row1 col3&quot; &gt;233&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level0_row2&quot; class=&quot;row_heading level0 row2&quot; rowspan=2&gt;male&lt;/th&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level1_row2&quot; class=&quot;row_heading level1 row2&quot; &gt;0&lt;/th&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col0&quot; class=&quot;data row2 col0&quot; &gt;77&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col1&quot; class=&quot;data row2 col1&quot; &gt;91&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col2&quot; class=&quot;data row2 col2&quot; &gt;300&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row2_col3&quot; class=&quot;data row2 col3&quot; &gt;468&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level1_row3&quot; class=&quot;row_heading level1 row3&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col0&quot; class=&quot;data row3 col0&quot; &gt;45&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col1&quot; class=&quot;data row3 col1&quot; &gt;17&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col2&quot; class=&quot;data row3 col2&quot; &gt;47&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row3_col3&quot; class=&quot;data row3 col3&quot; &gt;109&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level0_row4&quot; class=&quot;row_heading level0 row4&quot; &gt;All&lt;/th&gt; &lt;th id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002level1_row4&quot; class=&quot;row_heading level1 row4&quot; &gt;&lt;/th&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col0&quot; class=&quot;data row4 col0&quot; &gt;216&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col1&quot; class=&quot;data row4 col1&quot; &gt;184&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col2&quot; class=&quot;data row4 col2&quot; &gt;491&lt;/td&gt; &lt;td id=&quot;T_ed10372a_1e4f_11eb_98db_0242ac1c0002row4_col3&quot; class=&quot;data row4 col3&quot; &gt;891&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; sns.factorplot('Pclass', 'Survived', hue = 'Sex', data = train) plt.show() We use FactorPlot in this case, because they make the seperation of categorical values easy. Looking at the CrossTab and the FactorPlot, we can easily infer that survival for Women from Pclass 1 is about 95-96%, as only 3 out of 94 Women from Pclass 1 died. It is evident that irrespective of Pclass, Women were given first priority while rescue. Even Men from Pclass 1 have a very low survival rate. Looks like Pclass is also an important feature. Let’s analyse other features. *** Age -&gt; Continous Feature** print('Oldest Passenger was of:', train['Age'].max(), 'Years') print('Youngest Passenger was of:', train['Age'].min(), 'Years') print('Average Age on the ship:', train['Age'].mean(), 'Years') Oldest Passenger was of: 80.0 Years Youngest Passenger was of: 0.42 Years Average Age on the ship: 29.69911764705882 Years f, ax = plt.subplots(1, 2, figsize = (18, 8)) sns.violinplot(\"Pclass\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[0]) ax[0].set_title('Pclass and Age vs Survived') ax[0].set_yticks(range(0, 110, 10)) sns.violinplot(\"Sex\", \"Age\", hue=\"Survived\", data=train, split=True, ax=ax[1]) ax[1].set_title('Sex and Age vs Survived') ax[1].set_yticks(range(0, 110, 10)) plt.show() Observations: The number of cildren increases with Pclass and the survival rate for passengers below Age 10(i.e children) looks to be good irrespective of the Pclass. Survival chances for passengers aged 20-50 from Pclass 1 is high and is even better for women. For males, the survival chances decreases with an increase in age. As we had seen earlier, the Age feature has 177 null values. To replace these NaN values, we can assign them the mean age of the dataset. But the problem is, there were many people with many different ages. We just can’t assign a 4 year kid with the mean age that is 29 years. Is there any way to find out what age-band does the passenger lie? We can check the Name feature. Looking upon the feature, we cas see that the names have a salutaion like Mr or Mrs. Thus we can assign the mean values of Mr and Mrs to the respective groups. “What’s in a name?” —-&gt; Feature train['Initial']=0 for i in train: train['Initial']=train.Name.str.extract('([A-Za-z]+)\\.') # extract the Salutations Using the Regex: [A-Za-z]+).. So what it does is, it looks for strings which lie between A-Z or a-z and followed by a .(dot). So we successfully extract the Initials from the Name. pd.crosstab(train.Initial, train.Sex).T.style.background_gradient(cmap='summer_r') # checking the Initials with the sex #T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col0,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col1,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col3,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col4,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col5,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col7,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col8,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col12,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col15,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col16,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col2,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col6,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col9,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col10,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col11,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col13,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col14{ background-color: #ffff66; color: #000000; }#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col2,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col6,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col9,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col10,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col11,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col13,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col14,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col0,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col1,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col3,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col4,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col5,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col7,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col8,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col12,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col15,#T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col16{ background-color: #008066; color: #f1f1f1; } Initial Capt Col Countess Don Dr Jonkheer Lady Major Master Miss Mlle Mme Mr Mrs Ms Rev Sir Sex &lt;tr&gt; &lt;th id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002level0_row0&quot; class=&quot;row_heading level0 row0&quot; &gt;female&lt;/th&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col0&quot; class=&quot;data row0 col0&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col1&quot; class=&quot;data row0 col1&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col2&quot; class=&quot;data row0 col2&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col3&quot; class=&quot;data row0 col3&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col4&quot; class=&quot;data row0 col4&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col5&quot; class=&quot;data row0 col5&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col6&quot; class=&quot;data row0 col6&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col7&quot; class=&quot;data row0 col7&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col8&quot; class=&quot;data row0 col8&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col9&quot; class=&quot;data row0 col9&quot; &gt;182&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col10&quot; class=&quot;data row0 col10&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col11&quot; class=&quot;data row0 col11&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col12&quot; class=&quot;data row0 col12&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col13&quot; class=&quot;data row0 col13&quot; &gt;125&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col14&quot; class=&quot;data row0 col14&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col15&quot; class=&quot;data row0 col15&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row0_col16&quot; class=&quot;data row0 col16&quot; &gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002level0_row1&quot; class=&quot;row_heading level0 row1&quot; &gt;male&lt;/th&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col0&quot; class=&quot;data row1 col0&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col1&quot; class=&quot;data row1 col1&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col2&quot; class=&quot;data row1 col2&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col3&quot; class=&quot;data row1 col3&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col4&quot; class=&quot;data row1 col4&quot; &gt;6&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col5&quot; class=&quot;data row1 col5&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col6&quot; class=&quot;data row1 col6&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col7&quot; class=&quot;data row1 col7&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col8&quot; class=&quot;data row1 col8&quot; &gt;40&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col9&quot; class=&quot;data row1 col9&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col10&quot; class=&quot;data row1 col10&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col11&quot; class=&quot;data row1 col11&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col12&quot; class=&quot;data row1 col12&quot; &gt;517&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col13&quot; class=&quot;data row1 col13&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col14&quot; class=&quot;data row1 col14&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col15&quot; class=&quot;data row1 col15&quot; &gt;6&lt;/td&gt; &lt;td id=&quot;T_dae012f4_1e5f_11eb_98db_0242ac1c0002row1_col16&quot; class=&quot;data row1 col16&quot; &gt;1&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; There are some misspelled Initials like Mlle of Mme that stand for Miss. I will replace them with Miss and same thing for other values. train['Initial'].replace(['Mlle', 'Mme', 'Ms', 'Dr', 'Major', 'Lady', 'Countess', 'Jonkheer', 'Col', 'Rev', 'Capt', 'Sir', 'Don'], ['Miss', 'Miss', 'Miss', 'Mr', 'Mr', 'Mrs', 'Mrs', 'Other', 'Other', 'Other', 'Mr', 'Mr', 'Mr'], inplace=True) train.groupby('Initial')['Age'].mean() # check the average age by Initials Initial Master 4.574167 Miss 21.860000 Mr 32.739609 Mrs 35.981818 Other 45.888889 Name: Age, dtype: float64 Filling NaN Ages # assigning the NaN Values with the Ceil values of the mean ages train.loc[(train.Age.isnull())&amp;(train.Initial=='Mr'),'Age']=33 train.loc[(train.Age.isnull())&amp;(train.Initial=='Mrs'),'Age']=36 train.loc[(train.Age.isnull())&amp;(train.Initial=='Master'),'Age']=5 train.loc[(train.Age.isnull())&amp;(train.Initial=='Miss'),'Age']=22 train.loc[(train.Age.isnull())&amp;(train.Initial=='Other'),'Age']=46 train.Age.isnull().any() # so no null values left finally False f, ax=plt.subplots(1, 2, figsize=(20, 10)) train[train['Survived']==0].Age.plot.hist(ax=ax[0], bins=20, edgecolor='black', color='red') ax[0].set_title('Survived=0') x1=list(range(0, 85, 5)) ax[0].set_xticks(x1) train[train['Survived']==1].Age.plot.hist(ax=ax[1], color='green', bins=20, edgecolor='black') ax[1].set_title('Survived=1') x2=list(range(0, 85, 5)) ax[1].set_xticks(x2) plt.show() Observations: The Toddlers(age &lt; 5) were saved in large numbers(The Women and Child First Policy) The oldest Passenger was saved(80 years) Maximum number of deaths were in the age group of 30-40 sns.factorplot('Pclass', 'Survived', col='Initial', data=train) plt.show() The Women and Child first policy thus holds true irrespective of the class. *** Embarked -&gt; Categorical Value** pd.crosstab([train.Embarked, train.Pclass], [train.Sex, train.Survived], margins=True).style.background_gradient(cmap='summer_r') #T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col2{ background-color: #fcfe66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col1{ background-color: #d2e866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col2{ background-color: #f2f866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col3{ background-color: #d8ec66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col4,#T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col3{ background-color: #e8f466; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col1,#T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col2,#T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col3,#T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col4,#T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col2,#T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col3,#T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col4{ background-color: #ffff66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col1,#T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col0{ background-color: #f9fc66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col3,#T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col4{ background-color: #fbfd66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col1{ background-color: #e6f266; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col1{ background-color: #f0f866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col2{ background-color: #eef666; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col4,#T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col0{ background-color: #edf666; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col1{ background-color: #fefe66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col0{ background-color: #e3f166; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col2{ background-color: #ecf666; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col3{ background-color: #f8fc66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col4{ background-color: #ebf566; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col1{ background-color: #cde666; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col2{ background-color: #e4f266; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col3{ background-color: #bede66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col4{ background-color: #dbed66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col1{ background-color: #bdde66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col2{ background-color: #d3e966; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col3,#T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col1{ background-color: #dcee66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col4{ background-color: #d1e866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col0{ background-color: #52a866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col2{ background-color: #81c066; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col3{ background-color: #b0d866; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col4{ background-color: #9acc66; color: #000000; }#T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col0,#T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col1,#T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col2,#T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col3,#T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col4{ background-color: #008066; color: #f1f1f1; } Sex female male All Survived 0 1 0 1 Embarked Pclass &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level0_row0&quot; class=&quot;row_heading level0 row0&quot; rowspan=3&gt;C&lt;/th&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row0&quot; class=&quot;row_heading level1 row0&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col0&quot; class=&quot;data row0 col0&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col1&quot; class=&quot;data row0 col1&quot; &gt;42&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col2&quot; class=&quot;data row0 col2&quot; &gt;25&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col3&quot; class=&quot;data row0 col3&quot; &gt;17&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row0_col4&quot; class=&quot;data row0 col4&quot; &gt;85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row1&quot; class=&quot;row_heading level1 row1&quot; &gt;2&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col0&quot; class=&quot;data row1 col0&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col1&quot; class=&quot;data row1 col1&quot; &gt;7&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col2&quot; class=&quot;data row1 col2&quot; &gt;8&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col3&quot; class=&quot;data row1 col3&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row1_col4&quot; class=&quot;data row1 col4&quot; &gt;17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row2&quot; class=&quot;row_heading level1 row2&quot; &gt;3&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col0&quot; class=&quot;data row2 col0&quot; &gt;8&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col1&quot; class=&quot;data row2 col1&quot; &gt;15&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col2&quot; class=&quot;data row2 col2&quot; &gt;33&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col3&quot; class=&quot;data row2 col3&quot; &gt;10&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row2_col4&quot; class=&quot;data row2 col4&quot; &gt;66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level0_row3&quot; class=&quot;row_heading level0 row3&quot; rowspan=3&gt;Q&lt;/th&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row3&quot; class=&quot;row_heading level1 row3&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col0&quot; class=&quot;data row3 col0&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col1&quot; class=&quot;data row3 col1&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col2&quot; class=&quot;data row3 col2&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col3&quot; class=&quot;data row3 col3&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row3_col4&quot; class=&quot;data row3 col4&quot; &gt;2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row4&quot; class=&quot;row_heading level1 row4&quot; &gt;2&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col0&quot; class=&quot;data row4 col0&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col1&quot; class=&quot;data row4 col1&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col2&quot; class=&quot;data row4 col2&quot; &gt;1&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col3&quot; class=&quot;data row4 col3&quot; &gt;0&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row4_col4&quot; class=&quot;data row4 col4&quot; &gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row5&quot; class=&quot;row_heading level1 row5&quot; &gt;3&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col0&quot; class=&quot;data row5 col0&quot; &gt;9&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col1&quot; class=&quot;data row5 col1&quot; &gt;24&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col2&quot; class=&quot;data row5 col2&quot; &gt;36&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col3&quot; class=&quot;data row5 col3&quot; &gt;3&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row5_col4&quot; class=&quot;data row5 col4&quot; &gt;72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level0_row6&quot; class=&quot;row_heading level0 row6&quot; rowspan=3&gt;S&lt;/th&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row6&quot; class=&quot;row_heading level1 row6&quot; &gt;1&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col0&quot; class=&quot;data row6 col0&quot; &gt;2&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col1&quot; class=&quot;data row6 col1&quot; &gt;46&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col2&quot; class=&quot;data row6 col2&quot; &gt;51&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col3&quot; class=&quot;data row6 col3&quot; &gt;28&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row6_col4&quot; class=&quot;data row6 col4&quot; &gt;127&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row7&quot; class=&quot;row_heading level1 row7&quot; &gt;2&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col0&quot; class=&quot;data row7 col0&quot; &gt;6&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col1&quot; class=&quot;data row7 col1&quot; &gt;61&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col2&quot; class=&quot;data row7 col2&quot; &gt;82&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col3&quot; class=&quot;data row7 col3&quot; &gt;15&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row7_col4&quot; class=&quot;data row7 col4&quot; &gt;164&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row8&quot; class=&quot;row_heading level1 row8&quot; &gt;3&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col0&quot; class=&quot;data row8 col0&quot; &gt;55&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col1&quot; class=&quot;data row8 col1&quot; &gt;33&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col2&quot; class=&quot;data row8 col2&quot; &gt;231&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col3&quot; class=&quot;data row8 col3&quot; &gt;34&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row8_col4&quot; class=&quot;data row8 col4&quot; &gt;353&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level0_row9&quot; class=&quot;row_heading level0 row9&quot; &gt;All&lt;/th&gt; &lt;th id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002level1_row9&quot; class=&quot;row_heading level1 row9&quot; &gt;&lt;/th&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col0&quot; class=&quot;data row9 col0&quot; &gt;81&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col1&quot; class=&quot;data row9 col1&quot; &gt;231&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col2&quot; class=&quot;data row9 col2&quot; &gt;468&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col3&quot; class=&quot;data row9 col3&quot; &gt;109&lt;/td&gt; &lt;td id=&quot;T_7944395a_1e62_11eb_98db_0242ac1c0002row9_col4&quot; class=&quot;data row9 col4&quot; &gt;889&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; Chances for Survival by Port of Embarkation sns.factorplot('Embarked', 'Survived', data=train) fig=plt.gcf() fig.set_size_inches(5, 3) plt.show() The chances for survival for Port C is highest around 0.55 while it is lowest for S. f,ax=plt.subplots(2,2,figsize=(20,15)) sns.countplot('Embarked',data=train,ax=ax[0,0]) ax[0,0].set_title('No. Of Passengers Boarded') sns.countplot('Embarked',hue='Sex',data=train,ax=ax[0,1]) ax[0,1].set_title('Male-Female Split for Embarked') sns.countplot('Embarked',hue='Survived',data=train,ax=ax[1,0]) ax[1,0].set_title('Embarked vs Survived') sns.countplot('Embarked',hue='Pclass',data=train,ax=ax[1,1]) ax[1,1].set_title('Embarked vs Pclass') plt.subplots_adjust(wspace=0.2,hspace=0.5) plt.show() Observations: Maximum passengers boarded from S. Majority of them being from Pclass3. The passengers from C look to be lucky as a good proportion of them survived. The reason for this maybe the rescue of all the Pclass 1 and Pclass 2 passengers. The Embark S looks to the port from where majority of the rich people boarded. Still the chances for surical is low here, that is vecause many passengers from Pclass 3 around 81% didn’t survive. Port Q had almost 95% of the passengers were from Pclass 3. sns.factorplot('Pclass', 'Survived', hue='Sex', col='Embarked', data=train) plt.show() 0bservations: The survival chances are almost 1 for women for Pclass 1 and Pclass 2 irrespective of the Pclass. Port S looks to be very unlucky for Pclass 3 passengers as the survival rate for both men and women is very low.(Money Matters) Port Q looks to be unlukiest for Men, as almost all were from Pclass 3. Filling Embarked NaNAs we saw that maximum passengers boarded from Port S, we replace NaN with S. train['Embarked'].fillna('S', inplace=True) train.Embarked.isnull().any() # Finally No NaN values False *** SibSip -&gt; Discrete Feature**This feature represents whether a person is alone of with their family members.Sibling = brother, sister, stepbrother, stepsisterSpouse = husband, wife Part2: Feature Engineering and Data Cleaning Adding any few features Removing redundant features Converting features into suitable from for modiling Part3: Rredictive Modeling Running Basic Algorithms Cross Validation Ensembling Important Features Extraction","categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://ne-choi.github.io/tags/python/"},{"name":"EDA","slug":"EDA","permalink":"https://ne-choi.github.io/tags/EDA/"},{"name":"kaggle","slug":"kaggle","permalink":"https://ne-choi.github.io/tags/kaggle/"},{"name":"titanic","slug":"titanic","permalink":"https://ne-choi.github.io/tags/titanic/"}],"author":"ne-choi"},{"title":"page","slug":"about/me","date":"2020-10-30T02:00:40.000Z","updated":"2020-10-30T02:00:40.181Z","comments":true,"path":"/2020/10/30/about/me/","link":"","permalink":"https://ne-choi.github.io/2020/10/30/about/me/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Make Naeun Blog","slug":"hello-world","date":"2020-10-28T06:58:49.990Z","updated":"2020-11-04T06:42:01.476Z","comments":true,"path":"/2020/10/28/hello-world/","link":"","permalink":"https://ne-choi.github.io/2020/10/28/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new \"My New Post\" More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[],"tags":[],"author":"ne-choi"}],"categories":[{"name":"Study","slug":"Study","permalink":"https://ne-choi.github.io/categories/Study/"},{"name":"Postech","slug":"Study/Postech","permalink":"https://ne-choi.github.io/categories/Study/Postech/"},{"name":"Python","slug":"Study/Postech/Python","permalink":"https://ne-choi.github.io/categories/Study/Postech/Python/"},{"name":"Statistics","slug":"Study/Postech/Statistics","permalink":"https://ne-choi.github.io/categories/Study/Postech/Statistics/"},{"name":"Project","slug":"Project","permalink":"https://ne-choi.github.io/categories/Project/"},{"name":"Kaggle","slug":"Project/Kaggle","permalink":"https://ne-choi.github.io/categories/Project/Kaggle/"},{"name":"ML","slug":"Study/Postech/ML","permalink":"https://ne-choi.github.io/categories/Study/Postech/ML/"},{"name":"Bigdata","slug":"Study/Postech/Bigdata","permalink":"https://ne-choi.github.io/categories/Study/Postech/Bigdata/"},{"name":"ML","slug":"Study/ML","permalink":"https://ne-choi.github.io/categories/Study/ML/"},{"name":"Contest","slug":"Project/Contest","permalink":"https://ne-choi.github.io/categories/Project/Contest/"},{"name":"ADsP","slug":"Study/ADsP","permalink":"https://ne-choi.github.io/categories/Study/ADsP/"},{"name":"Python","slug":"Study/Python","permalink":"https://ne-choi.github.io/categories/Study/Python/"},{"name":"Tips","slug":"Tips","permalink":"https://ne-choi.github.io/categories/Tips/"},{"name":"Programming","slug":"Tips/Programming","permalink":"https://ne-choi.github.io/categories/Tips/Programming/"}],"tags":[{"name":"Postech","slug":"Postech","permalink":"https://ne-choi.github.io/tags/Postech/"},{"name":"Python","slug":"Python","permalink":"https://ne-choi.github.io/tags/Python/"},{"name":"리스트","slug":"리스트","permalink":"https://ne-choi.github.io/tags/%EB%A6%AC%EC%8A%A4%ED%8A%B8/"},{"name":"튜플","slug":"튜플","permalink":"https://ne-choi.github.io/tags/%ED%8A%9C%ED%94%8C/"},{"name":"함수","slug":"함수","permalink":"https://ne-choi.github.io/tags/%ED%95%A8%EC%88%98/"},{"name":"for문","slug":"for문","permalink":"https://ne-choi.github.io/tags/for%EB%AC%B8/"},{"name":"while문","slug":"while문","permalink":"https://ne-choi.github.io/tags/while%EB%AC%B8/"},{"name":"회귀모형","slug":"회귀모형","permalink":"https://ne-choi.github.io/tags/%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"},{"name":"주성분분석","slug":"주성분분석","permalink":"https://ne-choi.github.io/tags/%EC%A3%BC%EC%84%B1%EB%B6%84%EB%B6%84%EC%84%9D/"},{"name":"차원축소","slug":"차원축소","permalink":"https://ne-choi.github.io/tags/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C/"},{"name":"표본추출","slug":"표본추출","permalink":"https://ne-choi.github.io/tags/%ED%91%9C%EB%B3%B8%EC%B6%94%EC%B6%9C/"},{"name":"과적합","slug":"과적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%A0%81%ED%95%A9/"},{"name":"과소적합","slug":"과소적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/"},{"name":"신뢰구간","slug":"신뢰구간","permalink":"https://ne-choi.github.io/tags/%EC%8B%A0%EB%A2%B0%EA%B5%AC%EA%B0%84/"},{"name":"t-test","slug":"t-test","permalink":"https://ne-choi.github.io/tags/t-test/"},{"name":"p-value","slug":"p-value","permalink":"https://ne-choi.github.io/tags/p-value/"},{"name":"통계","slug":"통계","permalink":"https://ne-choi.github.io/tags/%ED%86%B5%EA%B3%84/"},{"name":"데이터시각화","slug":"데이터시각화","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"EDA","slug":"EDA","permalink":"https://ne-choi.github.io/tags/EDA/"},{"name":"Kaggle","slug":"Kaggle","permalink":"https://ne-choi.github.io/tags/Kaggle/"},{"name":"Kaggle beginner","slug":"Kaggle-beginner","permalink":"https://ne-choi.github.io/tags/Kaggle-beginner/"},{"name":"평균","slug":"평균","permalink":"https://ne-choi.github.io/tags/%ED%8F%89%EA%B7%A0/"},{"name":"분산","slug":"분산","permalink":"https://ne-choi.github.io/tags/%EB%B6%84%EC%82%B0/"},{"name":"딥러닝","slug":"딥러닝","permalink":"https://ne-choi.github.io/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"텍스트마이닝","slug":"텍스트마이닝","permalink":"https://ne-choi.github.io/tags/%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A7%88%EC%9D%B4%EB%8B%9D/"},{"name":"최소자승법","slug":"최소자승법","permalink":"https://ne-choi.github.io/tags/%EC%B5%9C%EC%86%8C%EC%9E%90%EC%8A%B9%EB%B2%95/"},{"name":"연관규칙","slug":"연관규칙","permalink":"https://ne-choi.github.io/tags/%EC%97%B0%EA%B4%80%EA%B7%9C%EC%B9%99/"},{"name":"로지스틱회귀분석","slug":"로지스틱회귀분석","permalink":"https://ne-choi.github.io/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80%EB%B6%84%EC%84%9D/"},{"name":"k-인접기법","slug":"k-인접기법","permalink":"https://ne-choi.github.io/tags/k-%EC%9D%B8%EC%A0%91%EA%B8%B0%EB%B2%95/"},{"name":"판별분석","slug":"판별분석","permalink":"https://ne-choi.github.io/tags/%ED%8C%90%EB%B3%84%EB%B6%84%EC%84%9D/"},{"name":"의사결정나무","slug":"의사결정나무","permalink":"https://ne-choi.github.io/tags/%EC%9D%98%EC%82%AC%EA%B2%B0%EC%A0%95%EB%82%98%EB%AC%B4/"},{"name":"랜덤포레스트","slug":"랜덤포레스트","permalink":"https://ne-choi.github.io/tags/%EB%9E%9C%EB%8D%A4%ED%8F%AC%EB%A0%88%EC%8A%A4%ED%8A%B8/"},{"name":"데이터마이닝","slug":"데이터마이닝","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A7%88%EC%9D%B4%EB%8B%9D/"},{"name":"다중회귀모형","slug":"다중회귀모형","permalink":"https://ne-choi.github.io/tags/%EB%8B%A4%EC%A4%91%ED%9A%8C%EA%B7%80%EB%AA%A8%ED%98%95/"},{"name":"SVM","slug":"SVM","permalink":"https://ne-choi.github.io/tags/SVM/"},{"name":"서포트벡터머신","slug":"서포트벡터머신","permalink":"https://ne-choi.github.io/tags/%EC%84%9C%ED%8F%AC%ED%8A%B8%EB%B2%A1%ED%84%B0%EB%A8%B8%EC%8B%A0/"},{"name":"lattice","slug":"lattice","permalink":"https://ne-choi.github.io/tags/lattice/"},{"name":"ggplot2","slug":"ggplot2","permalink":"https://ne-choi.github.io/tags/ggplot2/"},{"name":"파이썬머신러닝완벽가이드","slug":"파이썬머신러닝완벽가이드","permalink":"https://ne-choi.github.io/tags/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%EC%99%84%EB%B2%BD%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"회귀","slug":"회귀","permalink":"https://ne-choi.github.io/tags/%ED%9A%8C%EA%B7%80/"},{"name":"선형회귀","slug":"선형회귀","permalink":"https://ne-choi.github.io/tags/%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80/"},{"name":"릿지라쏘엘라스틱넷","slug":"릿지라쏘엘라스틱넷","permalink":"https://ne-choi.github.io/tags/%EB%A6%BF%EC%A7%80%EB%9D%BC%EC%8F%98%EC%97%98%EB%9D%BC%EC%8A%A4%ED%8B%B1%EB%84%B7/"},{"name":"로지스틱회귀","slug":"로지스틱회귀","permalink":"https://ne-choi.github.io/tags/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1%ED%9A%8C%EA%B7%80/"},{"name":"과적합과소적합","slug":"과적합과소적합","permalink":"https://ne-choi.github.io/tags/%EA%B3%BC%EC%A0%81%ED%95%A9%EA%B3%BC%EC%86%8C%EC%A0%81%ED%95%A9/"},{"name":"분류","slug":"분류","permalink":"https://ne-choi.github.io/tags/%EB%B6%84%EB%A5%98/"},{"name":"결정트리","slug":"결정트리","permalink":"https://ne-choi.github.io/tags/%EA%B2%B0%EC%A0%95%ED%8A%B8%EB%A6%AC/"},{"name":"앙상블학습","slug":"앙상블학습","permalink":"https://ne-choi.github.io/tags/%EC%95%99%EC%83%81%EB%B8%94%ED%95%99%EC%8A%B5/"},{"name":"정확도","slug":"정확도","permalink":"https://ne-choi.github.io/tags/%EC%A0%95%ED%99%95%EB%8F%84/"},{"name":"재현율","slug":"재현율","permalink":"https://ne-choi.github.io/tags/%EC%9E%AC%ED%98%84%EC%9C%A8/"},{"name":"F1스코어","slug":"F1스코어","permalink":"https://ne-choi.github.io/tags/F1%EC%8A%A4%EC%BD%94%EC%96%B4/"},{"name":"Titanic","slug":"Titanic","permalink":"https://ne-choi.github.io/tags/Titanic/"},{"name":"공모전","slug":"공모전","permalink":"https://ne-choi.github.io/tags/%EA%B3%B5%EB%AA%A8%EC%A0%84/"},{"name":"시각화","slug":"시각화","permalink":"https://ne-choi.github.io/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"ADsP","slug":"ADsP","permalink":"https://ne-choi.github.io/tags/ADsP/"},{"name":"데이터분석준전문가","slug":"데이터분석준전문가","permalink":"https://ne-choi.github.io/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D%EC%A4%80%EC%A0%84%EB%AC%B8%EA%B0%80/"},{"name":"dplyr","slug":"dplyr","permalink":"https://ne-choi.github.io/tags/dplyr/"},{"name":"read csv","slug":"read-csv","permalink":"https://ne-choi.github.io/tags/read-csv/"},{"name":"Vector","slug":"Vector","permalink":"https://ne-choi.github.io/tags/Vector/"},{"name":"Array","slug":"Array","permalink":"https://ne-choi.github.io/tags/Array/"},{"name":"python","slug":"python","permalink":"https://ne-choi.github.io/tags/python/"},{"name":"10 Minutes to Pandas","slug":"10-Minutes-to-Pandas","permalink":"https://ne-choi.github.io/tags/10-Minutes-to-Pandas/"},{"name":"pandas","slug":"pandas","permalink":"https://ne-choi.github.io/tags/pandas/"},{"name":"numpy","slug":"numpy","permalink":"https://ne-choi.github.io/tags/numpy/"},{"name":"matplotlib","slug":"matplotlib","permalink":"https://ne-choi.github.io/tags/matplotlib/"},{"name":"lists vs tuples","slug":"lists-vs-tuples","permalink":"https://ne-choi.github.io/tags/lists-vs-tuples/"},{"name":"dictionary","slug":"dictionary","permalink":"https://ne-choi.github.io/tags/dictionary/"},{"name":"list","slug":"list","permalink":"https://ne-choi.github.io/tags/list/"},{"name":"tuples","slug":"tuples","permalink":"https://ne-choi.github.io/tags/tuples/"},{"name":"kaggle","slug":"kaggle","permalink":"https://ne-choi.github.io/tags/kaggle/"},{"name":"unzip","slug":"unzip","permalink":"https://ne-choi.github.io/tags/unzip/"},{"name":"unzip multiple files","slug":"unzip-multiple-files","permalink":"https://ne-choi.github.io/tags/unzip-multiple-files/"},{"name":"titanic","slug":"titanic","permalink":"https://ne-choi.github.io/tags/titanic/"}]}